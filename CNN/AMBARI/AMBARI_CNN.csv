v1,v2
ns,For running a server (daemon)  the current states are: START and STARTED. It would be nice to have transition state: STARTING  and STOPPING.
ns,After agent execute commands on behave of the controller  agent should store cluster id  blueprint name and blueprint revision in the local disk to keep track of the current deployment state on the agent.
ns,If the heartbeat was not received correctly by the controller  then it should retry.
ns,Agent entities beans are located ambari-client: org.apache.ambari.common.rest.entities.agent. Hence  schema xsd generation for public rest api will include agent entities beans. We should move it to org.apache.ambari.common.rest.agent to exclude agent entities to expose to public.
ns,There is no checks for safemode when we reconfigure the cluster.
ns,hcat client should be installed at the templeton server node.
ns,Remove import of mysql puppet module from manifest because this module is deprecated.
ns,After the jdk install  we do not validate the path. This causes problems during service start in later stages.
ns,Undefined variable used.
ns,Api to get hosts should allow filtering out bad hosts
ns,Currently in Dashboard when service entry is clicked  right side alerts table caption does not indicate that it is showing alerts related to that service. So Cpation of the table should change indicating the corresponding service name.
ns,IT is sort of redundant..
ns,Following names need to be consistent: Hdfs -&gt; HDFS Mapreduce -&gt; MapReduce Zookeeper -&gt; ZooKeeper HADOOP -&gt; Hadoop
ns,Currently the enable lzo option shows a text box that needs to be filled with true/false. Changing the UI element to checkbox.
ns,Puppet agent timeout should be increases as sometimes (possibly due to a bug in ruby) puppet master takes long time to compile and send back the catalog.
ns,Refactor puppet kick loop to easily change retries and timeouts.
ns,We need to support data cleanup so that a cluster can be re-installed in case of failures.
ns,Like the title says: overwrite index.php with the contents of clusters.php  making sure to add a link to the AddNodesWizard as well.
ns,Uninstall/wipeout support from UI.
ns,There's a bunch of (temporary playground) files that got wrongly committed to the HMC codebase  so this is to remove all of them and get things into a cleaner state (at least on the face of things).
ns,Currently  there is no support for uninstall of mysql package.
ns,There's some images in the html/ directory (that should be in images/)  there's .html files whose extension should be changed to .htmli  and such  that would be nice to clean up.
s,In /etc/hive/conf/hive-site.xml&lt;property&gt; &lt;name&gt;hive.security.authorization.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;enable or disable the hive client authorization&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authorization.manager&lt;/name&gt; &lt;value&gt;org.apache.hcatalog.security.HdfsAuthorizationProvider&lt;/value&gt; &lt;description&gt;the hive client authorization manager class name. The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider. &lt;/description&gt;&lt;/property&gt;In /etc/hive/conf/hive-env.sh  add -export HIVE_AUX_JARS_PATH=/usr/lib/hcatalog/share/hcatalog/hcatalog-0.4.0.jar
ns,On the manage services page we have a section for client only services that have no long running processes.If the user has no such component there is a heading but no content. Maybe we can hide the heading when no such service is present.
s,Make the Naigos password (and re-type password) different so as you cause a validation error.This will let the user move on to the next screen by ignoring all other validation errors on this page.
ns,Nagios Admin Contact should be checked to ensure it is always an email address
ns,Content of /user/templeton dir-bash-3.2$ hadoop dfs -ls /user/templetonFound 4 items-rw-r--r-- 3 templeton hdfs 107373 2012-05-14 19:53 /user/templeton/hadoop-streaming.jar-rw------- 3 templeton hdfs 35352096 2012-05-14 19:53 /user/templeton/hive.tar.gz-rw------- 3 templeton hdfs 47909478 2012-05-14 19:53 /user/templeton/pig.tar.gz-rw------- 3 templeton hdfs 127652 2012-05-14 19:53 /user/templeton/ugi.jarOnly templeton user can use the jars
ns,Updating the short description and tooltip long description as follows:Changing the text that affects WebUI as follows: filesystem -&gt; file system HDFS Append Enabled -&gt; Append enabled HDFS WebHDFS Enabled -&gt; WebHDFS enabled Hadoop maximum Java heap size -&gt; Hadoop maximum Java heap size (MB)  Java Heap Size for slave daemons -&gt; Maximum Java heap size for daemons such as Balancer in MB (-Xmx)   NameNode maximum Java heap size -&gt; NameNode initial Java heap size (MB)  Java Heap Size for NameNode -&gt; Initial and minimum Java heap size for NameNode in MB (-Xms)   Hadoop Young Generation heap size -&gt; NameNode new generation size (MB)  Maximum size for New Generation for java heap size -&gt; Default size of Java new generation in MB for NameNode (-XX:NewSize)   DataNode Java heap size -&gt; DataNode maximum Java heap size (MB)  Java Heap Size for DataNode -&gt; Maximum Java heap size for DataNode in MB (-Xmx)
ns,We need an api for the UI to figure out the status of the cluster.
ns,Currently render of the review and deploy page is messy.
ns,Uninstall link is now hooked into the index page.
ns,Comment in addNodesWizardInit.js.
ns,HTML being spewed in the Review+Deploy page.
ns,Change the status message (success/error) location so that it shows below the page summary box  rather than above  more better visibility
ns,/etc/hbase/conf/regionserver should correctly populate the slaves list
ns,Upgrade to yui-3.5.1
ns,Externalize message resources for the welcome page.  Update styles on various pages.
ns,Uninstall's wipe flag should be correctly passed to puppet
ns,Grid mount points page doesn't let one pass with only a custom mount point
ns,Scale puppet master to large number of nodes.
ns,During any process in the cluster initialization wizard  if the user goes back to the '1 Create Cluster' tab  the user is stuck.
ns,Host level dependencies are being set in running stage as well  which is redundant. It can be assumed that dependencies were installed at install stage.
ns,Currently when the browser is closed one cannot view the install in progress. They will have to look at the logs on the hmc machine to figure out whats going on. It would be nice if we can provide a way to get back to the install progress.
ns,Artifacts (such as mysql-connector.zip  hive.tar.gz  pig.tar.gz  ext.zip) are being downloaded even though they are previously installed leading to additional execution time.
ns,Cluster status should be updated in db for each stage of the installation wizard. This is used to enable restart of browser and also showing status of the cluster on the index page.
ns,If the currentStage is null  we should not proceed with a transition.
ns,Logs disappear post uninstall because the transaction is also cleaned from the db.
ns,If no cluster has been set up yet  redirect to the welcome page.If a cluster is being configured (but has not gone thru deployment)  redirect to Step 1 of the cluster init wizard.If a cluster is being deployed  redirect to the deployment progress page.If a cluster has gone thru deployment but failed  redirect to the re-install page.If a cluster has gone thru deployment and succeed  do not perform any forced redirect.
ns,Redesign master service assignment page so that it takes up less vertical space
ns,Lock file is being created as part of create cluster. This is brittle and needs to be done as part of the rpm install.
ns,The new fonts/ subdirectory used for the buttons on the ManageServices page isn't packaged up.
ns,Improve Service Management page and general popup styling
ns,Mysql packages not being sent during install and uninstall.
ns,Uninstall does not handle component dependencies.
ns,Use lower count of maps/reduce slots for a single node install.
ns,ZooKeeper myid files not existent on ZK install.
ns,Host not found in db error on assign nodes page
ns,Cannot uninstall - the page hangs with the spinning icon
ns,Manual config changes for nn get reset on stop/start from hmc
ns,Completing successful add node takes one to initialize cluster page starting from scratch
ns,Unify the top nav for both Monitoring and Cluster Management
ns,Add rpm spec for hmc agent.
ns,Reset service back to original state after reconfiguration
ns,Improve style on error log popups
ns,According to Bikas:I tried to install a single node cluster. That failed for some random issue.I was presented an option to reinstall.I clicked on that option and it asked me to uninstall.I chose uninstall and wipe data option.Uninstall failed.This happened because the install did not complete in the first place.Thu May 24 17:45:28 -0400 2012 /Stage17/Hdp-oozie::Service/Hdp-oozie::Service::Createsymlinks/usr/lib/oozie/oozie-server/lib/mapred-site.xml/File/usr/lib/oozie/oozie-server/lib/mapred-site.xml/ensure (err): change from absent to present failed: Could not set 'present on ensure: No such file or directory - /usr/lib/oozie/oozie-server/lib/mapred-site.xml at /etc/puppet/agent/modules/hdp-oozie/manifests/service.pp:61
ns,Fix bug with jmx parsing on HBase.
ns,Using service stop instead of killall for uninstall
ns,This is to enable the routing layer to redirect appropriately.
ns,Duplicate definition: Class&#91;Hdp-hbase::Regionserver::Enable-ganglia&#93; is already defined; cannot redefine at /etc/puppet/agent/modules/hdp-ganglia/manifests/monitor.pp:37 on node ip-10-64-19-248.ec2.internal
ns,Install a cluster. Then go to http://&lt;host&gt;/hmc/html/initializeCluster.php URL. You get a page enter cluster name. If you enter a cluster name  existing install is wiped out. We need to disable this URL on an installed cluster.Only way to enable this should be uninstall.
ns,Boldify/Redify restart HMC message when nagios/ganglia is on the hmc host
ns,nagios shows service status critical if hbase is not installed
ns,Alerts that are being shown needs to be changed to be compatible with current implementation of showing errors. A link now appears to 'Show the duplicate nodes' which shows the duplicate nodes.
ns,Fix hive stop to escape $.
ns,Post-Install Add Nodes - update progress title and success/error messages to reflect what it's actually doing/has done
ns,Add missing JS file for making post cluster install Add Nodes work
ns,Show the same welcome page to the user if the user starts configuring a cluster but has not started deploy yet
ns,Adding slave nodes post cluster install will install MySQL Server if Hive was selected as a service at the time of cluster installation.
ns,We need to block users from being able to start/stop services when we have a batch of start/stop activities are already in progress.Do something similar to deployment progress display and bring the user back to that modal status display which will end when we detect success/failure.
ns,Some service config parameters are editable (can be customized on initial install)  but cannot be reconfigured post cluster install.This info is stored in ConfigProperties table  but the Service Reconfiguration screens are allowing these non-reconfigurable parameters to be changed.
ns,make support for os check a bit more robust
ns,Add rack_info as column in Hosts table
s,Fix node assignments not not allow slaves on master.
ns,Speed up page load/reload times
ns,Just like for Manage Services + Deploy + Uninstall.
ns,Modify the router to force redirection to 'Add Nodes Progress' popup
ns,Fix puppet manifests for tarball downloads via rpms.
ns,update to fix the ganglia monitor_and_server anchor problem
ns,Oozie smoke test failing with 'Error: E0301 : E0301: Invalid resource &#91;usr/lib/oozie/conf&#93;'
ns,Simplify user input.
ns,Simplify process for users that want to use different dependencies for PHP and Ruby.e.g PHP-5.3  ruby-1.8.7
ns,Rpm naming needs to be corrected.
ns,From Arpit:The configs for pig in templeton are wrong.The deployed configs are&lt;property&gt; &lt;name&gt;templeton.pig.archive&lt;/name&gt; &lt;value&gt;hdfs:///apps/templeton/&lt;/value&gt; &lt;description&gt;The path to the Pig archive.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;templeton.pig.path&lt;/name&gt; &lt;value&gt;/pig-0.9.2/bin/pig&lt;/value&gt; &lt;description&gt;The path to the Pig executable.&lt;/description&gt; &lt;/property&gt;They are missing the pig tar ball. For example they should be&lt;property&gt; &lt;name&gt;templeton.pig.archive&lt;/name&gt; &lt;value&gt;hdfs:///apps/templeton/pig.tar.gz&lt;/value&gt; &lt;description&gt;The path to the Pig archive.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;templeton.pig.path&lt;/name&gt; &lt;value&gt;pig.tar.gz/pig-0.9.2/bin/pig&lt;/value&gt; &lt;description&gt;The path to the Pig executable.&lt;/description&gt; &lt;/property&gt;So both the properties are missing 'pig-0.9.2.tar.gz'
ns,mkdir -p /usr/jdk32 ; chmod +x /tmp/HDP-artifacts//jdk-6u31-linux-i586.bin; cd /usr/jdk32 ; echo A | /tmp/HDP-artifacts//jdk-6u31-linux-i586.bin -noregister 2&gt;&amp;1Unpacking...Checksumming...Extracting.../var/www/html/downloads/jdk-6u31-linux-i586.bin: ./install.sfx.1794: /lib/ld-linux.so.2: bad ELF interpreter: No such file or directoryFailed to extract the files. Please refer to the Troubleshooting section ofthe Installation Instructions on the download page for more information.Puppet should ensure glibc.i686 is installed before trying to install jdk.
ns,hmc/php/frontend/addNodes/verifyAndUpdateNodesInfo.php hardcodes checks to only allow rhel5 or centos5 nodes.
ns,hmc/ShellScripts/puppet_agent_install.sh hardcodes to epel-release rpm for CentOS-5.4. Should be more intelligent to handle different OS types.
ns,Add support to jump to a specified state in the wizard for development purposes
ns,We need to fix the README to point to trunk after the merge of ambari-186 branch.
ns,Currently YUI 3.5.1 source files are in SVN.We don't really need to have them checked into SVN as we don't track changes. We are pre-concatenating and minifying the YUI js files  so we don't need the files to run Ambari anyhow.We should just remove them from SVN and instead just checkin the tarball.
ns,Modify pom.xml to create output files under ../site  rather than ./target.Make minor wording modifications.
ns,Puppet layer  when trying to install nagios  tries to install php-pecl-json when it is not needed and fails when trying to do so. On RHEL6  php-5.3 is default which has the json module built into it.
ns,Consolidate head tags for organization and combine CSS files for faster load
ns,Seems like a typo: $zookeeper_hosts = hdp_default('zookeeper_hosts')Should be $public_zookeeper_hosts
ns,In Custom config for Nagios: emails with multiple periods before the '@' fails validation
s,Currently the button looks disabled when there are client-side validation errors  but it is clickable. This is a problem because there is no server-side validation to make sure that the passwords match.
ns,special characters in hosts files created on some common windows editors causes issues
ns,Adding apache license to all the files within Ambari for rat tool compliance.
ns,The master role assignment page (Step 5 of install) and the cluster topology summary page are referencing the manager service name as 'HMC Server'. This needs to be changed to 'Ambari' server.The post-install success message on a single-node install has the same issue.Currently  the Help link in the top nav launches a new tab and loads the page that the user is on. Instead  load the Help page on Ambari project website (for now  we'll point it to the Install Guide as a placeholder).
s,Some PHP files have the HTML-style comments. This is problematic sincethe license headers are becoming part of the HTML response. Worse yet  the headers are repeated multiple times in the response. This can also cause unexpected behavior.
ns,This is so that stuff with different compatible licenses can be attributed appropriately.
ns,Remove /usr/bin/php dependency from the rpm's
ns,Fix lzo installs to work correctly on RHEL6
ns,Increase puppet timeouts to handle single-node installs timing out
ns,The database set up script has a duplicate definition of AmbariConfig so install fails
ns,ConfigProperties table has a column named 'display_type'.There's also a JSON-encoded 'display_attributes' column.The 'display_attributes' column has the attributes 'isPassword'  'displayType'  and 'noDisplay'. This is duplicate information is already stored by the 'display_type' column so these attributes are not needed.Upon inspecting the code  these attributes are not used so changing them have no effect. We should simply get rid of these attributes.
s,Suse environment has wrong configuration location for hdp-dashboard and hdp-nagios. Also  owner:group permissions were wrongly set to root:root instead of 'wwwrun' and group is 'www
ns,Fix invalid HTML markup on Monitoring Dashboard
ns,Add Nodes Progress: for partial failure that lets the user continue  display an orange bar rather than a red bar in the progress popup
s,Support for Hadoop Security (front-end changes)
s,Issue 1. Going back and forth between different stages in the Cluster Install Wizard  it is possible to get into a state where Custom Config form has the submit button disabled but no field errors are shown.Steps to replicate: Go up to Stage 6  but do not submit the form Go back to Stage 3 Go up to Stage 6 again. No field errors are shown when they should be.Issue 2. Custom Config stage is skipped once you get to 'Review and Deploy' and go back to a stage preceding Custom Config.Steps to replicate: Go thru the Cluster Install Wizard up to Stage 7 ('Review and Deploy')  but do not submit the form. Go back to Stage 4 or 5. Once you submit the form on Stage 5  Stage 6 is skipped and goes directly to Stage 7. If you go back to Stage 3 or earlier  then Stage 6 will not be skipped.
ns,Depending on which main hadoop repo is used to setup the cluster  sometimes  wrong packages may be pulled from add-on repos even if the main repo has a higher priority and has the same package. This can create problems by incompatible dependencies getting installed at times.
ns,Make puppet generate more logs on command failures
ns,Fix ambari agent init.d scripts and the bootstrapping.
ns,Ambari treats a Hive Metastore as something that it still needs to install and setup even though it may not have the necessary permissions to do so.
ns,Alerts should have text like :OK for about 17 hoursWARN for about 17 hoursCRIT for about 17 hours
ns,Sometimes disk capacity calculations end up in negative numbers.
ns,ambari-web/node_modules  ambari-web/public  ambari-web/ambari.iml were not meant to be checked in. Must remove.
ns,dashboard > Summary > capacity pie chart keeps changing colors
ns,Currently we dont fill anything which is ambiguous.
ns,When you switch services in the UI  sometimes alerts are mismatched compared to selected service. Also the highlights in left-bar are not working.
ns,Currently  if any errors are encountered during preparation for deploy  the user is taken to the deploy page and the hosts will be shown as 'Waiting' but nothing happens. This is bad UX.At a minimum  we should prevent the user from proceeding and display an appropriate error message if any error is encountered after 'Deploy' is clicked  but before we transition to Step 9.We should also think about how a user can recover from this situation.At this point  the deploy has not initiated  but certain API calls may have succeeded  so we may have incomplete info in the database. Currently there's no convenient way to 'rollback'.We can either ask the user to clean the slate by reinitializing the database and try again (should succeed if the original problem was temporary).We can also build more logic in the UI to retry  check if records already exist  etc...
ns,Need to be able to reliably recover from the case when the browser is closed during deploy (Step 8 post submission  Step 9) of the wizard.Even after submitting  Step 10  its taking to Step 9 after browser restart. Ideally it should take to monitoring page.
ns,User-specified custom configs (such as hdfs-site.xml overrides) should be persisted to maintain what the user specified
ns,With the install wizard went to assign slaves page successfully  returned back to Welcome page  reentered data and then the UI got stuck at Confirm hosts page.
ns,When datanode is stopped on a host  its status keeps jumping.
ns,When cluster is started after install  it has disk_total of null which results in Infinity.
ns,Leave a page with graphs open for several minutes.The graphs become really coarse.It probably has something to do with the fact that Ganglia only provides 6-minute averages beyond the first 61 minutes (first 61 minutes  Ganglia keeps 15-second samples) and we may not be exactly querying for the last 60 minutes.
ns,1) Cause notification to occur (for example  stop oozie)2) On dashboard  click notification icon3) On the notification popup  click 'go to nagios web UI'4) nothing happens.This issue is on IE9  Safari and Chrome. Works fine on Firefox.
ns,I saw this on a sles cluster. On EC2 they have a tmpfs mounted and Ambari picked it up.Not sure what the ideal solution is but i feel tmpfs should not be included in the available mount points.Also the tmpfs is being used in various directories that will have to change during the install.Attached screenshots.
ns,Installer Wizard - Retry feature in Deploy step (Step 9) is broken
s,Reconfigure fails silently; it's not firing any API calls due to a JS error
ns,Host jams in status 'Preparing' if host name is wrong
ns,The check boxes to check/uncheck one of the members in a multi artifact graphs is not very readable. It should be more apparent on which one the user clicked on
ns,Check the log/run dir locations to make sure its an abs path
s,Disk Info Metrics and memory usage sometimes do not show up for an hour or so.
ns,Clean up table header UI for sorting and filter clear 'x' for Hosts table
ns,Clean up table header UI for sorting and filter clear 'x' for Jobs table
ns,After adding hosts  the host count shown in the Dashboard is incorrect
ns,Detailed log view dialogs are not center-aligned
ns,For host-level popup info/logs that appear during the deploy step (Step 9) of the Install and Add Hosts Wizards  the information shown is not automatically updated based on the latest values in the models when the background poller updates the models.Currently  the user needs to close the popup and then re-open to see the updated info.
ns,Remove /hdp as the httpd conf for any of the nagios urls - should replace it with ambarinagios or something else.
ns,Allow capacity scheduler to be attached to host role configs for CS configurability in the API's.
ns,If any service start fails  'warn' the host and 'warn' the overall install.Allow the other start tasks to complete.Allow the user to click next.
ns,Add filters module
ns,Refactor Job Browser User filter
ns,Confirm Hosts page: It looks like hosts disappear if you are on 'Fail' filter and click on 'Retry Failed' button
ns,Drop the 'all' option from Hosts > Component Filter and Jobs > Users Filter
ns,Dashboard - make disk usage pie chart in HDFS summary easier to understand
ns,Replace sudo with su in the ambari setup script since ambari server setup is already run as root.
s,Directory permissions on httpd /var/www/cgi-bin should not be touched by Ambari.
ns,Host health indicator should have a tooltip showing details
ns,Install Options - line up the Target Hosts section with the rest of the page
ns,Use the Ember Handlebars precompiler plugin (ember-precompiler-brunch) that's in the NPM registry so that we can specify a specific plugin version to use in package.json  rather than the current plugin that Ambari Web uses (which is not in the NPM registry - it is retrieved from a git repo and Ambari Web can break if the plugin gets updated).
ns,Fix the host roles live status not go back to INSTALLED if it was in START_FAILED state.
ns,The JMXPropertyProvider contains a map of component names to ports ... JMX_PORTS.put('NAMENODE'  '50070'); JMX_PORTS.put('DATANODE'  '50075'); JMX_PORTS.put('JOBTRACKER'  '50030'); JMX_PORTS.put('TASKTRACKER'  '50060'); JMX_PORTS.put('HBASE_MASTER'  '60010');These ports can change in configuration. Need to create the mapping dynamically.This is required for secure HDP cluster to work.
ns,Service graphs are refreshing with spinners  rather than simply shifting to the left.See the attached movie clip.
ns,&lt;expression&gt; ::= [&lt;operator&gt;] [&lt;whitespace&gt;]* &lt;value&gt; [&lt;duration-unit&gt; | &lt;io-unit&gt;]&lt;operator&gt; ::= '&gt;' | '&lt;' | '='&lt;whitespace&gt; ::= ' '&lt;value&gt; :== int | float&lt;duration-unit&gt; ::= 's' | 'm' | 'h'&lt;io-unit&gt; ::= 'k' | 'kb' | 'm' | 'mb' | 'g' | 'gb'If &lt;operator&gt; is ommitted  '=' is assumed.If &lt;duration-unit&gt; is omitted  's' is assumed.If &lt;io-unit&gt; is omitted  'k' is assumed.&lt;duration-unit&gt; and &lt;io-unit&gt; are case-insensitive.
ns,Agent checks packages as part of host check but doesn't tell which ones are needed or conflicting
ns,Store example Hive Queries somewhere in Ambari that's easily accessible for demo/test purposes
ns,On Confirm Hosts page  add a link to show the Host Checks popup in the success message
s,No error message is shown when the user does not enter the correct 'old password' when editing an user.
ns,Navbar's height should be shrunk to ~40px.
ns,Steps:1. Change state (start/stop operations are preferable) for lot of components so that the number of background operations was approximately 20-30;2. Change size of browser (800-900 px);3. Go to down of page;4. Change state for next component;5. Confirm changing;Result:'Background operations' window was opened  but it 'OK' button is not visible.Expected result:'Background operations' window contains scroll bar for all background operations. 'OK' button is available for using.
ns,Failing build due to url moved on Suse. Looks like centos5 and 6 handle redirection all fine but doesnt look like the maven plugin handles that on SUSE.
ns,Current behavior is to dynamically layout elements as the browser viewport size changes. This causes elements to overlap  go outside the bounding box  positioned in a weird way  etc.Let's set minimum width to be 1024px (or maybe 980px to account for the scrollbar) and make sure that all elements are laid out correctly.Bigger than 1024px should still utilize more screen space as it does today.
ns,I was bootstrapping 4 nodes. One of them got stuck in 'Installing' phase and won't time out even after ~30 minutes.It seems like starting of ambari-agent was hanging.
ns,This happens when a service/host component is running  but stop fails. This leaves the desired_state in the INSTALLED (i.e.  STOPPED) state  while the live state is STARTED.Currently  UI assumes when desired_state==INSTALLED and state==STARTED  it is STARTING. This was a trick to get around the problem of backend live state update lagging and to make UI more responsive.
ns,Current behavior is to dynamically layout elements as the browser viewport size changes. This causes elements to overlap  go outside the bounding box  positioned in a weird way  etc.Let's set minimum width to be 1024px (or maybe 980px to account for the scrollbar) and make sure that all elements are laid out correctly.Bigger than 1024px should still utilize more screen space as it does today.
ns,Properly display Apps page aggregate summary and data table when there are no data to be shown
ns,Remove all text from Apps views  controllers  templates to messages.js
ns,Host health indicator should have a tooltip showing few details (refactoring)
ns,On the jobs page click on Job-X and see the popup with all the job information. Now switch to Services page and come back to the jobs page. Now clicking on Job-X will not launch popup.
ns,Cluster missing hosts after successful install and restart. This bug got introduced due to my patch in AMBARI-1301.
s,Add username validation for Ambari local users
ns,Show validation error when the user specifies target hosts that are already part of the cluster
ns,Refactor Job Browser filter
ns,When we are showing progress for cluster install  it takes a while for any of the host-level or overall progress to show any progress completion percentage (they can get stuck at 0% for several minutes). This is because host-level completion is not displayed until the first install task for the task is complete. Instead  we should advance percentage complete when the tasks changes the status (from PENDING-&gt;QUEUED-&gt;IN_PROGRESS) to better reflect the install progress to the end user.
ns,For manually installed kdc  the kinit path in the puppet script is /usr/kerberos/bin/kinitActual location:root@ip-10-38-13-250 data]# whereis kinitkinit: /usr/bin/kiniterr: /Stage&#91;2&#93;/Hdp-hbase::Hbase::Service_check/Hdp-hadoop::Exec-hadoop&#91;hbase::service_check::test&#93;/Hdp::Exec&#91;/usr/kerberos/bin/kinit -kt /etc/security/keytabs/hdfs.headless.keytab hdfs; hadoop --config /etc/hadoop/conf fs -test -e /apps/hbase/data/usertable&#93;/Exec&#91;/usr/kerberos/bin/kinit -kt /etc/security/keytabs/hdfs.headless.keytab hdfs; hadoop --config /etc/hadoop/conf fs -test -e /apps/hbase/data/usertable&#93;: Failed to call refresh: Could not find command '/usr/kerberos/bin/kinit'
ns,Externalize string resources to messages.js.
ns,Provide consistent ordering of hosts in heatmap
ns,'No alerts' badge on the Host Detail page should be green  not red
ns,App Browser rows colours should alternate from dark grey to light grey and back
ns,When hovering the mouse cursor around Show All  Filtered  the mouse cursor changes to the 'hand' icon as expected. To the right  the cursor turns into a hand even when hovering over areas where there's no link. Hovering over 'Clear filters'  the cursor does not turn into the 'hand'.
s,Since there is the ability to log in to Ambari Web as different users the current user should be indicated
ns,Wrong calculation for the duration filter if we enter just number  without h  m or s to specify the unit. By default we should take seconds as the default unit.
ns,Ambari Agent registration hangs due to Acceptor bug in Jetty for not reading through accepted connections.
s,Validation for username used in service configs is broken
ns,Failure popup shown for reconfiguring HDFS when MapReduce is not installed
ns,Remove hard-coded stack version
ns,Cannot proceed after bootstrapping in some cases due to a run-time error while running host checks
ns,Every 15 seconds  Ambari Web makes a call to retrieve information about all hosts in the cluster. With 400+ nodes  this call retrieves about 10MB of info  since the API returns last_agent_env  which is host check results used during bootstrap and makes the payload huge.By optimizing the query string for this API call  we can cut down on the payload by more than 90%.
ns,The API response for getting service/host component status back from the server is unnecessarily big due to nagios_alerts being included as part of the response. This optimization can cut the payload in half or more  and also eases load on the server since it does not have to get Nagios alerts as part of fulfilling the API call.
ns,Invocation count and exec time for ClustersImpl.mapHostToCluser very highorg.apache.ambari.server.state.cluster.ClustersImpl.mapHostToCluster(String  String) Time(ms): 252 925 Avg Time(ms): 5 269 Own Time(ms): 211Invocation Count: 48Each time host is mapped to cluster we refresh the entity manager. This results in the createHosts call taking excess of 10 minutes
ns,Optimize ganglia rrd script to be able to respond within reasonable time to queries made by the UI.
ns,Further work on optimizing query for getting host information on top of BUG-1460.
ns,Fix TestHostName to take care of issues when gethostname and getfqdn do not match.
ns,Fix alerts at host level if MapReduce is not selected not to alert for tasktrackers not running.
ns,Nagios script causes unwanted Datanode logs.
ns,Add hadoop-lzo to be one of the rpms to check for before installation.
ns,Make all service properties reconfigurable.
ns,Fix start up option for ambari-server where there is a missing space.
ns,This is due to performance enhancements for querying hosts.Hosts/disk_info for the hosts being added is expected in the Add Hosts wizard  but it is now missing.
ns,Show the host status filter at the top of the Hosts table  showing the status and the number of hosts in that status.
ns,The background polling to update the live status of services and host components runs every 6 seconds. When this happens  the whole UI freezes for several seconds periodically.
ns,Alerts take around 20-30 seconds to show up everytime you refresh the dashboard.
ns,1.Use Bootstrap-style tooltip for the hover tooltip on the host status dots as well as the disk usage bars. The current tooltip does not show up unless you rest your mouse cursor for more than a second - users will simply assume there's no tooltip hover when quickly moving the mouse cursor over them.2.We can kill some space between the status dots and the hostnames in the table as well.3.Also  there is too much margin at the top compared to the rest of the pages.
ns,Currently  the popup for any wizard resizes to fill up the width of the browser width with some horizontal margin. Vertically  it resizes to fit the height of the content; in some cases  it looks really silly on a wide screen.We should specify the max width of the wizard popup. Ambari Web supports two width configurations (wide and narrow)  so the wizard popup should look good in these two configurations.
ns,See the attachments  but after running both successful and failed MapReduce jobs  the jobs submitted count includes all jobs  the jobs successful appears correct  but the jobs failed count is still 0.
ns,Stack Upgrade Wizard - resume upon page refresh / login
ns,This is for the popup that shows up upon clicking already-performed or currently-in-progress async operations.We should take what we already have for host-level popup in Step 9 as a base and make it reusable  with a new higher level showing all hosts.
ns,Cannot start hadoop services after several restarts since the agents.
s,Templeton start fails due to multiple errors.[0;36mnotice: /Stage&#91;2&#93;/Hdp-templeton::Copy-hdfs-directories/Hdp-hadoop::Hdfs::Copyfromlocal&#91;/usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar&#93;/Hdp-hadoop::Exec-hadoop&#91;fs -copyFromLocal /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar /apps/webhcat/hadoop-streaming.jar&#93;/Hdp::Exec&#91;/usr/bin/kinit -kt /etc/security/keytabs/hcat.headless.keytab hcat; hadoop --config /etc/hadoop/conf fs -copyFromLocal /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar /apps/webhcat/hadoop-streaming.jar&#93;/Exec&#91;/usr/bin/kinit -kt /etc/security/keytabs/hcat.headless.keytab hcat; hadoop --config /etc/hadoop/conf fs -copyFromLocal /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar /apps/webhcat/hadoop-streaming.jar&#93;/returns: kinit: Client not found in Kerberos database while getting initial credentials^[[0m
ns,Config/Reconfig UI should not allow certain configs to have host-level overrides
ns,Security Wizard - integrate host progress popup
s,With Ambari 1.2.2  I cant get the metrics.mapred object to show up for the tasktracker component. Our code is hitting the URL: http://sdll4474.labs.teradata.com:8080/api/v1/clusters/sdll4474.labs.teradata.com/services/MAPREDUCE/components/TASKTRACKER?fields=host_components/*. Heres one of the objects in the host_components array. Note that the data in metrics.mapred.tasktracker is providing some data Ive never seen before. In previous versions we say properties such as reduces_running  reduceTaskSlots  maps_running  etc. in this object. { 'href' : 'http://aster39h1.td.teradata.com:8080/api/v1/clusters/aster39h1.td.teradata.com/hosts/byn001-17/host_components/TASKTRACKER'  'HostRoles' : { 'cluster_name' : 'aster39h1.td.teradata.com'  'desired_state' : 'STARTED'  'state' : 'STARTED'  'component_name' : 'TASKTRACKER'  'service_name' : 'MAPREDUCE'  'host_name' : 'byn001-17' }  'metrics' : { 'boottime' : 1.360089758E9  'process' : { 'proc_total' : 845.211111111  'proc_run' : 0.0 }  'rpc' : { 'rpcAuthorizationSuccesses' : 9  'SentBytes' : 6842  'rpcAuthorizationFailures' : 0  'ReceivedBytes' : 26187  'NumOpenConnections' : 0  'callQueueLen' : 0  'RpcQueueTime_num_ops' : 59  'rpcAuthenticationSuccesses' : 0  'RpcProcessingTime_num_ops' : 59  'rpcAuthenticationFailures' : 0  'RpcProcessingTime_avg_time' : 0.6666666666666666  'RpcQueueTime_avg_time' : 0.0 }  'mapred' : { 'shuffleOutput' : { 'shuffle_success_outputs' : 1  'shuffle_handler_busy_percent' : 0.0  'shuffle_output_bytes' : 1400  'shuffle_failed_outputs' : 0  'shuffle_exceptions_caught' : 0 }  'tasktracker' : { 'ConfigVersion' : 'default'  'HttpPort' : 50060  'TasksInfoJson' : '{/'running/':0 /'failed/':0 /'commit_pending/':0}'  'JobTrackerUrl' : 'aster39h1.td.teradata.com:50300'  'Healthy' : true  'Version' : '1.1.2.22  r'  'Hostname' : 'byn001-17'  'RpcPort' : 48526 } }  'ugi' : { 'loginFailure_num_ops' : 0  'loginSuccess_num_ops' : 0  'loginSuccess_avg_time' : 0.0  'loginFailure_avg_time' : 0.0 }  'disk' : { 'disk_total' : 36841.767  'disk_free' : 36776.9775333  'part_max_used' : 70.7 }  'cpu' : { 'cpu_speed' : 1999.0  'cpu_wio' : 0.0  'cpu_num' : 24.0  'cpu_idle' : 99.8886111111  'cpu_nice' : 0.0  'cpu_aidle' : 0.0  'cpu_system' : 0.1  'cpu_user' : 0.0227777777778 }  'rpcdetailed' : { 'getTask_avg_time' : 1.0  'ping_avg_time' : 0.0  'done_avg_time' : 1.0  'getProtocolVersion_avg_time' : 0.0  'getMapCompletionEvents_avg_time' : 0.0  'done_num_ops' : 9  'getMapCompletionEvents_num_ops' : 6  'canCommit_num_ops' : 6  'ping_num_ops' : 2  'commitPending_avg_time' : 1.0  'statusUpdate_num_ops' : 15  'statusUpdate_avg_time' : 1.0  'getTask_num_ops' : 9  'getProtocolVersion_num_ops' : 9  'commitPending_num_ops' : 3  'canCommit_avg_time' : 0.0 }  'load' : { 'load_fifteen' : 0.0  'load_one' : 0.0  'load_five' : 0.0 }  'jvm' : { 'memHeapCommittedM' : 100.4375  'NonHeapMemoryUsed' : 25214472  'logFatal' : 0  'threadsWaiting' : 17  'gcCount' : 122400  'threadsBlocked' : 0  'HeapMemoryUsed' : 77617416  'logWarn' : 0  'logError' : 0  'HeapMemoryMax' : 954466304  'memNonHeapCommittedM' : 26.125  'memNonHeapUsedM' : 24.046394  'gcTimeMillis' : 81352  'NonHeapMemoryMax' : 136314880  'logInfo' : 3  'memHeapUsedM' : 73.81818  'threadsNew' : 0  'threadsTerminated' : 0  'maxMemoryM' : 758.4375  'threadsTimedWaiting' : 10  'threadsRunnable' : 6 }  'network' : { 'pkts_out' : 111.684472222  'bytes_in' : 1428.83666667  'bytes_out' : 23201.8668056  'pkts_in' : 13.9853333333 }  'memory' : { 'mem_total' : 1.31854096E8  'swap_free' : 6291448.0  'mem_buffers' : 574794.711111  'mem_shared' : 0.0  'swap_total' : 6291448.0  'mem_cached' : 6061952.85556  'mem_free' : 1.23072573378E8 } }  'component' : [ { 'href' : 'http://aster39h1.td.teradata.com:8080/api/v1/clusters/aster39h1.td.teradata.com/services/MAPREDUCE/components/TASKTRACKER'  'ServiceComponentInfo' : { 'cluster_name' : 'aster39h1.td.teradata.com'  'component_name' : 'TASKTRACKER'  'service_name' : 'MAPREDUCE' } } ] }
ns,Undo should not be allowed on component hosts
ns,Oozie principal is create by the UI as oozie/${hostname}@realm.comPuppet script has a bug that does not read this property and uses default 'oozie' as the principal
ns,HDFS service check failure leads to this.After the failure of the 'HDFS service check' task  stage fails. But HDFS comes up.The Ambari server hostname for secure cluster: ec2-54-234-164-5.compute-1.amazonaws.com
ns,mapred.jobtracker.completeuserjobs.maximum is currently set to 0. This causes issues with failed(/successful) jobs from pig and other job submitters because the references to the failed job is cleared up asap in jobtracker preventing one from accessing the failure reason. This value should be bumped up to about 100 according to the MapReduce team.
ns,Upgrade requires agents to download the repo file anytime.
ns,In order to test upgrade  I modified the available stack definitions to allow upgrade from 1.2.0 to 1.2.2 and removed upgrade to 1.3.0. However  the FE still says '(Upgrade available: HDP-1.3.0)'. However  http://server:8080/api/v1/stacks2/HDP/versions/1.3.0/ has min_upgrade_version as null.{ 'href' : 'http://server:8080/api/v1/stacks2/HDP/versions/1.3.0/'  'Versions' : { 'stack_version' : '1.3.0'  'stack_name' : 'HDP'  'min_upgrade_version' : null } ...
ns,Error during starting hbase and zookeeper during secure install.Incorrectly parsed Files:hbase-env.shzookeeper-env.sh
ns,Add ability to use Oracle DB for Hive and Oozie in Ambari
ns,Add support for Stack 1.2.2 to Ambari.
ns,When trying to get tasks from more than one request_id returns tasks only for one.request 'api/v1/clusters/mycluster/requests?Requests/id=1|Requests/id=2'returns:{'href' : 'http://ec2-23-20-223-127.compute-1.amazonaws.com:8080/api/v1/clusters/mycluster/requests?Requests/id=1|Requests/id=2' 'items' : [{'href' : 'http://ec2-23-20-223-127.compute-1.amazonaws.com:8080/api/v1/clusters/mycluster/requests/2' 'Requests' :{ 'id' : 2  'cluster_name' : 'mycluster' }}]}
ns,Hue installation fails due to following errors:1. Change in the name of rpm bundle2. Empty values in the hue.ini sections
ns,Security wizard - Javascript error is thrown when zooKeeper is included as a secure service.
ns,Security wizard stops all services  applies configuration and then starts all services.Sometimes it has been noticed that the action to stop all service completes successfully but the action to start all services never sends the task to start NameNode.
ns,Regarding BUG-3509 when we send request to server likeapi/v1/clusters/cl1/services/HDFS/actions/HDFS_SERVICE_CHECKRequest Method:POST Form Data:{'RequestInfo':{'context':'Smoke Test'}}This request is not setting request_context.
s,Security wizard: Add missing secure configs to Hbase service and make 'zookeeper' as default primary name for zookeeper principal.
ns,HBase master shuts down immediately after start in a secure cluster.Wrong settings in the hbase_master_jaas. Need to replace the 'HOST with actual fqdn
ns,UI metadata properties are not being reconfigured and retained on saving services.
ns,Cluster Management > Services > MapReduce > Config throws JS error and the page comes up blank
ns,Currently when one service changes a property which doesnt effect all other services  the rest of the services are marked for restart. This is because the global tags are changed. Using work done in AMBARI-1797  only appropriate services should be marked as needing restart.
ns,Currently when we save host-overrides configuration  we have to do PUT of the delta on each host.This is problematic if admin provides an exception to 100 hosts. This will require 100 PUT calls which is expensive. There is a bulk update mechanism  but that requires the same content for all 100 hosts. This will not be the case if any hosts have other properties that are overridden.To save on network calls  we have to make one PUT call via the new API provided by AMBARI-1844.
ns,Cosmetic problems on HBase Dashboard
ns,When creating a new queue  populate all fields with default values except for:Queue NameCapacityMax CapacityUsersGroupsAdmin UsersAdmin Groups
s,Given the API call: http://dev.hortonworks.com:8080/api/v1/clusters/test403_2/host_components?HostRoles/component_name=NAGIOS_SERVER&amp;fields=HostRoles/nagios_alertsThe feedback is:{ 'status' : 400  'message' : 'The properties [HostRoles/nagios_alerts] specified in the request or predicate are not supported for the resource type HostComponent.' }
ns,Datanode install fails on multi-node clusters because the following configuration property:$ambari_db_rca_url= 'jdbc:postgresql://localhost/ambarirca'Ensure all of these variables are wired up:'ambari_db_rca_url''ambari_db_rca_driver''ambari_db_rca_username''ambari_db_rca_password'
ns,This may impact other versions  only branch-1.2 was changed.The ambari-agent.spec (generated from rpm-maven-plugin) claims ownership of /usr/sbin $ grep sbin target/rpm/ambari-agent/SPECS/ambari-agent.spec | grep attr%attr(755 root root) /usr/sbinThis is a problem because the filesystem RPM owns /usr/sbin.According to rpm-maven-plugin documentation&#91;0&#93;  this is because the only file under /usr/sbin is ambari-agent and'directoryIncludedIf the value is true then the attribute string will be written for the directory if the sources identify all of the files in the directory (that is  no other mapping contributed files to the directory). This is the default behavior.'The 'no other mapping contributed files to the directory' bit is important.The solution is to add directoryInclude=false to the mapping.&#91;0&#93; http://mojo.codehaus.org/rpm-maven-plugin/map-params.html
ns,Ambari FE is not setting proper value for fs.checkpoint.edits.dir
ns,HUE pid and log dir labels are flip flopped
ns,Capacity Scheduler: implement user/group and admin user/group validation rules
ns,Reassign Master Wizard  Step 2: prevent proceed next without changing target host
ns,FE needs to be modified to not to use 'filename' in stacks API to get the config tags. The new property is 'type'.
ns,API to map global properties to services is partially complete. The API gives 'global.xml' to approximately 47 properties. However the 'global' site has many more properties - so I dont know if some are fully missed.
ns,Open detailed view of any metric diagram on Services page  for example 'Total Space Utilization'. After that change width of browser less than width of detailed view of metric diagram. Right arrow for graph time paging is invisible and we can't to resolve this problem using horizontal scrollbar.
s,Go to Configs tab of Host.Result: Queues in Capacity Scheduler category are editable.Expected: Queues in Capacity Scheduler category shouldn't be editable.
ns,Update default stack version to 1.3.0
ns,HBase master doesn't come up after disabling security.
ns,Client is re-configured by re-installing all client only hosts. This results in multiple tasks for client install in the UI.API response:{ 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2'  'Requests' : { 'id' : 2  'cluster_name' : 'yusaku' }  'tasks' : [ { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/43'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: /Stage[2]/Hdp-oozie/Configgenerator::Configfile[oozie-site]/File[/etc/oozie/conf/oozie-site.xml]/content: content changed '{md5}eaf59cc452c92e64b559071586150a08' to '{md5}cb15303aab1c384d19c102a6ce650ed2'/nnotice: Finished catalog run in 2.04 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 43  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'OOZIE_CLIENT'  'start_time' : 1365743356709  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/51'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 51  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'GANGLIA_MONITOR'  'start_time' : 1365743407043  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/41'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: Finished catalog run in 2.22 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 41  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'HCAT'  'start_time' : 1365743356684  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/54'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 54  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'GANGLIA_SERVER'  'start_time' : 1365743407078  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/55'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 55  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'NAGIOS_SERVER'  'start_time' : 1365743407100  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/50'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 50  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'DATANODE'  'start_time' : 1365743407032  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/67'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 67  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'HBASE_REGIONSERVER'  'start_time' : -1  'stage_id' : 4 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/47'  'Tasks' : { 'exit_code' : 0  'stdout' : 'warning: Dynamic lookup of $hadoop_heapsize is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nnotice: /Stage[2]/Hdp-hive/Configgenerator::Configfile[hive-site]/File[/etc/hive/conf/hive-site.xml]/content: content changed '{md5}29d1def766d4aadfddbf38db13a2712e' to '{md5}2d26829fd012bf5f195e760fc8eeb7f9'/nnotice: Finished catalog run in 2.84 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 47  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'HIVE_CLIENT'  'start_time' : 1365743356758  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/45'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: Finished catalog run in 2.24 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 45  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'SQOOP'  'start_time' : 1365743356733  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/44'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: Finished catalog run in 2.13 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 44  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'PIG'  'start_time' : 1365743356721  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/56'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 56  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'NAMENODE'  'start_time' : 1365743407109  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/52'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 52  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'ZOOKEEPER_SERVER'  'start_time' : 1365743407062  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/42'  'Tasks' : { 'exit_code' : 0  'stdout' : 'warning: Dynamic lookup of $hadoop_heapsize is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nnotice: /Stage[2]/Hdp-hive/Configgenerator::Configfile[hive-site]/File[/etc/hive/conf/hive-site.xml]/content: content changed '{md5}27e517fec40f6157f75eb3116d5387bf' to '{md5}54ff14d0a6d9968e900f28853125b294'/nnotice: Finished catalog run in 2.28 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 42  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'HIVE_CLIENT'  'start_time' : 1365743356697  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/63'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 63  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'HBASE_MASTER'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/62'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 62  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'TASKTRACKER'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/46'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: Finished catalog run in 2.77 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 46  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'HCAT'  'start_time' : 1365743356744  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/58'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 58  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'GANGLIA_MONITOR'  'start_time' : 1365743407125  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/60'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 60  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'ZOOKEEPER_SERVER'  'start_time' : 1365743407164  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/69'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 69  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'OOZIE_SERVER'  'start_time' : -1  'stage_id' : 4 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/64'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 64  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'HIVE_METASTORE'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/48'  'Tasks' : { 'exit_code' : 0  'stdout' : 'warning: Dynamic lookup of $service_state at /var/lib/ambari-agent/puppet/modules/hdp-hadoop/manifests/init.pp:213 is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nwarning: Dynamic lookup of $tasktracker_port is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nwarning: Dynamic lookup of $ambari_db_rca_url is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nwarning: Dynamic lookup of $ambari_db_rca_driver is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nwarning: Dynamic lookup of $ambari_db_rca_username is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nwarning: Dynamic lookup of $ambari_db_rca_password is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nnotice: /Stage[2]/Hdp-hadoop::Initialize/Configgenerator::Configfile[core-site]/File[/etc/hadoop/conf/core-site.xml]/content: content changed '{md5}95bdcddd064261ac3a00d8c0a7f79fee' to '{md5}d6f5b9646bf280e915e3b0d42ed622a9'/nnotice: /Stage[2]/Hdp-hadoop::Initialize/Configgenerator::Configfile[hdfs-site]/File[/etc/hadoop/conf/hdfs-site.xml]/content: content changed '{md5}5f83b57cbac46a0b7007ed94720a8c3b' to '{md5}e0e38c4dc10fc81b12637e34796ced70'/nnotice: /Stage[2]/Hdp-hadoop::Initialize/Configgenerator::Configfile[mapred-site]/File[/etc/hadoop/conf/mapred-site.xml]/content: content changed '{md5}42b54b8e096eafa7ba53c8f5b53bda3e' to '{md5}409f4680bc7871db2864afecbcefdb36'/nnotice: Finished catalog run in 3.67 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 48  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'MAPREDUCE_CLIENT'  'start_time' : 1365743356769  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/70'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 70  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'WEBHCAT_SERVER'  'start_time' : -1  'stage_id' : 5 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/61'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 61  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'HBASE_MASTER'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/66'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 66  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'SECONDARY_NAMENODE'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/49'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: /Stage[2]/Hdp-oozie/Configgenerator::Configfile[oozie-site]/File[/etc/oozie/conf/oozie-site.xml]/content: content changed '{md5}001c4940080d2ea315a9720676f1bcad' to '{md5}282f1e354fe7f9da0f6de43425a40d40'/nnotice: Finished catalog run in 2.24 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 49  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'OOZIE_CLIENT'  'start_time' : 1365743356796  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/53'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 53  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'GANGLIA_MONITOR'  'start_time' : 1365743407070  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/68'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 68  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'HIVE_SERVER'  'start_time' : -1  'stage_id' : 4 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/65'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 65  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'JOBTRACKER'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/57'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 57  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'ZOOKEEPER_SERVER'  'start_time' : 1365743407117  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/59'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 59  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'MYSQL_SERVER'  'start_time' : 1365743407156  'stage_id' : 2 } } ]}
ns,Ambari Core-Site.xml Missing Property for LZO (enabled) - io.compression.codecs
ns,Attempted to install a cluster with 1.3.0 stack.Service install was all green  but JobTracker History Server failed to come up.The request for the service start is stalled  with no tasks in QUEUED or IN_PROGRESS state  but with some tasks in PENDING state.
ns,Allow for customers to install the users used for Nagios.
ns,Allow customization of ganglia gmetad and gmond users.For reference: looks like this is available in gsInstaller so the pattern exists to follow for Ambari impl. Defaults to nobody/nobody
ns,mvn clean install produces the following failure ...testCascadeDeleteStages(org.apache.ambari.server.actionmanager.TestActionManager):Exception [EclipseLink-4002] (Eclipse Persistence Services -2.4.0.v20120608-r11652):org.eclipse.persistence.exceptions.DatabaseException(..)
s,Ganglia Issue : Unspecified vulnerability in Ganglia Web before 3.5.1 allows remote attackers to execute arbitrary PHP code via unknown attack vectors. http://ganglia.info/?p=549 Ganglia Web 3.5.1 Release ?ESecurity Advisory There is a security issue in Ganglia Web going back to at least 3.1.7 which can lead to arbitrary script being executed with web user privileges possibly leading to a machine compromise. Issue has been fixed in the latest version of Ganglia Web which can be downloaded from https://sourceforge.net/projects/ganglia/files/ganglia-web/3.5.1/ Solution: Need to get upgraded rpms with the Ganglia Web version 3.5.7 which has the fix for this vulnerability.Nagios: Multiple stack-based buffer overflows in the get_history function in history.cgi in Nagios Core before 3.4.4  and Icinga 1.6.x before 1.6.2  1.7.x before 1.7.4  and 1.8.x before 1.8.4  might allow remote attackers to execute arbitrary code via a long (1) host_name variable (host parameter) or (2) svc_description variable. http://www.nagios.org/projects/nagioscore/history/core-3x http://lists.grok.org.uk/pipermail/full-disclosure/2012-December/089125.html Vulnerable software and versions - nagios:nagios:3.4.3 and previous versions
ns,has_key(): expects the first argument to be a hash  got '' which is of type String at /var/lib/ambari-agent/puppet/modules/hdp/manifests/init.pp:38 on node ip-10-38-25-227.ec2.internalsite-pp#12.04.2013 03:16:38import '/var/lib/ambari-agent/puppet/modules/hdp/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-hadoop/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-hbase/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-zookeeper/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-oozie/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-pig/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-sqoop/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-templeton/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-hive/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-hcat/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-mysql/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-monitor-webserver/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-repos/manifests/*.pp'$ambari_db_rca_password= ['mapred']$nagios_server_host= ['ip-10-38-25-227.ec2.internal']$ambari_db_rca_url= ['jdbc:postgresql://ip-10-38-25-227.ec2.internal/ambarirca']$webhcat_server_host= ['ip-10-38-25-227.ec2.internal']$hbase_rs_hosts= ['ip-10-38-25-227.ec2.internal']$slave_hosts= ['ip-10-38-25-227.ec2.internal']$namenode_host= ['ip-10-38-25-227.ec2.internal']$ganglia_server_host= ['ip-10-38-25-227.ec2.internal']$hbase_master_hosts= ['ip-10-38-25-227.ec2.internal']$hive_mysql_host= ['ip-10-38-25-227.ec2.internal']$oozie_server= ['ip-10-38-25-227.ec2.internal']$ambari_db_rca_driver= ['org.postgresql.Driver']$zookeeper_hosts= ['ip-10-38-25-227.ec2.internal']$jtnode_host= ['ip-10-38-25-227.ec2.internal']$ambari_db_rca_username= ['mapred']$hive_server_host= ['ip-10-38-25-227.ec2.internal']node /default/ { stage{1 :} -&gt; stage{2 :}class {'hdp': stage =&gt; 1}class {'hdp-zookeeper::quorum::service_check': stage =&gt; 2}}
ns,Ozzie smoke tests fail
ns,Run a mapreduce job.Find the attempt id and look for system logs on tasktracker.http://ec2-54-224-138-78.compute-1.amazonaws.com:50060/tasklog?attemptid=attempt_201304121816_0003_m_000000_0Actual result:The syslogs are not present here.Only stdout and stderr logs are present.
ns,Ambari-log4j has hardcoded hadoop-core and hadoop-tools dependency. Make it version as regular expression to pick from the range from 1.0 &lt;= x &lt; 2.0.Also  updating the repository url.
ns,Upon master component install failure  the host status becomes 'warning' instead of 'failed' for Add Service Wizard.
ns,Currently  when the red badge inside the Hosts tab is clicked  it shows hosts that have at least one alert and no other hosts are displayed.The fact that the alert filter is in effect is not clear to the user and causes confusion.
ns,Client install tasks are shown twice in progress popup during start phase of install wizard (update API call to include params/reconfigure_client)
ns,TaskTracker  RegionServer  HBase master process down because check_tcp failure.Looks like the hadoop-services.cfg.erb has:check_command check_tcp!&lt;%=scope.function_hdp_template_var('jtnode_port')%&gt;!-w 1 -c 1and looks like the ports are not getting replaced and end up being empty.
ns,NAGIOS_SERVER alerts are retrieved from the nagios server when requesting the host_component. There is a SystemException thrown in the case of an IOException  which propagates as a 500 error for the entire request.In this case  set the nagios_alerts element to null instead of the 500 error.
ns,dev[01-03].domain.com expanded to:dev1.domain.comdev2.domain.comdev3.domain.comShould be:dev01.domain.comdev02.domain.comdev03.domain.com
ns,1. Browse to Hosts2. Click one of the filters3. Browse to Services and back to Hosts4. The filter links show All as the current filter but the filter didn't reset and still shows a sub-set of hosts
ns,Action buttons on host details page not formatted properly on Firefox
ns,When Cancel is clicked on the Service Config page  simply reload the config (not the entire app).
ns,Filtering on Jobs table does not work under certain situations
ns,Clicking on the red badge in the Hosts tab should not toggle the 'Alerts' filter on the Hosts page (clicking anywhere in Hosts tab should go to Hosts page with 'All' selected).
ns,Performed install on mixed OS environment with 8 hosts.Ambari Server = RHEL6Three Hosts = RHEL6Four Hosts = RHEL5Performed manual ambari-agent bootstrap of the Four RHEL5 hosts. I was able to successfully register all hosts. When install started  the four RHEL5 hosts failed on installing their first component. Looking at the servers  looks like the right HDP.repo and HDP-epel.repo files are put in place.But looks like the ambari.repo file had been overwritten at some point during the install process  and now is point to the RHEL6 repos  causing failures.
ns,When components are marked in an UNKNOWN state  it is not possible to delete the cluster - this should be possible.
ns,stderr: $configuration&#91;hdfs-site&#93; is not an hash or array when accessing it with dfs.hosts.exclude at /var/lib/ambari-agent/puppet/modules/hdp-hadoop/manifests/hdfs/decommission.pp:24 on node ip-10-82-213-66.ec2.internal stdout:None
ns,The api's are being handled by a queuedthreadpool. The queuedthread pool size is 25.Somehow the http connections are being torn down from the UI side but the server still is hanging onto that socket and reading (most likely UI will also need to close http connections if its not using them - which might be an issue as well but doesnt have to addressed as urgent). The server has a read timeout of 0 which means it will just hang on to that socket for read. This causes all the threads to block at one time or the other. Simple solution is add read timeouts to all the SelectChannelConnector and SslSelectChannelConnector we use.Exception trace: SEVERE: The exception contained within MappableContainerException could not be mapped to a response  re-throwing to the HTTP containerorg.eclipse.jetty.io.EofException: early EOF at org.eclipse.jetty.server.HttpInput.read(HttpInput.java:65) at org.codehaus.jackson.impl.ByteSourceBootstrapper.ensureLoaded(ByteSourceBootstrapper.java:507) at org.codehaus.jackson.impl.ByteSourceBootstrapper.detectEncoding(ByteSourceBootstrapper.java:129) at org.codehaus.jackson.impl.ByteSourceBootstrapper.constructParser(ByteSourceBootstrapper.java:224) at org.codehaus.jackson.JsonFactory._createJsonParser(JsonFactory.java:785) at org.codehaus.jackson.JsonFactory.createJsonParser(JsonFactory.java:561) at org.codehaus.jackson.jaxrs.JacksonJsonProvider.readFrom(JacksonJsonProvider.java:414) at com.sun.jersey.json.impl.provider.entity.JacksonProviderProxy.readFrom(JacksonProviderProxy.java:139) at com.sun.jersey.spi.container.ContainerRequest.getEntity(ContainerRequest.java:474)Notice the API's is being called all the time - meaning they probalby had a browser up and running for a long time.There might be a possibilility that the browser might have some issues after running for a long time. Something to keep in mind when this happens again. Easy way to check that is to call Ambari server API's and also bring up a new browser window (new instance) and try hitting the browser UI.
ns,Add validation checks for Add Property on custom site configs
ns,Error when loading /main/services directly
ns,Service status: if any of the master components are in UNKNOWN state  show the unknown icon. The action buttons for the service are disabled. Host status: if the host status is UNKNOWN or the heartbeat has not been received in more than 180 seconds  show the unknown icon. Host component status: if the host component status is UNKNOWN  show the unknown icon. The action button for the host component is disabled.
ns,mvn test -Pclover -Dclover.license=&lt;clover.coverage.license&gt; should run the unit tests and return html/xml code coverage reports
ns,Disable 'Add Component' button in the Host Details page if the host is in UNKNOWN state or !isHeartbeating
ns,Steps to reproduce1. Go to Admin tab2. Click on 'Add Local User' button3. Click on Admin tab again4. Clicking on 'Add Local User' button does nothing
ns,Click on any of the service links shown on the Dashboard page.It transitions to the service page and the content displayed is correct for the service chosen  but the URL indicates that it is another service and the side-menu shows a different service highlighted.
ns,Add Unit test to verify  When INSTALL is schedules on client components it should also be scheduled on components that are in INSTALL_FAILED state
ns,If 'Install from Local Repository' selected in install wizard  Add Host wizard not working
ns,Host Detail page: if the host component is in INSTALL_FAILED state  we should let the user reinstall it
ns,HBase Heatmaps: clean up labels and units
ns,To customize the hadoop group  when it was changed from 'hadoop' to 'hadoopgroup'  it didn't look like it worked.root@ip-10-85-135-237 hdfsuser# id hdfsuid=495(hdfs) gid=494(hdfs) groups=494(hdfs) 495(hadoop)And looking at the /etc/group filepuppet:x:497:hadoopgroup:x:500:rrdcached:x:496:apache:x:48:hadoop:x:495:mapred hdfs
ns,'Preparing to install ' message needs spacing
ns,Post-install  if the user tries to reconfigure NN  SNN  MapReduce local/system directories  we should popup a confirmation/warning upon Save as this is potentially a dangerous operation.
ns,Admin role can't be assigned to LDAP user
ns,On SUSE  I received a puppet error on each agent that /tmp/changeUid.sh failed during installation. (Sorry I no longer have the error  I made puppet change and restarted to get by it). But in a nutshell  running the command manually gave:ip-10-82-233-26:/tmp # /tmp/changeUid.sh ambari-qa 1012 /tmp/ambari-qa /home/ambari-qa /var/spool/mail/ambari-qaChanging uid of ambari-qa from 1012 to 1012Changing directory permisions for /tmp/ambari-qa /home/ambari-qa /var/spool/mail/ambari-qausermod: UID 1012 is not unique.Note that the usermod is trying to change to an existing UID  so the command is failing everywhere
ns,STEPS:1) Get tasks for first request  /api/v1/clusters/&lt;cluster&gt;/requests/&lt;firstRequest&gt;  i.e. task1 ... taskN12) Get tasks for second request by task from first requst  like /api/v1/clusters/&lt;cluster&gt;/requests/&lt;secondRequest&gt;/tasks/task1 (note task1 belongs to first request  not second)3) Notice that task from first request are present in second request.
ns,Post Ambari upgrade  Hive and Oozie fail to start after reconfigure
ns,This affects both Install and Add Host Wizards.Steps to reproduce:While installing components  stop the agent on one of the hosts.Waiting for a while puts the components on the host into the UNKNOWN state.Click Retry from the UI. This causes a server-side error and the UI gets confused (the hosts are shown with 'Waiting' message and no 'Retry' button is available).The user is not able to get out of this state.
s,Stack upgrade testing is still showing this to be an issue.warning: Unrecognised escape sequence '/;' in file /var/lib/ambari-agent/puppet/modules/hdp-hive/manifests/hive/service_check.pp at line 32warning: Dynamic lookup of $configuration is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes.notice: /Stage[1]/Hdp::Snappy::Package/Hdp::Snappy::Package::Ln[32]/Hdp::Exec[hdp::snappy::package::ln 32]/Exec[hdp::snappy::package::ln 32]/returns: executed successfullynotice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: ls: cannot access /usr/share/java/*oracle*: No such file or directorynotice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: 13/05/09 15:01:52 WARN conf.HiveConf: DEPRECATED: Configuration property hive.metastore.local no longer has any effect. Make sure to provide a valid value for hive.metastore.uris if you are connecting to a remote metastore.notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: log4j:ERROR setFile(null true) call failed.notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: java.io.FileNotFoundException: /tmp/ambari_qa/hive.log (Permission denied)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at java.io.FileOutputStream.openAppend(Native Method)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:192)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:116)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.FileAppender.setFile(FileAppender.java:290)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:164)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:216)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:257)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:133)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:97)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:689)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:647)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:544)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:440)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:476)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:354)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hadoop.hive.common.LogUtils.initHiveLog4jDefault(LogUtils.java:124)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hadoop.hive.common.LogUtils.initHiveLog4jCommon(LogUtils.java:77)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hadoop.hive.common.LogUtils.initHiveLog4j(LogUtils.java:58)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hcatalog.cli.HCatCli.main(HCatCli.java:61)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at java.lang.reflect.Method.invoke(Method.java:597)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hadoop.util.RunJar.main(RunJar.java:160)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: log4j:ERROR Either File or DatePattern options are not set for appender [DRFA].notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: Exception in thread 'main' java.lang.RuntimeException: java.io.IOException: Permission deniednotice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:272)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hcatalog.cli.HCatCli.main(HCatCli.java:79)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
ns,Need to be able to run Ambari Web (and access Ambari REST APIs) over HTTPS. Should document assuming the user comes with their own certificate. User should also be able to configure which port to expose HTTPS.
ns,Enable customization of smoke test user
ns,Make modifications to ambari-web that will allow for the selection of a Hadoop Compatible Filesystem. These changes include allowing HDFS to be unselected (either HDFS or HCFS must be selected). If HCFS is chosen  push appropriate configuration (site-conf.xml) files during the install so that systems will work with HCFS as the underlying filesystem rather than HDFS.
ns,Use stack metadata to determine if a stack contains an HCFS service and generate modified RoleCommandOrder dependencies if it does.
ns,Set default value of oozie property 'oozie.service.AuthorizationService.authorization.enabled' to true.
ns,Ambari sets the followings:#Set path to where bin/hadoop is availableexport HADOOP_HOME=${HADOOP_HOME:-/usr}#set the path to where bin/hbase is availableexport HBASE_HOME=${HBASE_HOME:-/usr}#Set the path to where bin/hive is availableexport HIVE_HOME=${HIVE_HOME:-/usr}# add libthrift in hive to sqoop class path first so hive imports workexport SQOOP_USER_CLASSPATH=''ls ${HIVE_HOME}/lib/libthrift-*.jar 2&gt; /dev/null':${SQOOP_USER_CLASSPATH}'#Set the path for where zookeper config dir isexport ZOOCFGDIR=${ZOOCFGDIR:-/etc/zookeeper/conf}It should be the followings (also screenshot is available):#Set path to where bin/hadoop is availableexport HADOOP_HOME=${HADOOP_HOME:-/usr/lib/hadoop}#set the path to where bin/hbase is availableexport HBASE_HOME=${HBASE_HOME:-/usr/lib/hbase}#Set the path to where bin/hive is availableexport HIVE_HOME=${HIVE_HOME:-/usr/lib/hive}#Set the path for where zookeper config dir isexport ZOOCFGDIR=${ZOOCFGDIR:-/etc/zookeeper/conf}# add libthrift in hive to sqoop class path first so hive imports workexport SQOOP_USER_CLASSPATH=''ls ${HIVE_HOME}/lib/libthrift-*.jar 2&gt; /dev/null':${SQOOP_USER_CLASSPATH}
ns,HBASE master fails to start on master. From log:2013-05-14 21:06:43 487 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: Permission denied: user=hbase  access=EXECUTE  inode='/apps/hbase/data':hdfs:hdfs:drwx------ at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95) at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57) at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1134) at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:556) at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:779) at org.apache.hadoop.hbase.util.FSUtils.getVersion(FSUtils.java:287) at org.apache.hadoop.hbase.util.FSUtils.checkVersion(FSUtils.java:329) at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:434) at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:146) at org.apache.hadoop.hbase.master.MasterFileSystem.&lt;init&gt;(MasterFileSystem.java:131) at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:532) at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:391) at java.lang.Thread.run(Thread.java:662)
ns,On Step8 'Customize Services' for HIVE and OOZIE we can use option 'Existing Oracle DB'.But after Cluster install HIVE and OOZIE didn't start (with option 'Existing Oracle DB').In the logs I found such:/var/log/hive/hive.logCaused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the 'DBCP' plugin to create a ConnectionPool gave an error : The specified datastore driver ('oracle.jdbc.driver.OracleDriver') was not found in the CLASSPATH. Please check your CLASSPATH specification  and the name of the driver./var/log/oozie/oozie.log013-05-14 11:25:59 618 FATAL Services:533 - USER[-] GROUP[-] TOKEN[-] APP[-] JOB[-] ACTION[-] E0103: Could not load service classes  Cannot load JDBC driver class 'oracle.jdbc.driver.OracleDriver'org.apache.oozie.service.ServiceException: E0103: Could not load service classes  Cannot load JDBC driver class 'oracle.jdbc.driver.OracleDriver'
ns,Oozie start failure:^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Setting OOZIE_BASE_URL: http://ip-10-212-166-111.ec2.internal:11000/oozie^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Using CATALINA_BASE: /var/lib/oozie/oozie-server^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Setting OOZIE_HTTPS_KEYSTORE_FILE: /home/ooziexx/.keystore^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Setting OOZIE_HTTPS_KEYSTORE_PASS: password^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Setting CATALINA_OUT: /var/log/oozie//catalina.out^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Using CATALINA_PID: /var/run/oozie/oozie.pid^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: ^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Using CATALINA_OPTS: -Dderby.stream.error.file=/var/log/oozie//derby.log^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Adding to CATALINA_OPTS: -Doozie.home.dir=/usr/lib/oozie -Doozie.config.dir=/etc/oozie/conf -Doozie.log.dir=/var/log/oozie/ -Doozie.data.dir=/grid/0/hadoop/oozie/data/ -Doozie.config.file=oozie-site.xml -Doozie.log4j.file=oozie-log4j.properties -Doozie.log4j.reload=10 -Doozie.http.hostname=ip-10-212-166-111.ec2.internal -Doozie.admin.port=11001 -Doozie.http.port=11000 -Doozie.https.port=11443 -Doozie.base.url=http://ip-10-212-166-111.ec2.internal:11000/oozie -Doozie.https.keystore.file=/home/ooziexx/.keystore -Doozie.https.keystore.pass=password -Djava.library.path=/usr/lib/hadoop/lib/native/Linux-amd64-64^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: ^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: /usr/lib/oozie/oozie-server/bin/catalina.sh: line 386: /var/run/oozie/oozie.pid: Permission denied^[[0m^[[1;35merr: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: change from notrun to 0 failed: su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh' returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp/manifests/init.pp:340^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Anchor[hdp::exec::exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh'::end]: Dependency Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh'] has failures: true^[[0m^[[0;33mwarning: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Anchor[hdp::exec::exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh'::end]: Skipping because of failed dependencies^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp-oozie::Service::Directory[/var/log/oozie]/Hdp::Directory_recursive_create[/var/log/oozie]/Hdp::Directory[/var/log/oozie]/File[/var/log/oozie]/owner: owner changed 'oozie' to 'ooziexx'^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp-oozie::Service::Directory[/var/log/oozie]/Hdp::Directory_recursive_create[/var/log/oozie]/Hdp::Directory[/var/log/oozie]/File[/var/log/oozie]/group: group changed 'oozie' to 'hadoopxx'^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp-oozie::Service::Directory[/var/run/oozie]/Hdp::Directory_recursive_create[/var/run/oozie]/Hdp::Directory[/var/run/oozie]/File[/var/run/oozie]/owner: owner changed 'oozie' to 'ooziexx'^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp-oozie::Service::Directory[/var/run/oozie]/Hdp::Directory_recursive_create[/var/run/oozie]/Hdp::Directory[/var/run/oozie]/File[/var/run/oozie]/group: group changed 'oozie' to 'hadoopxx'^[[0m^[[0;36mnotice: Finished catalog run in 9.36 seconds^[[0m
ns,Add the ability to capture username and save in the table for config mappings. This applies to cluster and host level
ns,Ambari needs to set right path for GC log directory of Hbase process.
ns,Sometimes stale host / host component indicators are shown
ns,After upgrading ambari from 1.2.2.5 to 1.2.3.6 the server throws 500 error when starting/stopping any service
s,Datanode Start fails in secure cluster.
ns,Host status filter not restored on Hosts page when navigating back
ns,Fix currently failing unit tests.
ns,TEST BROKEN : FAIL: test_upgradeCommand_executeCommand (TestActionQueue.TestActionQueue)
ns,Remove '0.1' stack definition since its never been used and is redundant.
ns,When a HDP 2.0.x stack is installed  the Jobs page should be hidden.
ns,Update mock json data for Test mode
ns,Agent heartbeat can become lost during install. The underlying issue is that during install  various yum commands are executed for component installation. However  the heartbeat ALSO performs a 'yum -C repolist'. If that command is taking a long time  the yum process can become deadlocked. The results of the repolist'ing are not used at this time  so remove it until needed.
ns,Java stack information for the threads listed above:==================================================='Thread-2': at org.apache.ambari.server.state.ServiceImpl.getDesiredConfigs(ServiceImpl.java:240) waiting to lock &lt;0x000000077b356dd0&gt; (a org.apache.ambari.server.state.ServiceImpl$$EnhancerByGuice$$9e2acafa) at org.apache.ambari.server.state.ServiceComponentImpl.getDesiredConfigs(ServiceComponentImpl.java:292) locked &lt;0x000000077b39bce8&gt; (a org.apache.ambari.server.state.ServiceComponentImpl$$EnhancerByGuice$$af7a745c) at org.apache.ambari.server.state.svccomphost.ServiceComponentHostImpl.getDesiredConfigs(ServiceComponentHostImpl.java:1057) at org.apache.ambari.server.agent.HeartbeatMonitor.generateStatusCommands(HeartbeatMonitor.java:166) at org.apache.ambari.server.agent.HeartbeatMonitor.doWork(HeartbeatMonitor.java:137) at org.apache.ambari.server.agent.HeartbeatMonitor.run(HeartbeatMonitor.java:85) at java.lang.Thread.run(Thread.java:662)'main': at org.apache.ambari.server.state.ServiceComponentImpl.debugDump(ServiceComponentImpl.java:376) waiting to lock &lt;0x000000077b39bce8&gt; (a org.apache.ambari.server.state.ServiceComponentImpl$$EnhancerByGuice$$af7a745c) at org.apache.ambari.server.state.ServiceImpl.debugDump(ServiceImpl.java:354) locked &lt;0x000000077b356dd0&gt; (a org.apache.ambari.server.state.ServiceImpl$$EnhancerByGuice$$9e2acafa) at org.apache.ambari.server.state.cluster.ClusterImpl.debugDump(ClusterImpl.java:693) at org.apache.ambari.server.state.cluster.ClustersImpl.debugDump(ClustersImpl.java:517) at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:320) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:432)Found 1 deadlock.
ns,The ambari-server.py start script has a command defined for starting the ambari-server in debug mode (SERVER_START_CMD_DEBUG  which turns on remote debugging)  but there is currently no option supported that will force the script to use the debug start commaand. I propose adding a --debug option so that you can run 'ambari-server start --debug' to activate remote debugging.
ns,Background operations popup does not automatically refresh the task log
ns,Add unit tests for Utils
ns,Reassign Master Wizard: refreshing page on step 2  3 or 4 breaks wizard
ns,Change config loading mechanism to allow for different stack versions
ns,ActionQueue.py missing 'Unrecognized command' testcase (L. 173) /src/test/python/TestActionQueue.py:42 unused test_RetryAction stub (retry is implemented in another way) /src/main/python/ambari_agent/ActionQueue.py:221 not covered case if commandresult&#91;&#39;exitcode&#39;&#93; != 0: /src/main/python/ambari_agent/ActionQueue.py:247 not covered case if command.has_key('roleCommand') and command&#91;&#39;roleCommand&#39;&#93; == 'START':PuppetExecutor.py configureEnviron/generate_repo_manifests/run_manifest/runCommand are not covered.PythonExecutor.py isSuccessfull is not testedRepoInstaller.py prepareReposInfo/generateFiles are not coveredshell.py is not covered
ns,When setting up Oozie with an external database  the following commands are run:cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-setup.sh -hadoop 0.20.200 /usr/lib/hadoop/ -extjs /usr/share/HDP-oozie/ext.zip -jars /usr/lib/hadoop/lib/hadoop-lzo-0.5.0.jar:/usr/share/java/mysql-connector-java.jarThe above command succeeds.However  the next command fails:cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/ooziedb.sh create -sqlfile oozie.sql -run setting OOZIE_CONFIG=${OOZIE_CONFIG:-/etc/oozie/conf} setting OOZIE_DATA=${OOZIE_DATA:-/var/lib/oozie} setting OOZIE_LOG=${OOZIE_LOG:-/var/log/oozie} setting CATALINA_BASE=${CATALINA_BASE:-/var/lib/oozie/oozie-server} setting CATALINA_TMPDIR=${CATALINA_TMPDIR:-/var/tmp/oozie} setting CATALINA_PID=${CATALINA_PID:-/var/run/oozie/oozie.pid} setting JAVA_HOME=/usr/jdk/jdk1.6.0_31 setting OOZIE_LOG=/var/log/oozie/ setting CATALINA_PID=/var/run/oozie/oozie.pid setting OOZIE_DATA=/grid/0/hadoop/oozie/data/ setting JAVA_LIBRARY_PATH=/usr/lib/hadoop/lib/native/Linux-amd64-64Validate DB ConnectionError: Could not connect to the database: java.lang.ClassNotFoundException: com.mysql.jdbc.DriverStack trace for the error was (for debug purposes):--------------------------------------java.lang.Exception: Could not connect to the database: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver at org.apache.oozie.tools.OozieDBCLI.validateConnection(OozieDBCLI.java:358) at org.apache.oozie.tools.OozieDBCLI.createDB(OozieDBCLI.java:168) at org.apache.oozie.tools.OozieDBCLI.run(OozieDBCLI.java:112) at org.apache.oozie.tools.OozieDBCLI.main(OozieDBCLI.java:63)Caused by: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:247) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:169) at org.apache.oozie.tools.OozieDBCLI.createConnection(OozieDBCLI.java:347) at org.apache.oozie.tools.OozieDBCLI.validateConnection(OozieDBCLI.java:354) ... 3 more--------------------------------------
ns,user_name column was added to clusterconfigmapping and hostconfigmapping tables. This changes should be made to DDL scripts for Oracle and MySQL also.
ns,The current version of Ambari build is being used in several scenarios: Ensure Ambari Server installs the correct version of Ambari Agent Ensure that Ambari Server only accepts registration from correct version of Ambari Agent Ensure that DB version is compatible with Ambari Server  The DB version itself will be used to control DB upgrades  Towards this end the following open issues remain: Get the build version be automatically embedded in the version file Use the above for deploying agent as well as allowing agents to register Regarding DB version there are two possible paths:  Separate out DB version and have it be modified manually as needed Have the build version be used as DB version - this may make writing upgrade scripts little complicated as build version may change due to some proj mgmt decision
ns,Allow Security related configs to be modified via custom settings
ns,Start/Stop button stays enabled for atleast 30-40 seconds after its been clicked already.
ns,The example ...'hosts' : [ { 'href' : 'http://your.ambari.server/api/v1/clusters/c1/hosts/host1'  'Hosts' : { 'cluster_name' : 'c1'  'host_name' : 'some.cluster.host' } }  { 'href' : 'http://your.ambari.server/api/v1/clusters/c1/hosts/host2'  'Hosts' : { 'cluster_name' : 'c1'  'host_name' : 'another.cluster.host' } ]... should read ... 'hosts' : [ { 'href' : 'http://your.ambari.server/api/v1/clusters/c1/hosts/some.host'  'Hosts' : { 'cluster_name' : 'c1'  'host_name' : 'some.host' } }  { 'href' : 'http://your.ambari.server/api/v1/clusters/c1/hosts/another.host'  'Hosts' : { 'cluster_name' : 'c1'  'host_name' : 'another.host' } } ]
s,On 'install Options' page  when selecting 'Perform manual registration on hosts and do not use SSH' is setting 'Path to 64-bit JDK' disabled(Screen Shot 2013-06-03 at 10.54.49 AM.png).Also path to 64-bit JDK JAVA_HOME' input field is enabled with unchecked check box(unchecked.png).Steps:1. Go to 'Install Options' page.Result:'Path to 64-bit JDK JAVA_HOME' input field is available for editing when check box is unchecked.
s,The two-way SSL mechanism used during server-agent registration exists to protect communication. This is useful in production environments but in typical 'first use' or POC scenarios  having this level of security is not necessary. As well  certificate generation can be problematic causing failures.We need to provide a way to make this mechanism optional:1) By default  ship with Server-Agent Two-Way SSL off.2) At any time post install  a user should be able to turn on Two-Way SSL and turn it back off  etc.
ns,Configuration mapping metadata on ambari-web should be computed as per the stack selection.
ns,The Ambari Upgrade process should preserve the old configs and add the new config options to the old config files.Currently we have it the other way around  that we copy the needed 3 properties from the old config files - this is wrong. We need to use the older config file and add the new options to the old config file. This is because the older config file can have all kinds of config option that the user might have used. We have to really really keep in mind usability of the product when fixing issues.
ns,I was installing a new cluster in my VM when the progress blocked on step 12. Looking on browser log  the PUTs for clusters desired_configs succeeded  but the very next call to create service component failed.&#91;POST&#93; http://dev.hortonworks.com:8080/api/v1/clusters/vmc/services?ServiceInfo/service_name=HDFSStatus Code:500 Invalid arguments  clustername and componentname should be non-null and non-empty when trying to create a componentData uploaded:{'components':[{'ServiceComponentInfo':{'component_name':'NAMENODE'}} {'ServiceComponentInfo':{'component_name':'SECONDARY_NAMENODE'}} {'ServiceComponentInfo':{'component_name':'DATANODE'}} {'ServiceComponentInfo':{'component_name':'HDFS_CLIENT'}}]}:Exception on server console:Mar 21  2013 11:25:09 AM com.sun.jersey.spi.container.ContainerResponse mapMappableContainerExceptionSEVERE: The RuntimeException could not be mapped to a response  re-throwing to the HTTP containerjava.lang.IllegalArgumentException: Invalid arguments  clustername and componentname should be non-null and non-empty when trying to create a component at org.apache.ambari.server.controller.AmbariManagementControllerImpl.createComponents(AmbariManagementControllerImpl.java:387) at org.apache.ambari.server.controller.internal.ComponentResourceProvider$1.invoke(ComponentResourceProvider.java:88) at org.apache.ambari.server.controller.internal.ComponentResourceProvider$1.invoke(ComponentResourceProvider.java:85) at org.apache.ambari.server.controller.internal.AbstractResourceProvider.createResources(AbstractResourceProvider.java:229) at org.apache.ambari.server.controller.internal.ComponentResourceProvider.createResources(ComponentResourceProvider.java:85) at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:131) at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:75) at org.apache.ambari.server.api.handlers.QueryCreateHandler.persist(QueryCreateHandler.java:163) at org.apache.ambari.server.api.handlers.QueryCreateHandler.handleRequest(QueryCreateHandler.java:68) at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:98) at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:73) at org.apache.ambari.server.api.services.ServiceService.createServices(ServiceService.java:114) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597)
ns,Steps:1. Go to 'Services' page.2. Select 'HDFS' service.3. Select 'Configs' tab.4. Open 'Custom core-site.xml' panel.5. Add property with name 'ipc.client.idlethreshold' to this panel.Result:Property with name 'ipc.client.idlethreshold' was added to 'Custom core-site.xml' panel. But there is already presented property with same name in 'core-site.xml' file. After saving added property was disappeared from UI  and after service starting value of the old was changed to new (on UI and in 'core-site.xml' file).Expected result:UI should not allow to add property with existing name in specified file.
s,Security Wizard: navigation not locked down  causes artifacts  and other unwanted side effects
ns,1. http://ambari:8080/api/v1/clusters/cluster/services?fields=components/host_components/metricsincludes all metrics for NameNode and JobTracker  including cpu metrics.2. http://ambari:8080/api/v1/clusters/cluster/services?fields=components/host_components/metrics/cpudoes not return cpu metrics for NameNode and JobTracker
ns,Enhancements: Read RESOURCE_DIR from the ambari.properties Ask user twice to place drivers to /usr/share/java
ns,Unit Tests: Added tests to install wizard for step 3  5  10
ns,This test fails sometimes and is not very reliable.
s,This happens when templeton.kerberos.principal property is set to HTTP/_HOST@&lt;realm name&gt; instead of HTTP/&lt;internal host name&gt;@&lt;realm name&gt;
ns,List the installed stack and its services in Admin &gt; Cluster page.
ns,Added unit tests to models: Rack  Host  HostComponent.
s,When a cluster is in secure mode  there are additional manual steps that need to be performed when adding new hosts to the cluster.In the Review page  add a prominent box with a red background and display the following text:You are running your cluster in secure mode. You must set up the keytabs for all the hosts you are adding before you proceed.'Upon clicking 'Deploy'  show a confirmation popup with the text:Before you proceed  please make sure that the keytabs have been set up on the hosts you are adding per the instructions on the Review page. Otherwise  the assigned components will not be able to start properly on the hosts being added. OK CancelNote that the extra message and confirmation popup should show only when security is enabled on the cluster.
ns,AMBARI-2174 - Add missing unit tests for the code changes
ns,Steps:1. Install ambari-server.2. Go to 'Install Options' page.3. Set the hosts list and ssh-key  click 'Next' button.Result:Ambari Web is blocked in 'Confirm Hosts'. Confirming was not ended after not less than 40 minutes.
ns,install_jce_manually() in the download_jdk() result in fatal exception.Checking JDK...INFO: Loading properties from /etc/ambari-server/conf/ambari.propertiesERROR: Error getting ambari propertiesERROR: Exiting with exit code -1. Reason: Downloading or installing JDK failed: 'Fatal exception: Error getting ambari properties  exit code -1'. Exiting.
ns,Add support for 'classic' dashboard
s,If we run setup second time. Amabri should allow user to change the DB password for the ambari-user.Currently  the executed script should allow for this.command: &#91;&#39;su&#39;  &#39;-&#39;  &#39;postgres&#39;  &#39;--command=psql -f /var/lib/ambari-server/resources/Ambari-DDL-Postgres-CREATE.sql -v username=/&#39;&quot;ambari-server&quot;/&#39; -v password=&quot;/&#39;/&#39;&quot;&#39;&#93;
ns,Kerberos globals are shown in HDFS config page during install
ns,Installer Wizard step-6: NameNode and SNameNode should not be co-hosted by default on multinode cluster.
ns,HDFS Config page (post-install) does not load when App.testMode = true
ns,Set default widgets to show for the Dashboard
ns,STDOUTTraceback (most recent call last):File '/tmp/setupAgent.py'  line 192  in ?main(sys.argv)File '/tmp/setupAgent.py'  line 188  in mainsys.exit(runAgent(passPhrase  expected_hostname))File '/tmp/setupAgent.py'  line 80  in runAgentagent_retcode = subprocess.call('/usr/sbin/ambari-agent start --expected-hostname={0}'.format(expected_hostname)  shell=True)AttributeError: 'str' object has no attribute 'format'Error seems to be caused by python 2.4 version.
ns,STR:1) Run ambari-server setup.2) Choose custom user  user1.3) Delete ambari.user from ambari.properties.4) Run ambari-server setup.5) Choose custom user  different from choosen in step 2  user2.6) Run ambari-server start.Got error:ambari-server startUsing python /usr/bin/python2.6Starting ambari-serverHave root privileges.Checking iptables...iptables is disabled nowRunning server: &#91;&#39;/bin/su&#39;  &#39;user2&#39;  &#39;-s&#39;  &#39;/bin/sh&#39;  &#39;-c&#39;  &#39;/usr/jdk64/jdk1.6.0_31/bin/java -server -XX:NewRatio=3 -XX:+UseConcMarkSweepGC -XX:-UseGCOverheadLimit -XX:CMSInitiatingOccupancyFraction=60 -Xms512m -Xmx2048m -cp /etc/ambari-server/conf:/usr/lib/ambari-server/*:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin:/usr/lib/ambari-server/* org.apache.ambari.server.controller.AmbariServer &gt;/var/log/ambari-server/ambari-server.out 2&gt;&amp;1 &amp; echo $! &gt; /var/run/ambari-server/ambari-server.pid&#39;&#93;done.sh: /var/run/ambari-server/ambari-server.pid: Permission deniedsh: /var/log/ambari-server/ambari-server.out: Permission denied
s,Security wizard: smoke test for services fails with customized service user names.
ns,Upgrade from 1.2.2/1.2.3.7 to 1.2.5 fails because the ddl script does not work - metainfo version change is broken.Again we need thorugh testing around this. Please make sure we have tested 1.2.2 and 1.2.3  1.2.4 upgrade to 1.2.5.
ns,Add unit tests for bootstrap and setupAgent python scripts for the server.We need to find gaps on the bootstrap and setupagent script and add unit tests for them.
ns,We need a script which can manually set the configs when UI is not able to  or does not support.
ns,Ran into issues setting up Hive and Oozie with an Oracle database.1. We are hard-coding port 1521 for the JDBC URL. 2. There are two types of JDBC URLs for Oracle: jdbc:oracle:thin:@&#91;HOST&#93;&#91;:PORT&#93;:SID jdbc:oracle:thin:@//&#91;HOST&#93;&#91;:PORT&#93;/SERVICEWe are making the assumption that it is the latter  but this may not work depending on how Oracle is set up.3. We prompt for the 'Database Name'. In Oracle context  this could be the SID or SERVICE NAME  but it's not clear what this is.As a solution to all of the above  we will construct the JDBC URL based on the database type  host  and name for Hive/Oozie and present it to the user as an editable text field during install.Post-install  the JDBC URL remains editable  but does not change automatically as changes other database-related parameters.
ns,Remove unnecessary check for hostnames/ service name which is wrong.
ns,Sometimes  bootstrap actions reports success even if it has failed host tasks:&lt;bootStrapRequest&gt; &lt;status&gt;SUCCESS&lt;/status&gt; &lt;hostsStatus&gt; &lt;hostName&gt;andromeda54.hi.inet&lt;/hostName&gt; &lt;status&gt;FAILED&lt;/status&gt; &lt;statusCode&gt;1&lt;/statusCode&gt; &lt;log&gt; [...] &lt;/log&gt; &lt;/hostsStatus&gt; &lt;hostsStatus&gt; &lt;hostName&gt;andromeda55.hi.inet&lt;/hostName&gt; &lt;status&gt;FAILED&lt;/status&gt; &lt;statusCode&gt;1&lt;/statusCode&gt; &lt;log&gt; [...] &lt;/log&gt; &lt;/hostsStatus&gt; &lt;log&gt; [...] &lt;/log&gt;&lt;/bootStrapRequest&gt;
ns,This ticket solved:1. Dashboard page has padding space under footer.2. When a widget got deleted  the page scroll to top.(page should stay in place)
ns,We have noticed an issue where shell.killprocessgrp is not working correctly. We have seen a scenario where namenode start takes a long time when on a secure cluster the jce policy is unavailable. After 10 minutes when the agent tries to kill the puppet process it invariably fails.We need to run some experiment (perhaps using long running puppet processes) to ensure that shell.killprocessgrp works as expected.Also  we need to verify that the behavior is as expected on both RHEL and Suse.
ns,I got this blow-up (when bind anon was either false or I just pressed returnBind anonymously true/false (false):====================Review Settings====================authentication.ldap.primaryUrl: my.ldap:389authentication.ldap.secondaryUrl: asdauthentication.ldap.useSSL: falseauthentication.ldap.usernameAttribute: uidauthentication.ldap.baseDn: basednauthorization.userRoleName: userauthorization.adminRoleName: adminauthentication.ldap.bindAnonymously: falseTraceback (most recent call last):File '/usr/sbin/ambari-server.py'  line 3047  in &lt;module&gt;main()File '/usr/sbin/ambari-server.py'  line 2891  in mainsetup_ldap()File '/usr/sbin/ambari-server.py'  line 2353  in setup_ldapprint('%s: %s' % (property  ldap_property_value_mapproperty))KeyError: 'authentication.ldap.managerDn'
ns,Currently it is very difficult to know what principals and keytabs need to be created on which hosts.We should present this information to the end user in a format that is easy to consume.The user running the wizard may not be the one who will be creating keytabs and principals. We can expose the capability to download a csv file and send it to the appropriate person who may parse the data to create a script to generate principals/keytabs (or do so manually).Display the attached as a popup after Configure Services step is done.Let's show it as a popup so that we don't affect any existing navigation/flow.For generating the content:Keytab paths are based on the user inputPrincipal names are based on the user inputNameNode host: show the nn and HTTP principals and keytab pathsJobTracker host: show the jt principal and keytab pathOozie Server host: show the oozie and HTTP principals and keytab pathsNagios Server host: show the nagios principal and keytab pathHBase Master host: show the hbase principal and keytab pathHive Server host: show the hive principal and keytab pathWebHCat Server host: show the HTTP principal and keytab pathZooKeeper Server host: show the zookeeper principal and keytab pathDataNode host: show the dn principal and keytab pathTaskTracker host: show the tt principal and keytab pathRegionServer host: show the hbase principal and keytab pathIf there are duplicated principals on the same host  display it only once.Clickng on 'Download CSV' downloads the CSV file ('host-principal-keytab-list.csv'). The same content  except each row is a comma-delimited list with a /n at the end.
ns,ambari-agent/src/main/puppet/modules/hdp-nagios/templates/hadoop-services.cfg.erb shows the following:# HDFS::DATANODE Checksdefine service { hostgroup_name slaves use hadoop-service service_description DATANODE::DataNode process down servicegroups HDFS check_command check_tcp!&lt;%=scope.function_hdp_template_var('dfs_datanode_http_address')%&gt;!-w 1 -c 1 normal_check_interval 1 retry_check_interval 0.5 max_check_attempts 3}define service { hostgroup_name slaves use hadoop-service service_description DATANODE::DataNode storage full servicegroups HDFS check_command check_datanode_storage!&lt;%=scope.function_hdp_template_var('dfs_datanode_http_address')%&gt;!90%!90% normal_check_interval 5 retry_check_interval 1 max_check_attempts 2}We need to remove dependence on dfs_datanode_http_address and use the actual config property like:hdp_get_port_from_url($hdfs-site['dfs.datanode.http.address'])
ns,Expected flow:[root@localhost ~]# ambari-server setup-httpsUsing python /usr/bin/python2.6Setting up HTTPS properties...Do you want to configure HTTPS [y/n] (y)?SSL port (8443) ? Please enter path to Certificate: /some/path/on/my/host/server.crtPlease enter path to Private Key: /some/path/on/my/host/server.keyPlease enter password for Private Key:Importing and saving certificate...done.NOTE: Reset Ambari Server to apply changes ('ambari-server restart|stop|start')Ambari Server 'HTTPS setup' completed successfully. Exiting.
s,Cannot add property mapred.task.tracker.task-controller
ns,Decommission datanode does not do kinit before refresh.
ns,We need CSV content which shows which principals  keytabs etc. end up on the various hosts. This will be useful in scripts or other tools which can create appropriate environment.
ns,Zookeeper smoke test failing in secure cluster
ns,Add helpful message when not able to download jdk with setup options for the user to be able to specify the jdk.
s,/etc/conf/hadoop/taskcontroller.cfgIn secure mode permissions are set to '400'.
s,Some memory configs are set to -1 in ambari-mapred.cluster.reduce.memory.mb-mapred.jobtracker.maxtasks.per.job-mapred.cluster.max.reduce.memory.mb-mapred.cluster.map.memory.mb-mapred.job.map.memory.mb-mapred.job.reduce.memory.mb-mapred.cluster.max.map.memory.mbModify the stack definition to put default values as appropriate.
ns,The first iteration for creating custom repo URL persisted the new URL to disk. This poses a problem when running Ambari as non-root because the file system is owned by root. Change the implementation to save the override in the metainfo table.
ns,When confirming hosts using external addresses bootstrapping should be failed immediately and a warning should be logged. Right now this functionality is broken  neither warning in log nor failing immediately present
ns,Enhance the UI to use stacks API to give user a choice of stacks and be able to customize repository locations.
ns,Add step 'Create Principals and Keytabs' to Security wizard.
ns,We should be able to catch KeyBoardInterrupt in ambari-server main and print a useful message like:'Aborting ... Keyboard Interrupt.' and avoid the stack trace for the user.
s,Garbage responses to true/false questions just pass thru. Notice below  just put in garbage for Use SSL and that's what it would have written.Need validation to confirm they enter either 1) return to accept default or 2) the word true or 3) the word false. Else  inform the user'Property must be 'true' or 'false'.' and ask again.Secondary URL :Use SS &#91;true/false&#93; (false): asdUser name attribute* (uid):Base DN* :Property cannot be blank.Base DN* : asdBind anonymously* true/false (false):Manager DN* :asdEnter Manager Password*:Re-enter password:Passwords do not matchEnter Manager Password*:Re-enter password:====================Review Settings====================authentication.ldap.primaryUrl: my.url:849authentication.ldap.useSSL: asdauthentication.ldap.usernameAttribute: uid
ns,Using an ambari-qa user (that is a ldap user and he has hadoop set up as a primary ldap group) [root@va21 ldap]# id ambari-qauid=524(ambari-qa) gid=522(hadoop) groups=522(hadoop)causes ambari-server setup to create an ambari-qa local group (at /etc/group). ambari-qa:x:601:ambari-qa[root@va21 ldap]# id ambari-qauid=524(ambari-qa) gid=522(hadoop) groups=522(hadoop) 601(ambari-qa)Ldap users and groups are transparent for ambari-server  it starts well.The problem is that additional group is created.
ns,Report contains hosts  which don't have issues  but should show 'A space delimited list of hosts which have issues'.
ns,JS Error when deleting a widget after sorting it on remove/edit sign
ns,namenode_host is not an hash or array when accessing it with 0 at /var/lib/ambari-agent/puppet/modules/hdp/manifests/params.pp:70 on node host1.
ns,Steps (Installer Wizard):Go to 'Customize Services' page.Select 'HDFS' tab.Add custom property 'xxx' to 'Custom core-site.xml' panel.Try add custom property 'xxx' to 'Custom hdfs-site.xml' panel.Result:Custom property can not be added to 'Custom hdfs-site.xml' panel (see attachment).Steps (Ambari monitoring UI):Go to 'Customize Services' page.Select 'HDFS' tab.Add custom property to 'Custom core-site.xml' panel (for example  'install-test-core-site').Continue and end hadoop installation.Go to 'Services' page.Select 'MapReduce' tab.Try add custom property 'install-test-core-site' to 'Custom mapred-site.xml' panel.Result:Custom property can not be added to 'Custom mapred-site.xml' panel (see attachment).
ns,There are no quick links for Oozie. Similarly  some other services also are missing the quick links.
ns,zookeeper smoke test passes without having to 'kdestroy' on the user running the smoke test.
ns,FE only has support to provide single path for kinit. As Ambari supports mixed OS deployment it cannot be guaranteed that kinit exists at the same path on all nodes. FE should allow providing a set of look-up paths for kinit as well as the BE should support a set of default lookup paths.
ns,lets just rename agent -&gt; server
ns,Processes are truncated too short and can't really tell what's in conflict. Since there is a lot of space on the right (in fact  the hostname column is too far to the left compared to other sections)  we should display more characters (with hover tooltip showing full text).
ns,See attachecd screenshot.
ns,1. Install cluster2. On the host with SNameNode  we also have a region server3. Stop snamenode component on that host4. Services &gt; hbase &gt; summary shows region server is not liveAlso after stopping DataNode or TaskTracker  component status changes are not reflected on Services &gt; summary tab
ns,Host cleanup left two packages(ambari-log4j  libconfuse)
ns,Dashboard Widgets: 'hover to show details' experience is jarring
ns,Reset the latest stack version for 1.2.5
ns,In ambari-web/app/views/main/service/info/summary.js#hostComponentsUpd()  is called per each hostComponent's host and master property change. On a 150 node cluster  we get like 300 calls just for this method.Due to this  service_mapper  which usually maps in 600ms  takes now 5.8s.
ns,This happened in a very specific situation.1. Put mouse on the line space around a metric widget.2. hover on the widget with mouse down.Result:The legend show up as a strange bigger size.
s,Should say 'Hive Metastore and HiveServer2'  not just HiveServer2. Even though they are co-located master components  let's make it clear this principal is for both include keytab file column. In addition to the keytab full path column (/etc/security/keytabs/jt.service.keytab)  include a column with just the filename (jt.service.keytab). Easier to copy/paste/parse if you want to use the CSV file.
ns,Steps to reproduce: Install using non-default directories Upon install failure  go back to Customize Services page from the left nav. Certain directories (NN dirs  SNN dir  DN dirs  Oozie Data Dir  ZK Dir  etc) are reverted back to the default. Other parameters are not reverted back.
ns,Update Ember-I18n
ns,Read timeout issues in Oracle JDBC connections where read has a long timeout. This happens when the read timeout is too long. In order to set appropriate timeout  add specially prefixed values in ambari.properties
ns,Performed cluster setup as proposed at E2E test scenario. ambari-server setupambari-server setup-ldapambari-server encrypt-passwordsambari-server setup-httpsambari-server startServer does not start. It complains about missing password file / db password alias19:03:36 249 INFO Configuration:300 - Generation of file with password19:03:37 320 INFO CredentialProvider:146 - action =&gt; PUT  alias =&gt; ambari.db.password19:03:37 885 INFO Configuration:313 - Reading password from existing file19:03:38 838 INFO CredentialProvider:146 - action =&gt; PUT  alias =&gt; ambari.ldap.manager.password19:12:02 925 INFO Configuration:313 - Reading password from existing file19:12:02 946 INFO Configuration:324 - API SSL Authentication is turned on.19:12:02 946 INFO Configuration:329 - Reading password from existing file19:12:02 948 INFO Configuration:481 - Hosts Mapping File null19:12:02 951 INFO HostsMap:60 - Using hostsmap file null19:12:04 467 INFO MasterKeyServiceImpl:209 - Loading from persistent master: #1.0# Fri  Jul 12 2013 19:03:34.71719:12:06 016 INFO AmbariServer:446 - Getting the controller19:12:11 146 INFO CertificateManager:68 - Initialization of root certificate19:12:11 147 INFO CertificateManager:70 - Certificate exists:false19:12:11 147 INFO CertificateManager:137 - Generation of server certificate19:12:16 383 INFO ShellCommandUtil:43 - Command openssl genrsa -des3 -passout pass:n15KV1q6aWRZIP86XAjpTdbTaKo0HHWIsTuaOPZQdxycChECKG -out /var/lib/ambari-server/keys/ca.key 4096 was finished with exit code: 0 - the operation was completely successfully.19:12:16 431 INFO ShellCommandUtil:43 - Command openssl req -passin pass:n15KV1q6aWRZIP86XAjpTdbTaKo0HHWIsTuaOPZQdxycChECKG -new -key /var/lib/ambari-server/keys/ca.key -out /var/lib/ambari-server/keys/ca.crt -batch was finished with exit code: 0 - the operation was completely successfully.19:12:16 483 INFO ShellCommandUtil:43 - Command openssl x509 -passin pass:n15KV1q6aWRZIP86XAjpTdbTaKo0HHWIsTuaOPZQdxycChECKG -req -days 365 -in /var/lib/ambari-server/keys/ca.crt -signkey /var/lib/ambari-server/keys/ca.key -out /var/lib/ambari-server/keys/ca.crt was finished with exit code: 0 - the operation was completely successfully.19:12:16 496 INFO ShellCommandUtil:43 - Command openssl pkcs12 -export -in /var/lib/ambari-server/keys/ca.crt -inkey /var/lib/ambari-server/keys/ca.key -certfile /var/lib/ambari-server/keys/ca.crt -out /var/lib/ambari-server/keys/keystore.p12 -password pass:n15KV1q6aWRZIP86XAjpTdbTaKo0HHWIsTuaOPZQdxycChECKG -passin pass:n15KV1q6aWRZIP86XAjpTdbTaKo0HHWIsTuaOPZQdxycChECKG was finished with exit code: 0 - the operation was completely successfully.19:12:16 883 INFO AmbariServer:123 - ********* Meta Info initialized **********19:12:16 896 INFO ClustersImpl:88 - Initializing the ClustersImpl19:12:17 115 ERROR Configuration:610 - Error reading from credential store.19:12:17 116 ERROR Configuration:616 - Cannot read password for alias = /etc/ambari-server/conf/password.dat19:12:17 117 ERROR AmbariServer:455 - Failed to run the Ambari Serverjava.lang.RuntimeException: Unable to read database password at org.apache.ambari.server.configuration.Configuration.readPasswordFromFile(Configuration.java:596) at org.apache.ambari.server.configuration.Configuration.getRcaDatabasePassword(Configuration.java:583) at org.apache.ambari.eventdb.webservice.WorkflowJsonService.setDBProperties(WorkflowJsonService.java:95) at org.apache.ambari.server.controller.AmbariServer.performStaticInjection(AmbariServer.java:437) at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:125) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:452)Caused by: java.io.FileNotFoundException: File '/etc/ambari-server/conf/password.dat' does not exist at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:265) at org.apache.commons.io.FileUtils.readFileToString(FileUtils.java:1457) at org.apache.commons.io.FileUtils.readFileToString(FileUtils.java:1475) at org.apache.ambari.server.configuration.Configuration.readPasswordFromFile(Configuration.java:594) ... 5 more19:12:17 118 ERROR AmbariServer:420 - Error stopping the serverjava.lang.NullPointerException at org.apache.ambari.server.controller.AmbariServer.stop(AmbariServer.java:418) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:457)Content of ambari.properties:server.jdbc.rca.driver=oracle.jdbc.driver.OracleDriverauthentication.ldap.managerDn=uid=hdfs ou=people ou=dev dc=apache dc=orgauthentication.ldap.primaryUrl=localhost:389server.jdbc.rca.url=jdbc:oracle:thin:@ip-10-34-79-165.ec2.internal:1521/XEserver.connection.max.idle.millis=900000server.jdbc.port=1521server.version.file=/var/lib/ambari-server/resources/versionserver.jdbc.rca.user.passwd=/etc/ambari-server/conf/password.datapi.authenticate=truejce_policy.url=http://public-repo-1.hortonworks.com/ARTIFACTS/jce_policy-6.zipserver.persistence.type=remoteclient.api.ssl.key_name=https.keyauthentication.ldap.useSSL=falseambari-server.user=ambar-serverclient.api.ssl.port=8443authentication.ldap.usernameAttribute=uidserver.jdbc.user.name=ambariserver.jdbc.schema=XEjava.home=/usr/jdk64/jdk1.6.0_31server.os_type=redhat6api.ssl=truebootstrap.script=/usr/lib/python2.6/site-packages/ambari_server/bootstrap.pyclient.api.ssl.cert_name=https.crtauthentication.ldap.bindAnonymously=falseclient.security=ldapserver.jdbc.hostname=ip-10-34-79-165.ec2.internalresources.dir=/var/lib/ambari-server/resourcessecurity.passwords.encryption.enabled=truebootstrap.setup_agent.script=/usr/lib/python2.6/site-packages/ambari_server/setupAgent.pyserver.jdbc.driver=oracle.jdbc.driver.OracleDriverjdk.url=http://public-repo-1.hortonworks.com/ARTIFACTS/jdk-6u31-linux-x64.binsecurity.server.keys_dir=/var/lib/ambari-server/keysserver.jdbc.rca.user.name=ambariwebapp.dir=/usr/lib/ambari-server/webmetadata.path=/var/lib/ambari-server/resources/stacksserver.jdbc.url=jdbc:oracle:thin:@ip-10-34-79-165.ec2.internal:1521/XEserver.fqdn.service.url=http://169.254.169.254/latest/meta-data/public-hostnamebootstrap.dir=/var/run/ambari-server/bootstrapauthentication.ldap.baseDn=dc=apache dc=orgserver.jdbc.user.passwd=${alias=ambari.db.password}authentication.ldap.managerPassword=${alias=ambari.ldap.manager.password}server.jdbc.database=oraclesecurity.server.two_way_ssl=trueFile /etc/ambari-server/conf/password.dat is missingSetup flow:[root@ip-10-116-65-200 kerb]# ambari-server setupUsing python /usr/bin/python2.6Initializing...Setup ambari-serverChecking SELinux...SELinux status is 'enabled'SELinux mode is 'enforcing'Temporarily disabling SELinuxWARNING: SELinux is set to 'permissive' mode and temporarily disabled.OK to continue [y/n] (y)? yCustomize user account for ambari-server daemon [y/n] (n)? yEnter user account for ambari-server daemon (root):ambar-serverAdjusting ambari-server permissions and ownership...Checking iptables...iptables is disabled now. please reenable later.Checking JDK...Downloading JDK from http://public-repo-1.hortonworks.com/ARTIFACTS/jdk-6u31-linux-x64.bin to /var/lib/ambari-server/resources/jdk-6u31-linux-x64.binJDK distribution size is 85581913 bytesjdk-6u31-linux-x64.bin... 100% (81.6 MB of 81.6 MB)Successfully downloaded JDK distribution to /var/lib/ambari-server/resources/jdk-6u31-linux-x64.binTo install the Oracle JDK you must accept the license terms found at http://www.oracle.com/technetwork/java/javase/downloads/jdk-6u21-license-159167.txt. Not accepting will cancel the Ambari Server setup.Do you accept the Oracle Binary Code License Agreement [y/n] (y)? Installing JDK to /usr/jdk64Successfully installed JDK to /usr/jdk64/jdk1.6.0_31Downloading JCE Policy archive from http://public-repo-1.hortonworks.com/ARTIFACTS/jce_policy-6.zip to /var/lib/ambari-server/resources/jce_policy-6.zipSuccessfully downloaded JCE Policy archive to /var/lib/ambari-server/resources/jce_policy-6.zipCompleting setup...Configuring database...Enter advanced database configuration [y/n] (n)? ySelect database:1 - PostgreSQL (Embedded)2 - Oracle[1]:2Hostname [localhost]:ip-10-34-79-165.ec2.internalPort [1521]:Select Oracle identifier type:1 - Service Name2 - SID[1]:XEInvalid number.Select Oracle identifier type:1 - Service Name2 - SID[1]:1Service Name [ambari]:XEUsername [ambari]: Enter Database Password [bigdata]: WARNING: Before starting Ambari Server  you must copy the Oracle JDBC driver JAR file to /usr/share/java.Press &lt;enter&gt; to continue.Copying JDBC drivers to server resources...Configuring remote database connection properties...WARNING: Cannot find oracle sqlplus client in the path to load the Ambari Server schema. Before starting Ambari Server  you must run the following DDL against the database to create the schema sqlplus ambari/bigdata &lt; /var/lib/ambari-server/resources/Ambari-DDL-Oracle-CREATE.sql Press &lt;enter&gt; to continue.WARNING: The cli was not foundAmbari Server 'setup' completed with warnings.[root@ip-10-116-65-200 kerb]# less /etc/passwd
ns,Improve styles for HostCleanup code area
ns,Add umask checks for host checks - we should alert if umask is not 022.
ns,We should add a third text box to re-type the new password while changing the password for an ambari user.
ns,When all hosts fail to register  there are no warnings  and hence we show OK for host checks.
ns,Steps to reproduce: Go to step-3 (Generate principals and keytabs) of Enable security wizard. Restart Amabri server. Refresh on step-3. ui will take you to login page. Entering correct credentials  user will be navigated again to step-3 of security wizard. At this point JS error is encountered.
ns,Steps to reproduce: 1. Stop a component. 2. Wait for the corresponding BG operation to finish. Result: 'Start' button isn't available for the component within 10 seconds since stop operation finished in UI. It appears later.This happens cuz of update interval  it is set to 15 seconds (App.contentUpdateInterval)  that's why in some moments it can take up to 15 sec to update components status  after request is done.Solution: Create a seperate update interval specialy for updating host components. In config.js we even have App.componentsUpdateInterval = 6000; But this value was not used anywhere in code till now.
ns,setup ldap does not validate secondary url. It should validate the input (when entered) the same way as the primary.
ns,Notice in the ambari-server snippet below a lot of these messages:ERROR Configuration:616 - Cannot read password for alias = nullSteps to reproduce:Setup serverSetup encrypt passwords  don't persist the keySetup httpsStart the server  provided master keyDo cluster install
ns,Enable Security Wizard stops on step '2. Save Configurations' and doesn't let the user leave the wizard
ns,Datanode Live widget displays 0 dead when no datanode is live on a cluster.
ns,Report on Host Checks for 0 isses looks like:####################################### Host Checks Report## Generated: Tue Jul 09 2013 13:11:14 GMT+0300 (FLE Daylight Time)############################################################################# Hosts## A space delimited list of hosts which have issues.# Provided so that administrators can easily copy hostnames into scripts  email etc.######################################HOSTSShow Report button should be disabled for 0 issues or at least do not show HOSTS section in report.
s,Disable security not working in web-ui testMode.
ns,See the attached images. The third image added is a summary of what was going on.
ns,Implement a cleanup thread that removes files in ambari-agent data directory that are older than a month or so(must be configurable).It's required  because the directory will grow unbounded if it's not cleaned up.
ns,Fix JS Unit tests after merge 1.4.0 to trunk
ns,Installer wizard with HDP stack-2 selection in test mode
ns,Disable autocomplete on form tag for Ambari UI.
ns,Copytable jobs need to be submitted as the hbase super user however the uid for hbase super user created by Ambari has uid &lt; 1000
ns,On a host that stopped heartbeating  the UI shows actions to perform on host components. However  upon executing an action  the backend does not create any tasks and returns 200. The UI doesn't do anything in this case. Instead  the UI should disable action buttons in this case.
ns,Say Add Hosts wizard fails to add a host and the host components are in INSTALL_FAILED state. In this case  the UI displays the host component with a red gear and shows the action menu with the current state 'Install Failed' and the action 'Re-Install'. Once you invoke 'Re-Install'  the host component disappears from the UI. Once the host component finishes installing  it magically appears again.
ns,1. Hosts &gt; Design around full hostname being displayed. Consider being able to show 40 characters and then truncate &gt; 40 chars 2. Hosts &gt; Show # control should persist when navigating around app  and after logout/login3. Hosts &gt; Make Components list an expand/collapse control instead of a list with abbreviations4. Host Details &gt; Move Components area above Summary area since the Component Controls are used very often (much more often than viewing the Summary area info).
ns,Additional logs:  When a task is timed-out/failed API requests to update component and component hosts   Remove logs  Do not log ganglia population time when its less than 5 second
ns,Proceed to step 4 of security wizard  click on 'Start Services' or 'Stop Services' link.Result: popup window is shown with empty contentThis bug was happening due to js error 'Uncaught TypeError: Cannot call method 'filterProperty' of null' in setBackgroundOperationHeader: functionAfter background operation (host popup) popup was optimized for better peformace on large cluster this error showed up. New function on setting popup header  did not account that this popup (HostPopup) is used not only in BG operations  but also in security wizard.
ns,To reproduce go to the Jobs page from top menu (do not open .../main/apps directly). Table is not striped and pagination buttons are enabled even if there is only one page.
ns,When installing Hadoop 2 stack  Hive service check fails to run.
ns,In firefoxSteps:Go to 'Customize Services' page.Select 'Misc' tab.Change username for HDFS  HBase or Group (do not move focus to other elements).Click 'Next' button.Result:Browser was switched to 'Review' page.Was opened popup window for changing properties depended with user names. User must to refresh the page for popup menu disappearing.
ns,Running jobs as a local FS user does not work work 2.0.* stack because of permissions on /tmp/hadoop-yarn/staging  which is the default staging dir.
ns,Host Checks popup is showing that /usr/lib/hadoop already exists.When I clicked on 'Show Reports'  it is showing '/usr/lib/hadoop/ /folder'. This doesn't make much sense. FILES AND FOLDERS/usr/lib/hadoop/ /folderThe API is showing: 'stackFoldersAndFiles' : [ { 'name' : '/usr/lib/hadoop'  'type' : 'directory' }
s,Error which occurred while requesting cluster status is not informative.Change url in request for cluster state  go to admin page and try to enable security.
ns,Cannot save HDFS configs with SNN in MAINTENANCE mode
ns,YARN in current Hadoop 2 stack has only MR2 as application. Since it does not make sense to install YARN without a default application  and MR2 cannot be installed by itself  we should combine both into a single install option (see screenshot).
ns,We need API call for a graph which will show NodeManager status counts. NodeManagers can be in the following states: active  lost  unhealthy  rebooted  and decommissioned.
ns,We need to show below 2 additional information Across cluster memory - used/reserved/total Queues information if available
ns,Properties of capacity-scheduler.xml are truncated by ' '. It make impossible to create multiple queues  please see http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html
ns,In the Install Wizard  Customize Services page shows multiple directories delimited by newlines.However  when revisiting the page (go back to Customize Services from the Review page  for example)  the directories are shown in comma-delimited format. We should always show the directories in newline-delimited format. Note that when we actually store the value  the directories are comma-delimited (which is correct).
ns,Dashboard refactor and Unit tests
ns,keytab Tar for each host is packaged including hostname. Untaring it on a host creates path starting with &lt;hostanme&gt;/&lt;actual path&gt;. Fix is to package the content inside the hostname directory excluding the hostname directory itself.
ns,Installing 0.95.2 from internal repo 2.0.5. hbase-daemon.sh defines a default logger of INFO RFA{{  when log4j.properties uses {{INFO DRFA. They should match  as startup generates an error and does not continue.We should stop over writing the log4j properties for hbase. This will allow for hbase log4j properties to be in sync with those that come with the rpms.
ns,The issue is the following:getRequestsByTaskStatus behaves correctly and returns the latest N requests. Note that the N requests have M (M &gt;&gt; N) tasks.Then the call to findByRequestIds gets oldest N tasks where the requestId for tasks belong to the list returned by the first call. So instead of getting M tasks we only get N tasks that too N oldest tasks which are returned. As a result the call never returns that latest request/tasks. The fix is to drop the filter done by the calls findByRequestIds and findByRequestAndTaskIds. Filter should only be applied on the number of requests to be returned.
ns,Installed the Hadoop2 stack and upon finishing the zk_data_dir in global  and yarn.nodemanager.local-dirs in yarn-site have the folder names suffixed with ' ' in API. Consequently  the folder names on system end up with a ' ' at end. Ex: /hadoop/yarn  and /hadoop/zookeeper .
ns,1. Stop secondary namenode.2. Edit the pid file  default location = /var/run/hadoop/hdfs/hadoop-hdfs-secondarynamenode.pid3. Change the pid to any other process pid that is currently running.4. Start secondary namenode.Outcome:Secondary namenode start command succeeds but secondary namenode does not start. (indicated by live status of the component).
ns,API call for a graph which will show time series for YARN Allocated containers.
ns,Host registration fails with:INFO 2013-08-10 01:50:49 923 Controller.py:99 - Unable to connect to: https://c6401.ambari.apache.org:8441/agent/v1/register/c6403.ambari.apache.orgTraceback (most recent call last): File '/usr/lib/python2.6/site-packages/ambari_agent/Controller.py'  line 80  in registerWithServer ret = json.loads(response) File '/usr/lib64/python2.6/json/__init__.py'  line 307  in loads return _default_decoder.decode(s) File '/usr/lib64/python2.6/json/decoder.py'  line 319  in decode obj  end = self.raw_decode(s  idx=_w(s  0).end()) File '/usr/lib64/python2.6/json/decoder.py'  line 338  in raw_decode raise ValueError('No JSON object could be decoded')ValueError: No JSON object could be decodedambari-server.log is showing:01:55:22 732 WARN [qtp967966535-50] ServletHandler:514 - /agent/v1/register/c6401.ambari.apache.orgcom.google.gson.JsonSyntaxException: java.lang.IllegalStateException: Expected a string but was BEGIN_OBJECT at line 1 column 3680 at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:176) at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.read(TypeAdapterRuntimeTypeWrapper.java:40) at com.google.gson.internal.bind.ArrayTypeAdapter.read(ArrayTypeAdapter.java:72) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:93) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:172) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:93) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:172) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:93) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:172) at com.google.gson.Gson.fromJson(Gson.java:795) at com.google.gson.Gson.fromJson(Gson.java:761) at org.apache.ambari.server.api.GsonJsonProvider.readFrom(GsonJsonProvider.java:60) at com.sun.jersey.spi.container.ContainerRequest.getEntity(ContainerRequest.java:474) at com.sun.jersey.server.impl.model.method.dispatch.EntityParamDispatchProvider$EntityInjectable.getValue(EntityParamDispatchProvider.java:123) at com.sun.jersey.server.impl.inject.InjectableValuesProvider.getInjectableValues(InjectableValuesProvider.java:46) at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$EntityParamInInvoker.getParams(AbstractResourceMethodDispatchProvider.java:153) at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:183) at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288) at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108) at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84) at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
ns,Nagios fails to start with invalid configuration error. (This is likely intermittent).Error: Configuration validation failed - when Nagios is started./usr/sbin/nagios -v /etc/nagios/nagios.cfgCopyright (c) 1999-2009 Ethan GalstadLast Modified: 03-15-2013License: GPLWebsite: http://www.nagios.orgReading configuration data... Read main config file okay...Processing object config file '/etc/nagios/objects/commands.cfg'...Processing object config file '/etc/nagios/objects/contacts.cfg'...Processing object config file '/etc/nagios/objects/timeperiods.cfg'...Processing object config file '/etc/nagios/objects/templates.cfg'...Processing object config file '/etc/nagios/objects/hadoop-hosts.cfg'...Processing object config file '/etc/nagios/objects/hadoop-hostgroups.cfg'...Processing object config file '/etc/nagios/objects/hadoop-servicegroups.cfg'...Processing object config file '/etc/nagios/objects/hadoop-services.cfg'...Processing object config file '/etc/nagios/objects/hadoop-commands.cfg'...Error: Could not find any hostgroup matching 'resourcemanager' (config file '/etc/nagios/objects/hadoop-services.cfg'  starting on line 292) Error processing object config files!***&gt; One or more problems was encountered while processing the config files... Check your configuration file(s) to ensure that they contain valid directives and data defintions. If you are upgrading from a previous version of Nagios  you should be aware that some variables/definitions may have been removed or modified in this version. Make sure to read the HTML documentation regarding the config files  as well as the 'Whats New' section to find out what has changed.
ns,In Ambari UI  we show properties like NameNode RPC Time.We used to get this metric by querying the NameNode component for 'RpcQueueTime_avg_time'.However  in Hadoop 2  it looks like this property name changed to 'RpcQueueTimeAvgTime'  so the Ambari API no longer contains these metrics. Other properties related to NameNode RPC may have changed. We need to update the mapping accordingly.
ns,Steps to reproduce:1) Deploy HDP-2.0.5 cluster with Nagios.2) Start of Nagios failed  puppet log:notice: /Stage[1]/Hdp::Snappy::Package/Hdp::Snappy::Package::Ln[32]/Hdp::Exec[hdp::snappy::package::ln 32]/Exec[hdp::snappy::package::ln 32]/returns: executed successfullynotice: /Stage[2]/Hdp-nagios::Server::Enable_snmp/Exec[enable_snmp]/returns: executed successfullynotice: /Stage[2]/Hdp-nagios::Server::Config/Hdp-nagios::Server::Configfile[hadoop-hostgroups.cfg]/Hdp::Configfile[/etc/nagios/objects/hadoop-hostgroups.cfg]/File[/etc/nagios/objects/hadoop-hostgroups.cfg]/content: content changed '{md5}873d2be7b9f78137e0740223944d93af' to '{md5}780117e3c2407e9d02b37eab93159149'notice: /Stage[2]/Hdp-nagios::Server::Config/Hdp-nagios::Server::Configfile[nagios]/Hdp::Configfile[/etc/init.d//nagios]/File[/etc/init.d//nagios]/content: content changed '{md5}3990694abc37617c79e2ea5276d71089' to '{md5}c4c4454911c0c6c1ba29d9d0dc2aa28c'notice: /Stage[2]/Hdp-nagios::Server::Config/Hdp-nagios::Server::Configfile[hadoop-hosts.cfg]/Hdp::Configfile[/etc/nagios/objects/hadoop-hosts.cfg]/File[/etc/nagios/objects/hadoop-hosts.cfg]/content: content changed '{md5}7979396ff0b495e40901acdd2ecc457c' to '{md5}fa5ec3a93a4827cc6a691e0b8e22b4f6'notice: /Stage[2]/Hdp-nagios::Server::Config/Hdp-nagios::Server::Configfile[hadoop-services.cfg]/Hdp::Configfile[/etc/nagios/objects/hadoop-services.cfg]/File[/etc/nagios/objects/hadoop-services.cfg]/content: content changed '{md5}06c9d0bb0aa3b1e7b30b19fb1bb30b5a' to '{md5}934241156b5489483dab8018f9cecd22'notice: /Stage[2]/Hdp-nagios::Server::Web_permisssions/Hdp::Exec[htpasswd -c -b /etc/nagios/htpasswd.users nagiosadmin p]/Exec[htpasswd -c -b /etc/nagios/htpasswd.users nagiosadmin p]/returns: executed successfullynotice: /Stage[2]/Hdp-nagios::Server::Web_permisssions/Hdp::Exec[apache_permissions_htpasswd.users]/Exec[apache_permissions_htpasswd.users]/returns: executed successfullynotice: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]/returns: nagios is stoppednotice: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]/returns: Configuration validation failed[FAILED]err: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]/returns: change from notrun to 0 failed: service nagios start returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp-nagios/manifests/server.pp:284notice: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]/returns: nagios is stoppednotice: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]/returns: Configuration validation failed[FAILED]err: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]: Failed to call refresh: service nagios start returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp-nagios/manifests/server.pp:284notice: /Stage[2]/Hdp-nagios::Server::Services/Anchor[hdp-nagios::server::services::end]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-nagios::Server::Services/Anchor[hdp-nagios::server::services::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Anchor[hdp::package::httpd::begin]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Anchor[hdp::package::httpd::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Package[httpd]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Package[httpd]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Anchor[hdp::java::package::httpd::begin]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Anchor[hdp::java::package::httpd::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Exec[mkdir -p /tmp/HDP-artifacts/ ; curl -kf --retry 10 http://dev01.hortonworks.com:8080/resources//jdk-6u31-linux-x64.bin -o /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin httpd]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Exec[mkdir -p /tmp/HDP-artifacts/ ; curl -kf --retry 10 http://dev01.hortonworks.com:8080/resources//jdk-6u31-linux-x64.bin -o /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin httpd]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Exec[mkdir -p /usr/jdk ; chmod +x /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin; cd /usr/jdk ; echo A | /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin -noregister &gt; /dev/null 2&gt;&amp;1 httpd]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Exec[mkdir -p /usr/jdk ; chmod +x /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin; cd /usr/jdk ; echo A | /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin -noregister &gt; /dev/null 2&gt;&amp;1 httpd]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/File[/usr/jdk/jdk1.6.0_31/bin/java httpd]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/File[/usr/jdk/jdk1.6.0_31/bin/java httpd]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Anchor[hdp::java::package::httpd::end]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Anchor[hdp::java::package::httpd::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Anchor[hdp::package::httpd::end]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Anchor[hdp::package::httpd::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Anchor[hdp::exec::monitor webserver restart::begin]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Anchor[hdp::exec::monitor webserver restart::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Exec[monitor webserver restart]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Exec[monitor webserver restart]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Anchor[hdp::exec::monitor webserver restart::end]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Anchor[hdp::exec::monitor webserver restart::end]: Skipping because of failed dependencies3) Run checkconfig:[root@dev02 ~]# /usr/sbin/nagios -v /etc/nagios/nagios.cfg Nagios Core 3.5.0Copyright (c) 2009-2011 Nagios Core Development Team and Community ContributorsCopyright (c) 1999-2009 Ethan GalstadLast Modified: 03-15-2013License: GPLWebsite: http://www.nagios.orgReading configuration data... Read main config file okay...Processing object config file '/etc/nagios/objects/commands.cfg'...Processing object config file '/etc/nagios/objects/contacts.cfg'...Processing object config file '/etc/nagios/objects/timeperiods.cfg'...Processing object config file '/etc/nagios/objects/templates.cfg'...Processing object config file '/etc/nagios/objects/hadoop-hosts.cfg'...Processing object config file '/etc/nagios/objects/hadoop-hostgroups.cfg'...Processing object config file '/etc/nagios/objects/hadoop-servicegroups.cfg'...Processing object config file '/etc/nagios/objects/hadoop-services.cfg'...Processing object config file '/etc/nagios/objects/hadoop-commands.cfg'...Error: Could not find any servicegroup matching 'MAPREDUCE' (config file '/etc/nagios/objects/hadoop-services.cfg'  starting on line 67) Error processing object config files!***&gt; One or more problems was encountered while processing the config files... Check your configuration file(s) to ensure that they contain valid directives and data defintions. If you are upgrading from a previous version of Nagios  you should be aware that some variables/definitions may have been removed or modified in this version. Make sure to read the HTML documentation regarding the config files  as well as the 'Whats New' section to find out what has changed.[root@dev02 ~]# It seems we have invalid condition for generation of MAPREDUCE Nagios checks.
ns,Few puppet variables are missing in jdbc-connector and oozie service-check puppet scripts.
ns,stderr:None stdout:notice: /Stage[1]/Hdp::Snappy::Package/Hdp::Snappy::Package::Ln[32]/Hdp::Exec[hdp::snappy::package::ln 32]/Exec[hdp::snappy::package::ln 32]/returns: executed successfullynotice: /Stage[1]/Hdp::Snmp/Hdp::Package[snmp]/Hdp::Package::Process_pkg[snmp]/Hdp::Java::Package[snmp]/Hdp::Java::Jce::Package[snmp]/Exec[jce-install snmp]/returns: executed successfullynotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: Moved to trash: hdfs://domU-12-31-39-07-D5-91.compute-1.internal:8020/user/ambari-qa/examplesnotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: Moved to trash: hdfs://domU-12-31-39-07-D5-91.compute-1.internal:8020/user/ambari-qa/input-datanotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: Error: AUTHENTICATION : Could not authenticate  GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: Invalid sub-command: Missing argument for option: infonotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns:notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: use 'help [sub-command]' for help detailsnotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: Invalid sub-command: Missing argument for option: infonotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns:notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: use 'help [sub-command]' for help detailsnotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns:notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: workflow_status=err: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: change from notrun to 0 failed: sh /tmp/oozieSmoke.sh /etc/oozie/conf /etc/hadoop/conf ambari-qa true /etc/security/keytabs/smokeuser.headless.keytab EXAMPLE.COM jt/domu-12-31-39-07-d5-91.compute-1.internal@EXAMPLE.COM nn/domu-12-31-39-07-d5-91.compute-1.internal@EXAMPLE.COM /usr/bin/kinit returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp-oozie/manifests/oozie/service_check.pp:63notice: Finished catalog run in 51.05 seconds
ns,Oozie start failed with following in the log: hadoop dfs -chmod -R 755 /user/oozie/share' returned 1 instead of one of 0On the node: chmod: '/user/oozie/share': No such file or directoryoozie@c6402 ~$ hadoop dfs -ls /user/ DEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.Found 3 itemsdrwxrwx--- - ambari-qa hdfs 0 2013-08-02 02:40 /user/ambari-qadrwx------ - hive hdfs 0 2013-08-02 02:41 /user/hivedrwxrwxr-x - hdfs hdfs 0 2013-08-02 02:42 /user/oozieoozie@c6402 ~$ hadoop dfs -ls /user/oozieDEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.oozie@c6402 ~$The reason  the oozie smoke is failing to run an oozie job with more than one node in the cluster  due to bad settings in /etc/hadoop/core-site.xml:&lt;property&gt; &lt;name&gt;hadoop.proxyuser.oozie.hosts&lt;/name&gt; &lt;value&gt;host1&lt;/value&gt;&lt;/property&gt;host1  in my case is host on which oozie is not installed. After changing this to host2 (where oozie server is installed)  oozie smoke succeeded.
ns,After I had a cluster up and running successfully  but manually turned HDFS Safe Mode ON by running:hdfs dfsadmin -safemode enter.HDFS Execute Check failed due to Puppet timeout after 10 minutes.It should have failed quicker since task timeout is 10 minutes on server.
ns,Yarn component need hadoop-env.sh and core-site.xml. These files should be deployed on hosts on which only yarn components are deployed.
ns,Add HBase 96 metrics changes to jmx in a backwards compatible way.
ns,Currently  in HA NN cluster  starting HDFS service at services tab  tries to start SNAMENODE as well (through it is in Maintainance state because of executing curl -u admin:admin -i -X PUT -d '{'RequestInfo':{'context':'SNN maintenance'} 'Body':{'HostRoles':{'state':'MAINTENANCE'}}}' http://$SERVER:8080/api/v1/clusters/$CLUSTER/hosts/$SNN_HOST/host_components/SECONDARY_NAMENODE command )
ns,Currently Nagios alerts are shown in Ambari UI like so: (green check) NameNode process down &lt;- means NameNode process is up (red X) NameNode process down &lt;- means NameNode process is down (green check) Percent DataNode down &lt;- means % of DataNodes that are up is above the threshold (red X) Percent DataNode down &lt;- means % of DataNodes that are up is below the threshold (green check) Nagios status log staleness &lt;- means Nagios status log is fresh (red X) Nagios status log staleness &lt;- means Nagios status log is staleWhen a user sees the word 'down' with a positive indication (green check) for it  it's confusing. It's like saying 'this is red' in green... is it green or red?The proposal here is to rename these alerts  like so: (green check) NameNode process &lt;- means NameNode process is up/healthy (red X) NameNode process &lt;- means NameNode process is down/unhealthy (green check) Percent DataNodes live &lt;- means % of DataNodes that are up/healthy is above the threshold (red X) Percent DataNodes live &lt;- means % of DataNodes that are up/healthy is below the threshold (green check) Nagios status log freshness &lt;- means Nagios status log is fresh (red X) Nagios status log freshness &lt;- means Nagios status log is staleAlso there are inconsistencies in the way we show component names in alert titles and descriptions (like 'templeton server status' to mean 'WebHCat Server status'  etc). These need to be fixed.
ns,ResourceManager's rpc.rpc.RpcQueueTimeAvgTime and yarn.ClusterMetrics.NumActiveNMs are not being provided accurately due to changes introduced in AMBARI-2910 to YARN configuration to collect more metrics.We need to narrow down the scope of metric collection to keep getting these previous metrics.
ns,Steps to reporduce:1. Go to Installer-&gt;Welcome step2. Focus on cluster name textfield3. Hover on 'learn more' label4. Press Enter to route to the next pageResult:Popover remains visible on other pages
ns,Update stack definition for MAPREDUCE2 and the default site xml files.
ns,Update 1.3.2 stack definition for repo url
ns,Oozie smoke tests fail on Ambari with NPE in Oozie Server.
ns,Use a specific build number for the stck builds.
ns,Changing mapreduce log directory prefix results in Live status showing history server not started.
s,After enabling security  when browsing back to Admin &gt; Security  it would be good to see the tabs with the values entered during wizard setup (read-only)  just so it's easy to see what the user specifically entered during the wizard setup.
ns,When ambari-agent and ambari-server packages are uninstalled  uninstall scriplets don't stop running services. That's why processes remain running even after package removal. That may cause issues during upgrade (if administrator misses the 'stop services' step)[root@host01 ~]$ ambari-server statusUsing python /usr/bin/python2.6Ambari-server statusAmbari Server runningFound Ambari Server PID: '7850 at: /var/run/ambari-server/ambari-server.pid[root@host01]# ambari-agent statusFound ambari-agent PID: 3521ambari-agent running.Agent PID at: /var/run/ambari-agent/ambari-agent.pidAgent out at: /var/log/ambari-agent/ambari-agent.outAgent log at: /var/log/ambari-agent/ambari-agent.log[root@host01]# rpm -e ambari-server[root@host01]# rpm -e ambari-agent[root@host01]# ps aux | grep ambari | grep -v grep | grep -v postgresroot 7850 2.1 13.2 3031480 254212 ? Sl 20:51 0:36 /usr/jdk64/jdk1.6.0_31/bin/java -server -XX:NewRatio=3 -XX:+UseConcMarkSweepGC -XX:-UseGCOverheadLimit -XX:CMSInitiatingOccupancyFraction=60 -Xms512m -Xmx2048m -cp /etc/ambari-server/conf:/usr/lib/ambari-server/*:/sbin:/bin:/usr/sbin:/usr/bin:/usr/lib/ambari-server/* org.apache.ambari.server.controller.AmbariServerroot 3521 0.7 1.1 499760 22544 ? Sl 20:53 0:12 /usr/bin/python2.6 /usr/lib/python2.6/site-packages/ambari_agent/main.py start restart --expected-hostname=host01
ns,Should turn on predicate pushdown by default.
ns,Automatically trim whitespaces (both leading and trailing) for: Database Host in Oozie and Hive Database Name in Oozie and Hive Database URL in Oozie and Hive All directories  including log dir  pid dir data dir  etc (I believe we already trim  split  and join for the UI config type 'directories'. We should trim on the single-line directory values  if we are not doing so already).Automatically trim all trailing spaces (but not leading spaces) for all config values with the following exceptions: Password fields Values that consist of spaces only (such as ' ')
ns,Ambari should always point to latest repo.
ns,Customize Services page->Misc tab: Popup with related properties does not opened after 'Group User' value changing
ns,Following two values need to be added to oozie-site.xml in order to get sla and shell workflows to run successfully.shell-action-0.2.xsdoozie-sla-0.1.xsd oozie-sla-0.2.xsdFollowing is the property name&lt;property&gt;&lt;name&gt;oozie.service.SchemaService.wf.ext.schemas&lt;/name&gt;&lt;value&gt;shell-action-0.1.xsd email-action-0.1.xsd hive-action-0.2.xsd sqoop-action-0.2.xsd ssh-action-0.1.xsd distcp-action-0.1.xsd&lt;/value&gt;&lt;/property&gt;
ns,command: ambari-server setup -sCompleting setup...Configuring database...Enter advanced database configuration [y/n] &#40;n&#41;? ERROR: Connection properties not set in config file.Default properties detected. Using built-in database.Checking PostgreSQL...Running initdb: This may take upto a minute.About to start PostgreSQLConfiguring local database...Configuring PostgreSQL...Restarting PostgreSQLAmbari Server 'setup' completed successfully.Running ambari-server setup command again doesn't reproduce the error statement.
ns,Host cleanup should: remove files and folders in tmp folder based on what users are being cleaned remove default hadoop group
ns,Compare the config parameter ordering in Install Wizard &gt; Customize Services service configs and Monitoring &gt; Services &gt; Config; they are different.We should use the same parameter ordering that we use in Install Wizard &gt; Customize Services for Monitoring &gt; Services &gt; Config.
ns,The point is to use custom puppet function instead of regex for line$dfs_domain_socket_path_dir = regsubst($hdp-hadoop::params::dfs_domain_socket_path  '/[^//]+$'  '').It could be implemented with ruby:File.split('/path/to/file') # =&gt; ['/path/to'  'file']
ns,Installed a cluster  namenode start fails  install exits with warnings.If i go an look at the specific task that fails  the puppet warnings are still present. Seems like those puppet warnings didn't get cleared.In a task failure case  having the warnings clear is when it's most important.
ns,Fix Unit tests
ns,1) The last patch had an issue with the testcase because of which it did not compile. I might have accidently added a line while I was saving some of the files.(this is fixed in the new patch)The trunk currently fails for ambari-client.https://cwiki.apache.org/confluence/display/AMBARI/Ambari+python+Client has all the exposed methods
ns,yarn-site having:'yarn.nodemanager.local-dirs' : '/grid/0/hadoop/yarn /grid/1/hadoop/yarn /grid/2/hadoop/yarn /grid/3/hadoop/yarn /grid/4/hadoop/yarn /grid/5/hadoop/yarn' 'yarn.nodemanager.log-dirs' : '/grid/0/hadoop/yarn /grid/1/hadoop/yarn /grid/2/hadoop/yarn /grid/3/hadoop/yarn /grid/4/hadoop/yarn /grid/5/hadoop/yarn' Now /grid/3 was mounted as read-only due to some disk errors. Though other folders got successfully created  Ambari will not start the NodeManager process.notice: /Stage[1]/Hdp::Snappy::Package/Hdp::Snappy::Package::Ln[32]/Hdp::Exec[hdp::snappy::package::ln 32]/Exec[hdp::snappy::package::ln 32]/returns: executed successfullynotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Exec[mkdir -p /grid/3/hadoop/yarn]/Exec[mkdir -p /grid/3/hadoop/yarn]/returns: mkdir: cannot create directory '/grid/3/hadoop': Read-only file systemerr: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Exec[mkdir -p /grid/3/hadoop/yarn]/Exec[mkdir -p /grid/3/hadoop/yarn]/returns: change from notrun to 0 failed: mkdir -p /grid/3/hadoop/yarn returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp/manifests/init.pp:479notice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Exec[mkdir -p /grid/3/hadoop/yarn]/Anchor[hdp::exec::mkdir -p /grid/3/hadoop/yarn::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Exec[mkdir -p /grid/3/hadoop/yarn]/Anchor[hdp::exec::mkdir -p /grid/3/hadoop/yarn::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Directory[/grid/3/hadoop/yarn]/File[/grid/3/hadoop/yarn]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Directory[/grid/3/hadoop/yarn]/File[/grid/3/hadoop/yarn]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[capacity-scheduler]/File[/etc/hadoop/conf/capacity-scheduler.xml]/content: content changed '{md5}e5d17c21c7a5e1db9f3af35cba71df0a' to '{md5}2ca1d267a46f1aecac726caabaa16774'notice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[capacity-scheduler]/File[/etc/hadoop/conf/capacity-scheduler.xml]/owner: owner changed 'hdfs' to 'yarn'notice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[core-site]/File[/etc/hadoop/conf/core-site.xml]/content: content changed '{md5}86d742a780d59a957ea0a283dec03784' to '{md5}8506e4402ba8140ea4f9fed97b6f94e2'notice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[yarn-site]/File[/etc/hadoop/conf/yarn-site.xml]/content: content changed '{md5}d84a967ce47a6b77734ed8f53d817c6e' to '{md5}42940cca6e8f64ae5de50524fb131274'notice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Anchor[hdp-yarn::service::nodemanager::begin]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Anchor[hdp-yarn::service::nodemanager::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Anchor[hdp::exec::mkdir -p /var/log/hadoop-yarn::begin]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Anchor[hdp::exec::mkdir -p /var/log/hadoop-yarn::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Exec[mkdir -p /var/log/hadoop-yarn]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Exec[mkdir -p /var/log/hadoop-yarn]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Anchor[hdp::exec::mkdir -p /var/log/hadoop-yarn::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Anchor[hdp::exec::mkdir -p /var/log/hadoop-yarn::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Directory[/var/log/hadoop-yarn]/File[/var/log/hadoop-yarn]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Directory[/var/log/hadoop-yarn]/File[/var/log/hadoop-yarn]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Anchor[hdp::exec::mkdir -p /var/run/hadoop-yarn/yarn::begin]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Anchor[hdp::exec::mkdir -p /var/run/hadoop-yarn/yarn::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Exec[mkdir -p /var/run/hadoop-yarn/yarn]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Exec[mkdir -p /var/run/hadoop-yarn/yarn]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Anchor[hdp::exec::mkdir -p /var/run/hadoop-yarn/yarn::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Anchor[hdp::exec::mkdir -p /var/run/hadoop-yarn/yarn::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Directory[/var/run/hadoop-yarn/yarn]/File[/var/run/hadoop-yarn/yarn]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Directory[/var/run/hadoop-yarn/yarn]/File[/var/run/hadoop-yarn/yarn]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Anchor[hdp::exec::su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager'::begin]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Anchor[hdp::exec::su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager'::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Anchor[hdp::exec::su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager'::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Anchor[hdp::exec::su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager'::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Anchor[hdp::exec::sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1::begin]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Anchor[hdp::exec::sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Anchor[hdp::exec::sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Anchor[hdp::exec::sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Anchor[hdp-yarn::service::nodemanager::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Anchor[hdp-yarn::service::nodemanager::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Anchor[hdp-yarn::nodemanager::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Anchor[hdp-yarn::nodemanager::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[mapred-site]/File[/etc/hadoop/conf/mapred-site.xml]/content: content changed '{md5}093cb1899b3c3b9dc4a7c1c93729c18b' to '{md5}4c462999cc47e6f6ba0e6381d71d81ba'notice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[mapred-site]/File[/etc/hadoop/conf/mapred-site.xml]/owner: owner changed 'mapred' to 'yarn'notice: Finished catalog run in 2.39 seconds
ns,Input and output bytes columns have been removed from the UI  but the web service doesn't know about that yet so it doesn't map the column index to a sort field correctly.
ns,The web service is labeling these incorrectly when they are retrieved from the db.
ns,'HBase Master Heap' widget on Dashboard displays parameters in TB (param1.png)  but real values are in MB (param0.png).
ns,Precondition: hadoop is installed. Total disk space for both machines is 100 GB. Used disk space is about 5-15%.Steps: Go to 'Heatmaps' page. Select 'Host Disk Space Used %' metric ('Maximum' input field has '100' value). Both indicators has green color (disk space usage is in 0-20% category). Choose 'Maximum' input field value to '99'.Result:One indicator has red color (79.2% - 99% category)  but real disk space usage is 8.3% and indicator should be green.
ns,Ambari 1.2.5  attached the configs to the cluster  in order to provide ability for override behavior at the service and host component levels.The JMX ports read from the service configs can no longer be modified from their default values since the existing code reads the service configurations which do not exist.
ns,The height of blocks 'Alerts and health checks' is changed sometimes after Service page refresh.It can change to normal sizes after another tries.Produced only in Firefox.
s,Steps to reproduce:1. enable security WITHOUT pre-configuring kerberos on cluster and see failures on '3. Start Services';2. disable security.In the end DataNode fails on ALL hosts without a possibility to get started.When you try to start DataNode manually it also ends with error:err: /Stage[2]/Hdp-hadoop::Datanode/Hdp-hadoop::Service[datanode]/Hdp::Exec[su - hdfs -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start datanode']/Exec[su - hdfs -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start datanode']/returns: change from notrun to 0 failed: su - hdfs -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start datanode' returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp/manifests/init.pp:479
ns,See the screenshot for HBase Cumulative Requests graph.The graph does not seem like it's displaying cumulative read/write requests  but rather showing the rate... so the label no longer makes sense.This is on the 2.0.5 stack. On the 1.x  I believe this graph would show cumulative read/write requests (which were less useful).So for 2.x stack  we should probably change the label for this graph to say Reads/Writes per Second (or whatever we are showing - need to confirm).
s,API Call:curl -u admin:admin http://localhost:8080/api/v1/clusters/c1/services?fields=components/ServiceComponentInfo components/host_components components/host_components/HostRoles components/host_components/metrics/jvm/memHeapUsedM components/host_components/metrics/jvm/memHeapCommittedM components/host_components/metrics/mapred/jobtracker/trackers_decommissioned components/host_components/metrics/cpu/cpu_wio components/host_components/metrics/rpc/RpcQueueTime_avg_time components/host_components/metrics/flume/flume components/host_components/metrics/yarn/QueueAmbari log:20:54:57 044 WARN [qtp912472968-20] ServletHandler:514 - /api/v1/clusters/c1/servicesjava.lang.NullPointerException at org.apache.ambari.server.controller.ganglia.GangliaPropertyProvider.getRRDRequests(GangliaPropertyProvider.java:225) at org.apache.ambari.server.controller.ganglia.GangliaPropertyProvider.populateResources(GangliaPropertyProvider.java:110) at org.apache.ambari.server.controller.internal.VersioningPropertyProvider.populateResources(VersioningPropertyProvider.java:98)
ns,1. Go to Admin page -&gt; Users tab2. Click Edit for some user3. Try to enter wrong value for Current Password field.The message does not tell me that my entered current password is wrong  so it can be confusing for user.Another scenario:1. Go to Admin page -&gt; Users tab2. Click Edit for some user3. Try to enter wrong value for Current Password field and don't fill fields for a new passwordIt don't really change the password but it still allows to enter wrong Current Password without any message
s,Number of ExecutionCommandEntity objects keep growing and result in Out of memory on large cluster (100 nodes).Script to re-create the issue:&#91;root@domain user&#93;# cat test1.shfor i in{0..100}doecho 'doing $i'curl -u admin:admin 'http://domain.net:8080/api/v1/clusters/c1/requests?to=end&amp;page_size=10&amp;fields= tasks/Tasks/*' &gt; /dev/nullsleep 5done
ns,Currently  each ExecutionCommandEntity stores the whole config blob. This is a severe duplication of data as tagged configuration information is already available as immutable entry.We need to reduce the footprint of ExecutionCommandEntity by storing only configuration tags. The tags can be replaced with actual configuration value when the command is handed off to the agent.We need to ensure that when Ambari is upgraded and there is a mix of ExecutionCommandEntity instances with and without embedded config - it works.
ns,The ambari-agent service script should return non-zero when the agent is not running. For example  if a customer wants to have puppet ensure the service is always running  it will not start a killed service because it thinks it's already running when it returns 0.&#91;root@host-123-123-123 init.d&#93;# service ambari-agent statusambari-agent currently not runningUsage: /usr/sbin/ambari-agent {start|stop|restart|status}&#91;root@host-123-123-123 init.d&#93;# echo $?0For comparison...&#91;root@host-123-123-123 init.d&#93;# service winbind statuswinbindd is stopped&#91;root@host-123-123-123 init.d&#93;# echo $?3Possible fix:AMBARI_AGENT_PID_PATH='/var/run/ambari-agent/ambari-agent.pid';RES='3';if [ -f $AMBARI_AGENT_PID_PATH ]then RES='cat $AMBARI_AGENT_PID_PATH | xargs ps -f -p | wc -l'; AMBARI_AGENT_PID='cat $AMBARI_AGENT_PID_PATH';else RES=-1;fiif [ $RES -eq '2' ]then echo 'OK: Ambari agent is running &#91;PID:$AMBARI_AGENT_PID&#93;'; exit 0;else echo 'CRITICAL: Ambari agent is not running &#91;$AMBARI_AGENT_PID_PATH not found&#93;'; exit 2;fi
ns,New ganglia version retains almost 50 times more data for higher resolution metrics collection. Ambari needs to modify the configuration to match the older resolution as the new default config has a very high disk space requirement.Looks like the default config for ganglia changed fromRRAs 'RRA:AVERAGE:0.5:1:244' 'RRA:AVERAGE:0.5:24:244' 'RRA:AVERAGE:0.5:168:244' 'RRA:AVERAGE:0.5:672:244' 'RRA:AVERAGE:0.5:5760:374'toRRAs 'RRA:AVERAGE:0.5:1:5856' 'RRA:AVERAGE:0.5:4:20160' 'RRA:AVERAGE:0.5:40:52704'Its an increase from 1350 data points to 78720. After reverting to older configuration the file size for a single metrics and its summary info are-rw-rw-rw- 1 nobody nobody 12224 Sep 7 18:29 ./HDPNameNode/c6402.ambari.apache.org/disk_free.rrd-rw-rw-rw- 1 nobody nobody 23656 Sep 7 18:29 ./HDPNameNode/__SummaryInfo__/disk_free.rrdIn contrast it was-rw-r--r-- 1 root root 630768 Sep 7 17:48 /tmp/rrds/HDPNameNode/c6402.ambari.apache.org/disk_free.rrd-rw-r--r-- 1 root root 1261000 Sep 7 17:47 /tmp/rrds/HDPNameNode/__SummaryInfo__/disk_free.rrdThe recommendation is to revert back to the old config (i.e. do not use the new default config). Confirming that with an older installation of Ambari.
ns,PROBLEM: When installing with Ambari and selecting an existing MySQL database for oozie and a new MySQL database for Hive  then the Ambariinstall fails when installing the oozie database. The error thrown is 'could not connect to database' [it appears to drop the 'existing' databaseand you are required to start MySQL on the machine and create the database]BUSINESS IMPACT: This will affect all customers who choose an existing MySQL database for oozie and a new MySQL DB for hive when installing a cluster using AmbariSTEPS TO REPRODUCE: Choose an exisiting oozie MySQL database and point Ambari at this  and select a new Hive MySQL database on the installation optionsACTUAL BEHAVIOR: It seems that it drops the exisiting oozie database and then fails with an errot of could not connect (This is due to MySQL being down  but also the Oozie database is not there anymore)After starting MySQL and creating the database then the installer can continue from where it left off.EXPECTED BEHAVIOR: The installer should not stop MySQL and drop the oozie database if you select create a new Hive database and exisiting MySQL database for oozie.SUPPORT ANALYSIS: Reproduced in the lab in HDP 1.3.2 by following the steps above.
ns,ambari-server command outputs should point to a generic link for current ambari documentation.
ns,Yarn smoke test uses REST api exposed by ResourceManager to get its status. After configuring web authentication yarn client that is assigned yarn service check needs to negotiate 401 HTTP authentication response received while using REST api.
ns,ZKFailoverController should be shown as a component that can be started/stopped in Host Details page
ns,Steps to reproduce1. Go to Services page2. click on different services. They all have a status and a message for status in 'Alerts and Health Checks' list (as example hive.png)3. WebHCat service has a status but does not have a status message
ns,I have a host with 4 stopped host_components. When I issue a DELETE on say http://c6401:8080/api/v1/clusters/vmc/hosts/c6404.ambari.apache.org/host_components/DATANODEThe response is:{ 'status' : 500  'message' : 'org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: To remove master or slave components they must be in MAINTENANCE/INIT/INSTALL_FAILED/UNKNOWN state. Current=INSTALLED.'}
ns,ambari-server reset fails due to syntax error in centos 5.8 (postgres 8.1).psql:/var/lib/ambari-server/resources/Ambari-DDL-Postgres-DROP.sql:18: LINE 1: DROP DATABASE IF EXISTS ambari;The IF EXISTS clause was only added to DROP command in PotgreSQL8.2.+Show warnings if any SQL commands failed during server reset or upgradestack
ns,Due to backend change of the position of NameNode startTime property  this will not show up.
ns,Ambari does not set the correct value for 'templeton.storage.class' in webhcat-site.xmlIn an Ambari deployed cluster currently the following value in /etc/hcatalog/conf/webhcat-site.xml is set incorrectly: &lt;property&gt; &lt;name&gt;templeton.storage.class&lt;/name&gt; &lt;value&gt;org.apache.hcatalog.templeton.tool.ZooKeeperStorage&lt;/value&gt; &lt;/property&gt;With the change from HIVE-4895 this should be: &lt;property&gt; &lt;name&gt;templeton.storage.class&lt;/name&gt; &lt;value&gt;org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage&lt;/value&gt; &lt;/property&gt;All jobs involving MapReduce fail because of this issue.
ns,These logs do not need to be at INFO level. When user is using the UI then theer is roughly one log per second.06:56:34 232 INFO [pool-1-thread-12] URLStreamProvider:81 - readFrom spec:http://c6402.ambari.apache.org:50075/jmx06:56:34 235 INFO [pool-1-thread-5] URLStreamProvider:81 - readFrom spec:http://c6401.ambari.apache.org:50075/jmx06:56:34 248 INFO [qtp1620999494-18] URLStreamProvider:81 - readFrom spec:http://c6401.ambari.apache.org/cgi-bin/rrd.py?c=HDPSlaves&amp;h=c6401.ambari.apache.org c6402.ambari.apache.org&amp;m=cpu_wio jvm.JvmMetrics.MemHeapUsedM rpc.rpc.RpcQueueTimeAvgTime jvm.JvmMetrics.MemHeapCommittedM&amp;e=now&amp;pt=true...
ns,When integrating Ambari with LDAP if you specify the Bind DN  or Bind credentials that are invalid there is no logging to identify that the authentication fails  so the following search for the logging in user DN will fail. I had to use wireshark to figure out why the integration wasn't working.
s,1. Add 'ambari-server setup-security' to replace all of the following operations:'setup-https|setup-ganglia-https|setup-nagios-https|encrypt-passwords'2. Add new operation 'setup-kerberos-auth' to ask the user for: ambari.keytab ambari.principalRelated to  https://issues.apache.org/jira/browse/AMBARI-2941
s,STR: Go to Admin page -&gt; Security tab Switch to another tab on admin page (Misc tab  HA tab  etc.) Go back to Security tab--------------------Verify that all input fields on this page are read-onlyExpected Result: Everything should be read-only.Actual Result: Some fields become editable .
ns,We have various experimental functionality provided in UI. We need an easy UI page http://server:8080/#/experimental to enable/disable these functionalities.This will make it easy for users to test these functionalities and give feedback.Refresh of Ambari UI will clear the changes.
ns,Current text:-c JCE_POLICY  --jce-policy=JCE_POLICY Use specified jce_policy. Must be valid on all hostsThis is required only on ambari-server  the agents will download from the server.
ns,Cleanup UX...Enter advanced database configuration [y/n] (n)? y==============================================================================Choose one of the following options:[1] - PostgreSQL (Embedded)[2] - Oracle==============================================================================Enter choice (1):
ns,Dashboard page: buttons are shifted if screen width is more than 1200px
ns,This happened because when fixing an old issue  jobTrackerCpu got ignored about that fix.
ns,Help Text for NameService ID when enabling HA is random in responding to mouse movement and clicks.
ns,Earlier dfs.web.authentication.kerberos.keytab field was being used for NameNode and SNameNode component. So we planned to pull this key to NameNode category when HA is enabled as it's the only component then using the key.After HDFS-5091 fix  journalNode also uses this key.So instead of pulling this config key in NameNode section  it should be kept in General category and the description of this principal and keytab location field should be changed accordingly.
ns,Notice after agent registration (automatic with SSH key)  I see the SSH key in the popup. Also see it in the ambari-agent.log.Not sure we want to capture this in the log and show in the UI?
ns,Unavailable stacks should be hidden.
ns,Allow the latest stack repo url to be parameterized for the build. This allows the build to cater to the stack repo as it goes through dev/private/public builds each with different URLs.
ns,Services &gt; YARNSee screen shot.Should say 'Percent NodeManagers live'
ns,Fix Unit tests and create new test for step3 installer
ns,HDFS service check should not be disabled when NN HA is enabled and one NN is down. In fact  service check passing is an indication that NN is available with one NN down.
s,Security wizard: 'Create Principals and Keytabs' step doesn't save state after page refresh
ns,Use correct case for YARN
ns,Constrain loading hostComponents into model  so then it loads only on initial loading or when added new hostComponents.
ns,Running mvn test on agent fails/hangs under Mac Os
ns,In Ambari Web  browse to HDFS  YARN  MapReduce2  etc. Click on the Configs tab  the Quick Links option disappears.See the same regardless of 1.3.2 or 2.0.6  and tried on Firefox and Chrome
ns,Navigation from Hosts to Services page or from Hosts to Dashboard pagesometimes fails (nothing happens besides highlighting 'Services' tab) and sometimes navigates to an empty page.Uncaught Error: assertion failed: calling set on destroyed object ember-latest.js:43Ember.assert ember-latest.js:43set ember-latest.js:1386Ember.Observable.Ember.Mixin.create.set ember-latest.js:7769App.MainServiceMenuView.Em.CollectionView.extend.renderOnRoute menu.js:52invokeAction ember-latest.js:3174iterateSet ember-latest.js:3156sendEvent ember-latest.js:3273notifyObservers ember-latest.js:1865Ember.notifyObservers ember-latest.js:1980propertyDidChange ember-latest.js:2613set ember-latest.js:1419(anonymous function) ember-latest.js:10459f.event.dispatch jquery-1.7.2.min.js:3h.handle.i jquery-1.7.2.min.js:3
ns,When NameNode HA is config'd:1) HDFS status green if and only if there is Active NameNode; red otherwise2) green -&gt; HDFS Service Stop enabled  HDFS Service Start disabled. red -&gt; HDFS Service Start enabled  HDFS Service Stop disabled
ns,When an non-existent file is provided as jce-policy parameter we should get ambari-server setup process failed instead of downloading the file from public repo.Also  if the path is a folder then we should gracefully error out instead of allowing shutil.copy to fail with an error stack.
ns,*This also affects trunk*The ambari-agent.spec (generated from rpm-maven-plugin) claims ownership of /usr/sbin $ grep sbin target/rpm/ambari-agent/SPECS/ambari-agent.spec | grep attr%attr(755 root root) /usr/sbinThis is a problem because the filesystem RPM owns /usr/sbin.According to rpm-maven-plugin documentation&#91;0&#93;  this is because the only file under /usr/sbin is ambari-agent and'directoryIncludedIf the value is true then the attribute string will be written for the directory if the sources identify all of the files in the directory (that is  no other mapping contributed files to the directory). This is the default behavior.'The 'no other mapping contributed files to the directory' bit is important.The solution is to add directoryInclude=false to the mapping.&#91;0&#93; http://mojo.codehaus.org/rpm-maven-plugin/map-params.html
ns,The property which defines the https address of namenode is dfs.namenode.https-address. In ambari   the property name is mentioned as 'dfs.https.namenode.https-address'
ns,Modify the upgrade mappings for hdfs-site  core-site  mapred-site  and global to reflect the latest.
ns,JSVC_HOME is being set to /usr/lib/hadoop/sbin/Linux-amd64-64/ instead of /usr/lib/bigtop-utils.
ns,All ajax-requests are completed  but progress bar doesn't filled to 100%.
ns,STD:Make the browser window's width less than actual width of the page.Go to Services page.Stop all services.Result:The number of alerts in MapReduce item in menu gets out on the next line.
ns,The oracle db was installed on a host where port 1521 was not accessible to the ambari-server host. However  'ambari-server setup' did not report failure.Enter advanced database configuration [y/n] (n)? y==============================================================================Choose one of the following options:[1] - PostgreSQL (Embedded)[2] - Oracle==============================================================================Enter choice (1): 2Hostname (localhost): test-sm1.iad1Port (1521):Select Oracle identifier type:1 - Service Name2 - SID(1):Service Name (ambari): XEUsername (ambari):Enter Database Password (bigdata):Copying JDBC drivers to server resources...Configuring remote database connection properties...Copying JDBC drivers to server resources...Ambari Server 'setup' completed successfully.
ns,Add hcat.bin to pig.properties for hcat integration.hcat.bin=/usr/bin/hcat
ns,On 2.x stack  dfs.block.local-path-access.user should not be set in hdfs-site
s,YARN cluster should not have shared directories between yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs
ns,Error with RM nagios alert for rpc latency.[1380746845] SERVICE NOTIFICATION: nagiosadmin;&lt;hostaname&gt;;RESOURCEMANAGER::ResourceManager RPC latency;CRITICAL;notify-service-by-email;CRITICAL: Data inaccessible  Status code = 200
ns,Clicking on 'all|none' affects disabled checkboxes.
ns,When hadoop.ssl.enabled=true  ResourceManager port is 8090. When it is false  the port is still 8088.
ns,STD:On the latter stages of installing cluster  refresh page 'Install  Start and Test'..Result:Appeared 'Next' button and progress of installation is setted to 100%  but message of installation on the second host says that the Nagios Server is not installed yet. After clicking 'Next' all seems good and Nagios Server is installed normally.
ns,When multiple MR2 Clients are installed  the MR2 summary panel has a label like: '3 MapReduce2 Client s Installed' (with an unnecessary space)
ns,Steps:Open browser console.Run start or stop operation for any service.Switch between some services repeatedly.Result:'Calling set on destroyed view' was appeared in console.
ns,The problem is that host can have actual status only when service mapper recieve response(could be long latency  about 10 - 12 seconds) with new hostComponents and then status mapper compute them and set status to host.
ns,In 1.5.0 release  ambari-agent will not need to process RCO to re-order/parallelizing tasks. Let's remove the code/unit-test and keep them aside in a JIRA targeted for release after 1.5.0. If possible  let's remove upgrade related code as well as there is no plan for automatic stack upgrade for Baikal. We can keep the python executor as there is a requirement for python executor.
ns,HBase Master/RegionServer can no longer be started after reconfiguring HBase or HDFS with NameNode HA enabled
ns,Hbase secure config properties in HDP-2.x stack revert back to non-secure values on reconfiguration
ns,Installed NodeManagers don't appears as selected checkboxes on the 'Assign Slaves' step.
ns,check_cpu needs to be disabled for Suse for JobHistory server and ResourceManager
ns,In host stauts filter label shows incorrect number of hosts after deploy failed.Wrong progress bar color  on fail color should be red or yellow  instead of blue.
ns,Select Hive or Oozie and go to step 'Customize services'.'Open' Oozie tab.Database Url is 'jdbc'.Click 'Existing MySQL Database'.Click 'New Derby Database'.Database Url became jdbc:derby:${oozie.data.dir}/${oozie.db.schema.name}-db;create=true.Expect:proper value should be right after step is loaded.
ns,Hadoop Core Health Check script needs to be included in Ambari HDP installations
ns,Services like PIG  Sqoop can't be customized.Wizard should check services-list (that user want to add) and if no one service can't be customized  should skip 'Customize' step ('Back' click on the next step should also be verified).
ns,Allow log4j properties to be applied via the API in Ambari for hadoop/oozie/hbase/hive/zookeeper/pig.
ns,Install HDFS with customized hostname hdfs1. Start NameNode HA wizard and Refresh on step-2 (select host). Proceed ahead. Create checkpoint step asks to run command with incorrect user name:sudo su -l hdfs -c 'hdfs dfsadmin -safemode enter' Above command returns safemode: Access denied for user hdfs. Superuser privilege is required Actual command should be:sudo su -l hdfs1 -c 'hdfs dfsadmin -safemode enter'
ns,Go to Config Step on the addServiceWizard.Refresh page.Got JS error  because selected services where not saved.Expect: get page with config list for selected services.
ns,As documented in AMBARI-3531  the restart flags will be provided in host_components itself and services will have an API to get restart host_components easily. Due to this  there is no need for actual_configs on the client  and the code to calculate diffs with global properties.
ns,This ticket mainly covers UI label and message changes in reassign master wizard.
ns,Some wizards (installer  addHosts  addServices) use common local storage objects.But each should has separated object (for example  based on controllerName).
ns,Service reconfiguration fails for HDFS  MapReduce and Hive service with js error. For other services it fails silently without any error (no API call is triggered).
ns,The /etc/redhat-release on my RHEL6.4 dev box containsRed Hat Enterprise Linux Workstation release 6.4 (Santiago)os_type_check.sh is checking for 'Server' on rhel boxes. It should simply check for Red Hat Enterprise Linux and ignore text up to the version number.
ns,1. Use a shadow with high contrast to make the current filter more visible.2. Show how many hosts total / how many are in the current view  and a 'Clear all filters' link.
ns,Steps: Go to 'Admin' page -&gt; 'High Availability' tab and run 'Enable NameNode HA' wizard. Close wizard. Sign out (or reopen browser). Sign in.Result:After sign in was opened first page of 'Enable NameNode HA' wizard instead 'Dashboard' page.
ns,Ambari agent creates an empty directory /var/ambari-agent during installation.This directory isn't needed  /var/run/ambari-agent is used instead.
ns,1. Do the right-float on the action menus on the Components section.2. Rename buttons: on SERVICE PAGES: Maintenance --&gt;Service Actions...on HOST PAGES: Maintenance --&gt; Host Actions...on HOST PAGES / COMPONENT SECTION: Actions --&gt; Actions...
ns,make dialog according to left mockup
ns,Steps to reproduce: On installer wizard  make install phase fail by killing any install task of master component. Go back and change hdfs username to hdfs1. Proceed ahead and installer wizard completes successfully. HDFS service is red. Nagios shows no alerts  but API returns INSTALLED status for all hdfs host components. UI impact: On starting HDFS  all tasks completes successfully with 100% green progress bar but service status always remains red. Restarting agent resolves the issue.Looks like AmbariConfig.servicesToPidNames is not getting updated when username is changed.
ns,I attempted to stop the ambari-agent without going to root first. Prints a pretty bad traceback.&#91;vagrant@c6403 ~&#93;$ ambari-agent stop/usr/sbin/ambari-agent: line 66: /var/lib/ambari-agent/ambari-env.sh: Permission deniedVerifying Python version compatibility...Using python /usr/bin/python2.6Found ambari-agent PID: 2996Stopping ambari-agentTraceback (most recent call last):File '/usr/lib/python2.6/site-packages/ambari_agent/main.py'  line 235  in &lt;module&gt;main()File '/usr/lib/python2.6/site-packages/ambari_agent/main.py'  line 190  in mainsetup_logging(options.verbose)File '/usr/lib/python2.6/site-packages/ambari_agent/main.py'  line 73  in setup_loggingrotateLog = logging.handlers.RotatingFileHandler(logfile  'a'  10000000  25)File '/usr/lib64/python2.6/logging/handlers.py'  line 112  in initBaseRotatingHandler.init(self  filename  mode  encoding  delay)File '/usr/lib64/python2.6/logging/handlers.py'  line 64  in initlogging.FileHandler.init(self  filename  mode  encoding  delay)File '/usr/lib64/python2.6/logging/init.py'  line 827  in __initStreamHandler.init(self  self._open())File '/usr/lib64/python2.6/logging/init.py'  line 846  in _openstream = open(self.baseFilename  self.mode)IOError: &#91;Errno 13&#93; Permission denied: '/var/log/ambari-agent/ambari-agent.log'Removing PID file at /var/run/ambari-agent/ambari-agent.pidrm: cannot remove '/var/run/ambari-agent/ambari-agent.pid': Permission deniedambari-agent successfully stopped
ns,Steps:Stop YARN service.Go to 'Dashboard'.Result:'NodeManagers Live' widget contains 'null' values.Similar problem is present for 'HBase Ave Load' widget - 'NaN' value.
ns,Occurred during install as warning (could not start the service). Clicked next to continue  when into Ambari  then tried to start Hive there as well  same issue.
ns,UI needs to know which host_components need restart due to stale_configs (saved but not picked up). Server API provide stale_configs flag per host-component. We need this polled and maintained on client model.
ns,When we have 3 HBase masters we show in UI that one of them is active. When the master is stopped  the other HBase master is not marked as active in UI. In API it does become active.
s,The default value of 'Default virtual memory for a job's map-task' (in Customize Services page -&gt; MapReduce2 tab -&gt; General) was '619.5'.Warning hint says 'Must contain digits only'Value depends on quantity of installed components.
ns,On a few days old cluster I attempted to add a host. The add host failed due to ambari-server trying to install ambari-agent-1.4.1.17 and the repo having ambari-agent-1.4.1.23-1.The message in /var/run/ambari-server/bootstrap/11/hostname.log was:STDERRscp /usr/lib/python2.6/site-packages/ambari_server/setupAgent.py done for host srimanth1-5.c.pramod-thangali.internal  exitcode=0Copying files finishedRunning setup agent...STDOUTError: Nothing to do{'exitstatus': 1  'log': ('Loaded plugins: downloadonly  fastestmirror  security/nDetermining fastest mirrors/n * base: www.gtlib.gatech.edu/n * extras: centos.mirror.netriplex.com/n * updates: mirror.cogentco.com/nSetting up Install Process/nNo package ambari-agent-1.4.1.17 available./n'  None)}
ns,This task fails due to bad request:/api/v1/clusters/c1/hosts/HDFS_CLIENT/host_components/dev01.hortonworks.comShould be: api/v1/clusters/c1/hosts/dev01.hortonworks.com/host_components/HDFS_CLIENT
ns,On the Hosts page  we have a typo in the label 'filterd'. Should be 'filtered'.
ns,STR: Install  setup and start Ambari server by default. Reach 'Choose services' phase of installer.Actual result:'Confirm hosts' shows warning that ntpd service isn't running on hosts  but it's running in console by command service ntpd status
ns,python /usr/lib/python2.6/site-packages/ambari_agent/HostCleanup.py -s -k 'users'To cleanup in interactive mode  remove *-s* option. To cleanup all resources  including _users_  remove *-k users* option. Use *--help* for a list of available options. The motivation is to provide the conservative option but minimal detail to allow for full clean up.
ns,For a 657 node cluster:~10 minute for 10 MB and 20 log files store about 200 minutes (~3 hours) of log. This is not ideal if an error overnight needs to be investigated. We should try for the log to last 24 hours - ideally 72 hours to account for weekends.-rw-r--r-- 1 root root 10485854 Oct 17 17:35 ambari-server.log.6-rw-r--r-- 1 root root 10485836 Oct 17 17:44 ambari-server.log.5-rw-r--r-- 1 root root 10485811 Oct 17 17:52 ambari-server.log.4-rw-r--r-- 1 root root 10485793 Oct 17 18:01 ambari-server.log.3-rw-r--r-- 1 root root 10485793 Oct 17 18:10 ambari-server.log.2-rw-r--r-- 1 root root 10485854 Oct 17 18:18 ambari-server.log.1
ns,Modifications of dynamic properties are being persisted on server  but default values are being shown in UI. Also  we should not validate dynamic configs which are of type string. mapreduce.map.java.opts mapreduce.reduce.java.opts yarn.app.mapreduce.am.command-opts
ns,For the 'defaultsProvider' and 'serviceValidator' functionalities  we need unit tests
ns,Steps: Open 'Reassign Master Wizard' for NameNode or SNameNode. Go to 'Manual commands' page. Close browser and open it again.Result: Was opened 'Manual commands' page  but hostnames and foldername were replaced with '{1}'  '{2}' etc.Attached picture for other page  but behavior is similar.
ns,If a user is using local repos  and customizes Advanced Repository Options during install  the user might need this info to debug post install (since it is used in Add Hosts)  for example.Note: We should show this information regardless if the user customizes repos or not during install.
ns,When host has slave down status 'No Heartbeat' status is shown instead of 'Slave Down'.
ns,Goto Service (that have stale configs) -&gt; configs   no restart indicators are shown. Refresh page  for 10-15 seconds you see indicators then they disappear. The same for host detail page
ns,Ganglia monitor started with second or third attempt on secure cluster
ns,The OK and the text should be on the same centerline row.
ns,Following error messages are printed to log with default log level and are misleading.04:19:52 438 ERROR [main] MasterKeyServiceImpl:109 - Master key is not provided as a System property or an environment varialble.04:19:52 439 INFO [main] Configuration:415 - Credential provider creation failed.Master key initialization failed.
ns,When there are multiple HBase Masters  the HBase Links widget looks broken. Let's get rid of the 'and X Standby Masters' static text as it is not a link and not very useful.So the widget would look like:'HBase Master''X RegionServers''Master Web UI'
ns,This is a 2-node cluster with 2GB of RAM each. Cluster deployment goes fine but MR jobs do not complete resulting in service check failures for MR  OOZIE  Pig  etc.
ns,See screenshot
ns,When adding hosts  a user should be able to select which config-groups this host belongs to. Configurations of that group (Default or config-group) will be applied on that host.
ns,This one was discovered while quick navigating through services on Services page.To reproduce just try to click on services links fast.After that service content is not displayed.
ns,http://hortonworks.com/community/forums/topic/installing-hdp2-0-6-on-centos6-4/The current ERROR in the agent log can be cryptic.
ns,Ambari allows changing Ganglia directory during installation  but not after the cluster is installed. We should allow changing this directory after cluster installed
ns,UI makes a call to http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions?fields=Versions operatingSystems/repositories/Repositories.The API is missing the operatingSystems info  except for the suse11 one:{ 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions?fields=Versions operatingSystems/repositories/Repositories'  'items' : [ { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/1.2.0'  'Versions' : { 'active' : false  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '1.2.0' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/1.2.1'  'Versions' : { 'active' : false  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '1.2.1' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/1.3.0'  'Versions' : { 'active' : false  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '1.3.0' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/1.3.2'  'Versions' : { 'active' : true  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '1.3.2' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/1.3.3'  'Versions' : { 'active' : true  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '1.3.3' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.5'  'Versions' : { 'active' : false  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '2.0.5' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6'  'Versions' : { 'active' : true  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'operatingSystems' : [ { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/centos5'  'OperatingSystems' : { 'os_type' : 'centos5'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/centos6'  'OperatingSystems' : { 'os_type' : 'centos6'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/oraclelinux5'  'OperatingSystems' : { 'os_type' : 'oraclelinux5'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/oraclelinux6'  'OperatingSystems' : { 'os_type' : 'oraclelinux6'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/redhat5'  'OperatingSystems' : { 'os_type' : 'redhat5'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/redhat6'  'OperatingSystems' : { 'os_type' : 'redhat6'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/sles11'  'OperatingSystems' : { 'os_type' : 'sles11'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/suse11'  'OperatingSystems' : { 'os_type' : 'suse11'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/suse11/repositories/HDP-2.0.6'  'Repositories' : { 'base_url' : 'http://public-repo-1.hortonworks.com/HDP/suse11/2.x/updates/2.0.6.0'  'default_base_url' : 'http://public-repo-1.hortonworks.com/HDP/suse11/2.x/updates/2.0.6.0'  'mirrors_list' : null  'os_type' : 'suse11'  'repo_id' : 'HDP-2.0.6'  'repo_name' : 'HDP'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' } } ] } ] } ]}
ns,Various config-group actions (add/remove/rename/duplicate) should be provided in the manage-config-groups dialog. Any of these should not rely on the Save button  but are immediately persisted via API.
ns,Currently  we store disks info in DB and this information is never updated after registration. As disks details can change (space availability changes  disks get mounted/unmounted  etc.) the persisted information is not useful.We should instead hold the details in memory and refresh it at certain intervals (e.g. once every 10 minutes) and then alert if space availability hits some lower limit.
ns,Disable and gray out the Add Services button if there aren't any more services to be added.Upon hover  show a tooltip saying 'No more services to be added'.Although we hide the 'Add Component' button when no more components are to be added  we don't actually like that and want to move towards 'disabling/graying out with hover tooltip' pattern.
ns,In the Manager Configuration Groups dialog I selected the Default config-group  and the actions to remove and rename config-group are enabled. These ops are not allowed for Default config-group.
ns,On a host's configs page  provide a Change action beside config-group to switch from one group to another  or to Default.
ns,I had only the Default config-group when I launched the manage config-groups dialog. When I clicked on Cancel or X  it would not close dialog with the following error:Uncaught TypeError: Cannot read property 'id' of null This was in method updateConfigGroupOnServicePage() at the first line belowselectedConfigGroup = managedConfigGroups.findProperty('id'  selectedConfigGroup.id); if(selectedConfigGroup){ mainServiceInfoConfigsController.set('selectedConfigGroup'  selectedConfigGroup); }else{ mainServiceInfoConfigsController.set('selectedConfigGroup'  managedConfigGroups.findProperty('isDefault'  true)); }
ns,When you go to host configs page  there is no gap between the services sidebar and the tabs. This should be changed so that the config-group bar and services sidebar have the same gap from the tabs at top.
ns,When App.supports.hostOverridesHost is enabled  and you visit the configs page for a host  the services are in some random order. They should be in the same order as the services page.Also  there is a small empty entry in the menu  which gives like a 5px extra space between some services. You can even hover on this empty entry and it will highlight.
ns,After creating new Config Group in Manage Configuration Groups dialog try to add host to this group and save. Also sometimes '+' button to add hosts is disabled for newly created group.
ns,This happens because non-admin users are not authorized to make POST/PUT calls to any resource  including 'persist'.For now  let's hide 'Settings' if the user is a non-admin user.
ns,1. Add two hosts in Add Host Wizard.2. In Conform Hosts step  one registered successfully  the other failed.3. Proceed to next  JS error happened when deploy  wizard UI hang up.
ns,Add host fails after configuring NN HA with JavaScript error
ns,Test resource User of resource management . Test all actions and all the attributes. Please make sure we mock to check both cases when user already exists and when it's not.Note here we should not call directly provider methods like action_create  but make resource management library do that for us  by calling env.run(). In other case test won't cover resources definitions  and other import logic
ns,Override like 3 configs in a config group and save. Now go to the Manage Config Groups dialog and select this group and click on Duplicate action. A popup with name and description pops up and hitting OK creates the duplicated config group.Though this duplicated config group has the correct name/description  it does not have the duplicated configs (3 that we overrode). When doing the POST call  we should populate the desired_configs to be the exact same as the source config-group.
ns,PROBLEM: The ResourceManager Heap metrics on Ambari web console doesn't show the correct value  not only the current heap usage doesn't reflect the correct usage  but also the total heap size doesn't match what we configure for the ResourceManager Heap size  even the total heap size number is changing constantly.
ns,This issue was discovered while upgrading the cluster to hash 87adc8c2d29b20a30f01e54c12f67dcbbe34b32e of 1.4.2 branch. The logic inside configuration_controller.js function getConfigsByTags(tagObject) has a reference to undefined variable and js error was encountered. This happened when any of the service page was rendered and program flow from quick_view_link_view.js function didInsertElement() -&gt; setQuickLinks() -&gt; loadTags() -&gt; loadTagsSuccess(data) -&gt; getSecurityProperties() -&gt; configurationController.getConfigsByTags(tag)
ns,Background operations popup window minimum size should be fixed when narrowing down the browser
ns,When components and hosts required to restart we show refresh icon near the service name. On icon hover event we show tooltip with count of components and hosts.
ns,Screenshot attached.
ns,When config-groups are used for HDP 1.3.x stack services  the stale_configs are always false.
ns,'HDFS Short-circuit read' config property is repeated
ns,Modify messages from 'reassign master' to 'move master'
s,Trying to enable security  the configurations page is blank.
ns,Cluster should have service(HDFS PIG SQOOP) which doesn't have any slave or client host-components.1. Run Add Host wizard2. Proceed to configuration stepResult: Wizard popup stuck in loading processJS error:Uncaught TypeError: Cannot call method 'get' of undefined app.js:10245(anonymous function) app.js:10245App.AddHostController.App.WizardController.extend.loadServiceConfigGroups app.js:10242(anonymous function) app.js:43659f.Callbacks.o vendor.js:95f.Callbacks.p.add vendor.js:95(anonymous function) app.js:43657f.Callbacks.o vendor.js:95f.Callbacks.p.fireWith vendor.js:95f.Callbacks.p.fire vendor.js:95(anonymous function)
ns,I opened the manage-config-groups dialog and hovered on one of the actions (remove host from config-group). Then I saved or cancelled to close the dialog. The hover still remains in the middle of page.I have seen this in other usages as well. We need to make sure that all hovers are closed when the focus is lost.
ns,Steps:1. Create 2 config groups2. Rename 1 config group3. Add hosts to renamed config group4. Add host to other config group.Result:Save stops working. Error in the JS console.Error in JS console:Uncaught TypeError: Cannot call method 'sort' of undefined manage_config_groups_controller.js:460(anonymous function) manage_config_groups_controller.js:460(anonymous function) manage_config_groups_controller.js:458ComputedPropertyPrototype.get ember-latest.js:2949get ember-latest.js:1355getPath ember-latest.js:1477get ember-latest.js:1348Ember.Observable.Ember.Mixin.create.get ember-latest.js:7695App.ModalPopup.show.onPrimary item.js:251newFunc ember-latest.js:949ActionHelper.registeredActions.(anonymous function).handler ember-latest.js:19458(anonymous function) ember-latest.js:11250f.event.dispatch jquery-1.7.2.min.js:3h.handle.i
ns,When execute any service check  host message become empty. Server return  unsupported on UI  command of task - 'SERVICE_CHECK'.
ns,JS error when switching config groups in Hive / Oozie service config pages
ns,When multiple HBase Masters are set up  HBase service-level alert section shows multiples of the following: HBase Master process HBase Master Web UI HBase Master CPU utilizationThe label is exactly the same for all masters  so you can't distinguish which alert is for which master. Ambari should mirror what ambari does for NameNode alerts so that the user can tell them apart (append hostname in the alert label).
ns,Issue 1:Steps followed: 1. Install a 3-node cluster with Hbase and Zookeeper and 3 zookeeper servers. 2. After installation on each host the property hbase.zookeeper.quorum in /etc/hbase/conf/hbase-site.xml has:&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;c6401.ambari.apache.org c6402.ambari.apache.org c6403.ambari.apache.org&lt;/value&gt;3. After adding a host with Hbase Region Server  and after that a ZookeeperServer to it  the property on the added c6404.ambari.apache.org host has the same value  as in 2. 4. After restarting HBase master on c6401 (which is proposed by UI)  the property value on c6401 becomes:  &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;c6401.ambari.apache.org&lt;/value&gt;. On other hosts it remains unchanged. 5. After restarting HbaseRegionServers on the rest of the hosts (not proposed by ui)  the property changes too  to the same value  as in 4. __________________________________________________________Issue 2:Property templeton.zookeeper.hosts in /etc/hcatalog/conf/webhcat-site.xmlPrior to adding a zookeeperServer on host c6404  config on WebHCat server lookes like:[root@c6402 vagrant]# cat /etc/hcatalog/conf/webhcat-site.xml |grep templeton.zookeeper.hosts -C 2 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;templeton.zookeeper.hosts&lt;/name&gt; &lt;value&gt;c6401.ambari.apache.org:2181 c6402.ambari.apache.org:2181 c6403.ambari.apache.org:2181&lt;/value&gt; &lt;/property&gt;After adding a ZookeeperServer on c6404 and restarting WebHCatServer on c6402: [root@c6402 vagrant]# cat /etc/hcatalog/conf/webhcat-site.xml |grep templeton.zookeeper.hosts -C 2 &lt;/property&gt; &lt;property&gt; &lt;name&gt;templeton.zookeeper.hosts&lt;/name&gt; &lt;value&gt;c6401.ambari.apache.org&lt;/value&gt; &lt;/property&gt;__________________________________________________________Issue 3. On a 3-node cluster with HA-enbaled  property ha.zookeeper.quorum in /etc/hadoop/conf/core-site.xml  has the same value on all hosts:  &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;c6401.ambari.apache.org:2181 c6402.ambari.apache.org:2181 c6403.ambari.apache.org:2181&lt;/value&gt;It doesn't change on either of the hosts after adding c6404 to the cluster  installing ZookeeperServer on it and restarting HDFS.
ns,Services mysteriously disappear after Stack upgrade
ns,STR:Go through the Reassign NameNode wizard.On the last step click on the Start All Services link.Change the state of flag Do not show this dialog again when starting a background operation.Click OK.Click Start All Services link again.Result: State of flag was not changed.
ns,1. Change config group for a service.2. Click 'Stop components' on service config page.3. Click 'Start components' on the same page.Actual results:Background Operations popup will always show up. (as attached)Expected results:Load the 'do not show this dialog..' flag first  then determine if show this popup.
ns,In Services -&gt; Configs YARN and MapReduce2 configs is not displayed.
ns,AbstractProviderModule.updateClusterVersion sets the cluster version for the resource providers with the following ... PropertyHelper.MetricsVersion version = clusterVersion.startsWith('HDP-1') ? PropertyHelper.MetricsVersion.HDP1 : PropertyHelper.MetricsVersion.HDP2;So  the Cluster/version property set to 'HDPLocal-1.3.2' will incorrectly be detected as HDP2. This causes the property providers to use the wrong metric mapping files which causes many JMX properties not to be set properly.
ns,When any service config-group is saved  we have a confirmation popup saying save was successful. We should enhance that popup to have a button to Manage Config Groups dialog  along with appropriate message. When button is clicked  the popup should go away and the Manage Config Groups dialog should show.
ns,1) Browse to Services &gt; HDFS &gt; Configs2) Change some props3) Do not click save4) Browse away  to Summary or to another serviceUser would have lost config changes. We should prompt before allowing user to navigate away from Configs.'You have unsaved changes. Save changes or discard?'&#91;Discard&#93; &#91;Save&#93;
ns,Cluster wide we prohibit reuse of config-group name. However names can be reused across services. The API should be updated to tolerate POST/PUT of similar named config-groups.
ns,Steps: Open background operations window and select any operation.Result: If the host name is too long  they are not placed on designated place.Solution:If the host name is too long  we show part of the string with '...' at the end.Also the string should keep in a single line all the time
ns,In 'Customize services' step in 'Add Service Wizard' config group and configs are not displayed. See screenshot.
ns,Install cluster with 2 hosts (with HDFS  ZooKeeper).Go to Add Service Wizard.Select some configurable service.Go to Step 4 (Customize services).Close wizard.Go to Hosts page.Result:each host appears two times.
ns,Reason:We use in-consistent value to show Heap size for different components.And some of them are missing.Solution:Make sure all Heap size percentage value (including NN  RM  JT and HBase Master) use the same property.
ns,HiveSchema file for Hive should be hive-schema-0.12.0.oracle.sql
ns,On the dashboard  calls to /clusters/{clusterName}?fields=Clusters/desired_configs are made every 6s. We need to verify if this really is necessary. Initial investigation revealed calls from quick-links  where links were set based on security being enabled. But there is no need to poll every 6s for this.
ns,During reconfigure  in the Manage Config Groups dialog  the actions below the config-groups table (left table - Add/Remove/Duplicate/Rename) are immediate - you do not need to hit Save. Save is only for host membership changes. If host membership changes  the actions under left-table are disabled till Save. During install however  all actions are allowed till Save is hit. If you rename/duplicate and hit Cancel  all changes are lost - something which does not happen during reconfigure.The installer dialog should have similar behavior to reconfigure dialog.
ns,Refactor templates and popups
ns,Label on top of left hand config-group table should be removed Left hand table and right hand table should occupy 1/3 and 2/3 of the dialog.
ns,We wanted to change just the config-group description  but were unable to change without changing the name. The rename config-group dialog should allow changing description only also.
ns,We show the restart indicator for stale-configs on services and individual host. Since we already have that information on the client  we need to show that on the Hosts page table. We need a filter to select stale-config hosts  and also show beside each host the stale-config indicator.
ns,ambari-agent and server are not set to start automatically  so if a machine reboots it becomes inaccessible.
ns,On review page of install wizard/add host wizard -List the repos with their OS-Say 'Repositories'  not 'Local Repository'-Not a yes or no
ns,Installer's host list should show masters at top
ns,After enabling HA go to hosts page.Open host's page for host that has ZKFC.ZKFC doesn't have service.Expect:ZKFC should have HDFS as service.
ns,I selected HDP 2.0.8 stack and Centos5 base-url was bad. The reason was given in the response - but it is not shown anywhere in UI. On the red exclamation mark we need a tooltip showing the error message from server along with HTTP error code.
ns,The unit test failsFAIL: test_fqdnDomainHostname (TestHardware.TestHardware)----------------------------------------------------------------------Traceback (most recent call last): File '/home/dmitry/incubator-ambari/ambari-common/src/test/python/mock/mock.py'  line 1199  in patched return func(*args  **keywargs) File '/home/dmitry/incubator-ambari/ambari-agent/src/test/python/ambari_agent/TestHardware.py'  line 83  in test_fqdnDomainHostname self.assertEquals(result['hostname']  'ambari')AssertionError: 'dmitry-pc' != 'ambari'
ns,1) Create config group2) just enter a &lt;space&gt; for the name3) That enables the OK button to save4) You can save that config group w/o a name.is reproducible on installer (works fine after installation)
ns,Manage Config Group link appears on host detail page (and cannot dismiss it once opened)
ns,HA wizard not accessible after upgrade
ns,Steps: Create a config group for any of the mentioned services and add a host to the config group. Save the config group.Result:Restart indicators do not appear.
ns,Load Repositories info in the same way as for add Host Wizard.
ns,Add option to Actions menu for 'restart'. That should queue the stop and start tasks.We should assume 'RESTART' as a default command. The base/default implementation is to call STOP and then START.
ns,UI don't respond after enabling security
ns,Fix UI Unit tests
ns,YARN check execute fails on install. See attached logs.IP Address on Hosts Page is shown as OS NOT SUPPORTED (w/o js errors  cluster installed using regular Nano-CentOS5.9 image type)
ns,Fix versions of rpms in the stack to match the installed ones.
ns,On the Deploy step Global Configs are not sent to server.So  Nagios and Ganglia (if installed via add service wizard) don't have any configs in service config page.
ns,Misalignment of 'Filter' combobox on installer when browser window is narrow
ns,Remove double borders on alerts and host component page sections Remove some text to not wrap in 'delete host' dialog Remove 'Hosts' table header to clean-up config group dialog
ns,Steps: Go to any host with slave component (DataNode  for example). Click 'Stop' options for slave component.Result: status marker for slave component start blinking red  but status marker for host behaves strangely: sometimes it also start blinking red immediately/ sometimes changes status to not-blinking red only after slave component stopping will be ended.The same for slave component starting.
ns,For stack 1.x history server will be as separate master component. Add ability to choose host on which this component will be installed. Set value for 'mapreduce.history.server.http.address'  i.e. &lt;historyserverHost&gt;:&lt;historyServerPort&gt;.
ns,JS error: Uncaught TypeError: Cannot call method 'forEach' of undefined
ns,Redesign existing Mirroring page with datasets table according to new attached mockups.
ns,Padding for config override properties
ns,See the series of curl calls below.There are no components with stale configs but 4 components are being stopped. There are only 4 because Ambari only allows INSTALL call on already installed clients[root@c6401 vagrant]# curl -i -uadmin:admin -H 'X-Requested-By: ambari' http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/host_components?HostRoles/stale_configs=true{ 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/host_components?HostRoles/stale_configs=true'  'items' : [ ]}[root@c6401 vagrant]# curl -i -uadmin:admin -H 'X-Requested-By: ambari'-d '{'HostRoles': { 'state': 'INSTALLED'}}' -X PUT http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/host_components?HostRoles/stale_configs=true{ 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14'  'Requests' : { 'id' : 14  'status' : 'InProgress' }}[root@c6401 vagrant]# curl -i -uadmin:admin -H 'X-Requested-By: ambari' http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14{ 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14'  'Requests' : { 'aborted_task_count' : 0  'cluster_name' : 'c1'  'completed_task_count' : 0  'failed_task_count' : 0  'id' : 14  'progress_percent' : 9.0  'queued_task_count' : 4  'request_context' : ''  'request_status' : 'PENDING'  'task_count' : 4  'timed_out_task_count' : 0 }  'tasks' : [ { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14/tasks/178'  'Tasks' : { 'cluster_name' : 'c1'  'id' : 178  'request_id' : 14 } }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14/tasks/179'  'Tasks' : { 'cluster_name' : 'c1'  'id' : 179  'request_id' : 14 } }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14/tasks/180'  'Tasks' : { 'cluster_name' : 'c1'  'id' : 180  'request_id' : 14 } }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14/tasks/181'  'Tasks' : { 'cluster_name' : 'c1'  'id' : 181  'request_id' : 14 } } ]}
ns,When starting/stopping components based on the Start/Stop Components buttons that show up after service reconfiguration  it creates one request per component (as shown in the Background Operations popup). This floods the request history and the user cannot see what has been done prior. From the user's standpoint  Start Components and Stop Components actions should each show up as one request.Also  this has major performance implications  since multiple requests cannot be processed in parallel by the server (unlike tasks within a single request).
ns,Define mock data and make this functional in App.testMode.E2E integration will be a separate task.
ns,After reconfiguring  restart indicators do not show up on the Host pages sometimes.Clicking on an individual host shows the restart indicator in the Host Details page.
ns,Useful if you want to set AMBARI_JVM_ARGS in the environment. For example: Setting http proxy that Ambari Serverexport AMBARI_JVM_ARGS='-Dhttp.proxyHost=the.proxy.host -Dhttp.proxyPort=1234'ambari-server start
ns,For new services (those  that are not hardcoded at LiveStatus.py)  status commands are not executed.
ns,Move this 3 elements to the second line (under All  Healthy...).Also refactor their code-realization.
ns,Results :Failed tests: testDoWork(org.apache.ambari.server.state.scheduler.BatchRequestJobTest): (..)Tests run: 1265  Failures: 1  Errors: 0  Skipped: 8
ns,After routing by services on Service page number of calls to server(to get Alerts) grows.Also mock json with alerts need to be added.
ns,API call:curl -u admin:admin -H 'X-Requested-By:ambari' -i -X POST -d '[{'RequestSchedule':{'batch':[{'requests':[{'order_id' : '1' 'type':'POST' 'uri':'/api/v1/clusters/c1/requests' 'RequestBodyInfo':{'RequestInfo':{'context':'Restart Nagios' 'command':'RESTART' 'service_name':'NAGIOS' 'component_name':'NAGIOS_SERVER' 'hosts':'c6401.ambari.apache.org'}}}]} {'batch_settings':{'batch_separation_in_seconds':120 'task_failure_tolerance':1}}]}}]' http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/request_schedulesRequest Schedule:{href: 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/request_schedules/23' RequestSchedule: {batch: {batch_requests: [{order_id: 1 request_type: 'POST' request_uri: '/api/v1/clusters/c1/requests' request_body: '{'RequestInfo':{'context':'Restart Nagios' 'command':'RESTART' 'service_name':'NAGIOS' 'component_name':'NAGIOS_SERVER' 'hosts':'c6401.ambari.apache.org'}}' request_status: 'InProgress' return_code: 202}] batch_settings: {batch_separation_in_seconds: 120 task_failure_tolerance_limit: 1}} cluster_name: 'c1' description: null id: 23 last_execution_status: 'InProgress' schedule: null status: 'SCHEDULED'}}
ns,Add new master components to Install Wizard -&gt; Assign Masters page.
ns,Add master components Storm UI Server  DRPC Server  Log Viewer Server. Create config categories for components. Replace exist configs according to categories.
ns,1. There is no need to send request to restart clients.2. When user clicks on Restart All actions  components start restarting immediately. But all other actions shows confirmation popup like 'Are you sure?' (Start  Stop  Run Service Check etc).
ns,Task timeout is a configurable knob at the ambari-agent.timeout_seconds = 600This opens up the possibility of different timeout value at different agent instances as well as different value between the server and the agent. This can lead to state where server may have timed out the tasks but agent may not have. The other way is benign.This config can be moved to the server and server can hand it off to the agent when agent registers. This also makes it easier to manage the timeout value.
ns,Steps: Go to 'Customize Services' page. Open 'Manage Configuration Groups' window for any service. Add new custom group and assign host to it. Save changes. Open 'Manage Configuration Groups' window again. Select custom group and remove assigned host.Result: 'Save' button is disabled  'Add hosts' also is disabled.Note: after selecting default group 'Save' button become active.Also if after all steps try to add hosts to custom group  'Save' button will not be active.
ns,Move 1.3.4 stack to 1.3.3 using the pythin libraries.
ns,Rename 2.0.8 to 2.1.1 in the stack definition.
ns,For Slaves (DataNodes  NodeManagers  RegionServers  or TaskTrackers): Start Stop Restart Decommission Recommission
ns,HostCleanup should also check the following directories (and clean):/tmp/hadoop-*For example  if the /tmp/hadoop-nagios directory is present  but it doesn't have the right ownership/perms for nagios user  the Hive Metastore Nagios alert will occur. I saw this after doing an install on an un-clean machine.Hive Metastore statusCRIT for about a minuteCRITICAL: Error accessing Hive Metastore status [Error creating temp dir in hadoop.tmp.dir /tmp/hadoop-nagios due to Permission denied]An easy way to reproduce this:1) Perform install2) Go to Nagios server machine3) Change perms on /tmp/hadoop-nagios so that the nagios user does not have access4) The Nagios alert will fire
ns,The relocate resources python script is not a part of the rpm package.
ns,Currently when we make rolling restart API calls  the task_failure_tolerance value is given as count of hosts. Rather it should be percentage of total hosts.
ns,Currently  action definition are stored in database. This is not in-line with the declarative definition of stack and custom commands (as part of stack). The custom actions are essentially same as custom commands except they are defined at the level of clusters. So we should move the custom actions to XML formatted files that ambari-server can read when starting up. This means that when one needs to make any edit they will have to restart ambari-server. This requirement is OK as adding/modifying custom actions is not a frequently done operation.
ns,When Nagios not installed Alerts block should show message that it's not installed instead of spinner.
ns,For each host:P0: Start All Components Stop All ComponentsP1: Restart All Components
ns,dfs.datanode.data.dir must be handled as comma separated directories.
ns,ambari-server should use repo when downloading jdk 7
ns,Misc code cleanup
ns,Unit test: org.apache.ambari.server.state.ConfigGroupTest#testRemoveHostThis unit test is not a part of 1.4.3 branch  it was added later. (trunk)Exception thrown during ConfigGroupImpl.removeHost()2014-01-06 17:46:35 989 ERROR [main] configgroup.ConfigGroupImpl (ConfigGroupImpl.java:removeHost(274)) - Failed to delete config group host mapping  clusterName = foo  id = 1  hostname = h1java.lang.IllegalArgumentException: Object: org.apache.ambari.server.orm.cache.ConfigGroupHostMappingImpl@cc34948d is not a known entity type. at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.performRemove(UnitOfWorkImpl.java:3538) at org.eclipse.persistence.internal.jpa.EntityManagerImpl.remove(EntityManagerImpl.java:518) at org.apache.ambari.server.orm.dao.ConfigGroupHostMappingDAO.removeByPK(ConfigGroupHostMappingDAO.java:250) at com.google.inject.persist.jpa.JpaLocalTxnInterceptor.invoke(JpaLocalTxnInterceptor.java:58) at org.apache.ambari.server.state.configgroup.ConfigGroupImpl.removeHost(ConfigGroupImpl.java:272) at com.google.inject.persist.jpa.JpaLocalTxnInterceptor.invoke(JpaLocalTxnInterceptor.java:66) at org.apache.ambari.server.state.cluster.ClustersImpl.deleteConfigGroupHostMapping(ClustersImpl.java:640) at org.apache.ambari.server.state.cluster.ClustersImpl.unmapHostFromCluster(ClustersImpl.java:615) at org.apache.ambari.server.state.ConfigGroupTest.testRemoveHost(ConfigGroupTest.java:203)
ns,https://issues.apache.org/jira/browse/MAPREDUCE-5028https://issues.apache.org/jira/browse/MAPREDUCE-2308
ns,Property fs.checkpoint.size is deprecated. The upgrade script should remove it. Users can add the replacement themselves - dfs.namenode.checkpoint.txns.
ns,Background operations dialog goes into a weird state after hitting an exception with request_schedule information. It always expects request_schedule information to be present.
ns,STR: Deployed minimal cluster with HDFS and ZK. HDFS start failed. Added YARN+MR2  Nagios and Ganglia. Picture with HDFS was the same.Output:Fail: Execution of 'ulimit -c unlimited &amp;&amp; if [ 'ulimit -c' != 'unlimited' ]; then exit 77; fi &amp;&amp; export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start secondarynamenode' returned 1. -bash: line 0: ulimit: core file size: cannot modify limit: Operation not permittedFull folders with logs are attached.
ns,PROBLEM:ORA-01795: maximum number of expressions in a list is 1000 in Ambari Server log. Customer recently upgraded to Ambari 1.4.2Error is:08:54:51 320 ERROR [qtp1280560314-2070] ReadHandler:84 - Caught a runtime exception executing a queryLocal Exception Stack: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.4.0.v20120608-r11652): org.eclipse.persistence.exceptions.DatabaseExceptionInternal Exception: java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000Error Code: 1795Call: SELECT task_id  attempt_count  event  exitcode  host_name  last_attempt_time  request_id  role  role_command  stage_id  start_time  status  std_error  std_out FROM host_role_command WHERE (task_id IN (? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?)) ORDER BY task_id bind =&gt; [2551 parameters bound]Query: ReadAllQuery(referenceClass=HostRoleCommandEntity sql='SELECT task_id  attempt_count  event  exitcode  host_name  last_attempt_time  request_id  role  role_command  stage_id  start_time  status  std_error  std_out FROM host_role_command WHERE (task_id IN ?) ORDER BY task_id') at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:333) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:646) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeCall(DatabaseAccessor.java:537) at org.eclipse.persistence.internal.sessions.AbstractSession.basicExecuteCall(AbstractSession.java:1800)STEPS TO REPRODUCE: Over 1000 entries in the host_role_command and execution_command tables when oracle is used for Ambari backend databseACTUAL BEHAVIOR: Oracle throws the errorEXPECTED BEHAVIOR: There should be a limit to prevent this (possibly modify the syntax of the oracle query)
ns,Similar to Oracle/Postgres we need to make sure that ambari-server upgradestack works for MySQL
ns,There is currently no easy way to find the version of Ambari that is in use from the Ambari Web interface. This info should be accessible via an 'about' link in Ambari Web
ns,See attached. Using 1.5.0.335On Hosts page. Other pages seem fine.
ns,When performing Restart on Slaves via Bulk Ops menu on the Hosts page  Restart should popup the rolling restart dialog  just like it does when Restarting Slaves under Service Actions.
ns,I am using Ambari (1.4.3.38) for hadoop cluster installation and management. All the cluster nodes are built on centos 6.0.During the ambari server installation  ambari-server recognized the primary/cluster os as redhat6 (see ambari.properties). During the ambari agent bootstrap/host register  ambari-agent regonized the agent os as centos linux6 (see log). From log files (ambari-server.log  ambari-agent.log)  I found the inconsistence caused the warning of ambari-agent bootstrapping and failure of host registering.I'm still not sure why this happen  but I guess it's caused by the differene of os checking methods among ambari server side code  ambari-agent bootstrap script (os_type_check.sh based on os release file) and registering script (Controller.py/Register.py based on os hardware profile) .I just share to see if anyone can fix the issue.BTW  for me  to solve the problem  I manually edited the script files to make it work temporarily:To avoid warning of agent bootstrapping  in os_type_check.sh  add current_os=$RH6 above the echo line or add res=0 after case statement;To make the node register work  in Controller.py  add data=data.replace('centos linux' 'redhat') before sending registering request;Thanks.
ns,Oozie Server installation fails when Falcon is selected
ns,Note: We let the user delete Storm Supervisor already.
ns,The alert badge shown in the left nav shows up a bit strange (too little padding on the right). Also  when the restart indicator appears  the padding for the alert badge fixes itself  but the restart indicator appears too close. See attached.
ns,OOS status for component on host detail page makes button too big
ns,Refactor and Unit tests for host summary
ns,BG operation pop-up: JS error encountered on clicking on host in progress state.
ns,See attached.
ns,Need a graph implemented to show the Tez DAG for Hive queries
ns,Add a new category in 'Hosts Check' popup after registering hosts.So for '/' mountpoint  we check if the disk free space is larger than 2.0GB.And for '/usr/lib' or '/usr'  check the free space is larger than 1.0GB. Then show related warning message if any of them got space shortage.
ns,Tracking few issues related to decommission support Even with HBase HA only one master should be used for decommission Alternatively  for HDFS/MR/YARN use all master host components Do not use 'default' method while using python based system resources Add support for command details and custom command names
ns,After ambari-server restart  if FE remains at the Dashboard page  the service status (Yellow buttons) never gets updated even if the API indicates that the status are all Green/Red. In my case  never is 6 minutes.Upon refresh or moving to other tabs the view is promptly updated.
ns,Actions pull down on the Host Detail page likes to hide itself when it's open. This is a bit annoying.
ns,We have removed option -c or --jce-policy. So the warning message should not use that option in the help statement.jce_download_fail_msg = ' Failed to download JCE Policy archive : {0}. ' / 'Please check that JCE Policy archive is available ' / 'at {1} . Also you may install JCE Policy archive manually using ' / '--jce-policy command line argument.'.format('{0}'  jce_url)
ns,adding more master components styling is missing
ns,see attached
ns,Go to host page with some clients installedRefresh itGot: clients are not displayed on the pageExpected: list of installed clients
ns,medkit-icons are shifted out of their places  in hosts table in Safari.
ns,Exception trace:18:27:19 191 WARN [qtp1643608425-22] ServletHandler:514 - /api/v1/clusters/c1/servicesjavax.persistence.RollbackException: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.4.0.v20120608-r11652): org.eclipse.persistence.exceptions.DatabaseExceptionInternal Exception: java.sql.BatchUpdateException: ORA-00942: table or view does not existError Code: 942 at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commitInternal(EntityTransactionImpl.java:102) at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:63) at com.google.inject.persist.jpa.JpaLocalTxnInterceptor.invoke(JpaLocalTxnInterceptor.java:87)
ns,STR:1. Install  setup and start Ambari server by default.2. Deploy Hadoop by default (stack 2.0.6  choose all services to install).3. Enable HA.4. Wait for at least 150 seconds.4. Go to HDFS Service/Hosts page page.Actual result:'Secondary NameNode Process' alert displayed.Expected result:There should not be 'Secondary NameNode' alert on page.
ns,1. Try to enable HA with fail on 9th step for Start All services2. Use link http://&lt;host&gt;:8080/#/main/admin/highAvailability/enable/step13. HA Wizard is shown without previously loaded Login Page (see url and js errors in attached screenshot)Expexted result:Login Page should be loaded firts.
ns,Restart for a service calls stop and start on all components. Currently there is no implementation for stopping a client component. This leads to error message: 'Stop not implemented for component'
ns,Steps to reproduce:1. Launch Add Service wizard2. Close wizard3. Go by link #/main/services/add/step1 (type it in address bar)The router goes to nonexistent page '#/main/services/add/summary'.
ns,Step to reproduce.Go untill to Customize Services Page.Click to Choose Services menu link.Go to Assign Masters page.
ns,Add Supervisor to Hosts page's Bulk Ops menu if Storm is installed.Decommission and Recommission should be disabled  as Supervisors do not support these operations.
ns,Steps to reproduce: Deploy cluster without Ooozie and some other customizable service. Go to the Add Service wizard. Add Oozie service. Fail starting Oozie server. Close add service wizard. Ensure that Oozie was added as service. Go again to Add Service Wizard and choose some customizable service. Go to Customize Services page.Result: Customize Services page proposes to customize Oozie  but it was already added to the cluster during installation.
ns,Write unnitests for HDFS install script on HDP1 and HDP2
ns,The sort order arrows are right-justified within the column.This makes it look like the arrows apply to the column next to it.Instead  we should show the arrows right next to the respective column label.Like:Name (arrows) IP Address (arrows)Not:Name (arrows) IP Address
ns,Oozie tests fail
ns,Updated HDFS configs aren't applied after RESTART.Steps: On a 3 node cluster  create a ConfigGroup for datanode. Override heap size to 1025m instead of 1024m Restart DN.Result: The /var/lib/ambari-agent/data/config.json  has the correct values for the config type:'global': {'2': 'version1392171779044'  'tag': 'version1'} The API call still shows global as default version:http://hostname1:8080/api/v1/clusters/c1/hosts/hostname1/host_components/DATANODE Note:It works after agent is restarted.{global: {overrides: {2: 'version1392171779044'} default: 'version1'}The cause is that hooks aren't executed for the custom_command like RESTART. Since configs for HDFS are generated in hook.py  we must execute hook.py before custom commands or move config generation from hook.py.
s,Add unittets for hooks in secured mode.
ns,Falcon Server can not be restarted
ns,Impl alert for ATS server ATS process (running / not running)Note: this alert should be disabled/removed when ATS gets deleted (when Kerb is enabled).
ns,After adding new ZK server needs to change value of storm.zookeeper.servers property.
ns,ATS has changed structure of http://server:8188:8188/ws/v1/apptimeline/HIVE_QUERY_ID/&lt;id&gt; where the entire query JSON structure is now represented as a string under 'otherinfo/query'. UI will need to deserialize this string back into JSON and continue.
ns,AmbariManagementControllerTest Test fails with unable to delete the last user.Results :Tests in error: testDeleteUsers(org.apache.ambari.server.controller.AmbariManagementControllerTest): Could not remove user user1. System should have at least one user with administrator role. Tests run: 1404  Failures: 0  Errors: 1  Skipped: 7
ns,Earlier History Server was not a different service component and was always co-hosted with JobTracker for HDP-1.x. So we had a same section for Job Tracker and History server in security wizard config page.After AMBARI-2617 and AMBARI-4207 fix  it's possible to have JobTracker and History Server on different host via Ambari web-ui. With this capability it's important to show Job History Server as a different section in Security Wizard MR service config page.
ns,STR: Deploy cluster with DataNodes on 2 from 3 hosts. Change DataNode maximum Java heap size from 1024 to 1025. Check that Restart indicators appeared and 2 DataNodes require restart. Add DataNode on missing host. Change property DataNode maximum Java heap size back to 1024.Result: Those two DataNodes still require restart and added DataNode not.Gluster properties shouldn't be added unless GLUSTERFS is installed.
ns,See attached. During Customize Services.I installed a cluster w/o Storm  and went to add Storm.
ns,The file defined by dfs.domain.socket.path must give +x permission for other user. &lt;property&gt; &lt;name&gt;dfs.domain.socket.path&lt;/name&gt; &lt;value&gt;/var/lib/hadoop-hdfs/dn_socket&lt;/value&gt; &lt;/property&gt;Currently  In ambari installed cluster  /var/lib/hadoop-hdfs does not give +x permission to other user[root@ambari-sec-1392876050-hdfs-re-8 ~]# stat /var/lib/hadoop-hdfs/ File: '/var/lib/hadoop-hdfs/' Size: 4096 Blocks: 8 IO Block: 4096 directoryDevice: 803h/2051d Inode: 1182008 Links: 3Access: (0750/drwxr-x---) Uid: ( 1005/ hdfs) Gid: ( 500/ hadoop)Access: 2014-02-18 18:10:35.000000000 -0800Modify: 2014-02-20 07:50:55.274766162 -0800Change: 2014-02-20 07:50:55.274766162 -0800Due to this Issue  hadoop commands are seeing below WARN messages. 2014-02-18 05:54:32 734|beaver.machine|INFO|RUNNING: /usr/bin/hdfs dfs -tail /user/hrt_qa/hdfsRegressionData/smallFiles/smallRDFile7552014-02-18 05:54:35 528|beaver.machine|INFO|14/02/18 05:54:35 WARN hdfs.BlockReaderLocal: error creating DomainSocket2014-02-18 05:54:35 528|beaver.machine|INFO|java.net.ConnectException: connect(2) error: Permission denied when trying to connect to '/var/lib/hadoop-hdfs/dn_socket'2014-02-18 05:54:35 528|beaver.machine|INFO|at org.apache.hadoop.net.unix.DomainSocket.connect0(Native Method)2014-02-18 05:54:35 529|beaver.machine|INFO|at org.apache.hadoop.net.unix.DomainSocket.connect(DomainSocket.java:250)2014-02-18 05:54:35 529|beaver.machine|INFO|at org.apache.hadoop.hdfs.DomainSocketFactory.createSocket(DomainSocketFactory.java:158)2014-02-18 05:54:35 529|beaver.machine|INFO|at org.apache.hadoop.hdfs.BlockReaderFactory.nextDomainPeer(BlockReaderFactory.java:691)2014-02-18 05:54:35 529|beaver.machine|INFO|at org.apache.hadoop.hdfs.BlockReaderFactory.createShortCircuitReplicaInfo(BlockReaderFactory.java:439)2014-02-18 05:54:35 529|beaver.machine|INFO|at org.apache.hadoop.hdfs.client.ShortCircuitCache.create(ShortCircuitCache.java:669)The expected Permissions on this location is as below.[root@ambari-sec-1392876050-yarn-10 ~]# stat /var/lib/hadoop-hdfs/ File: '/var/lib/hadoop-hdfs/' Size: 4096 Blocks: 8 IO Block: 4096 directoryDevice: 803h/2051d Inode: 1181767 Links: 3Access: (0751/drwxr-x--x) Uid: ( 1005/ hdfs) Gid: ( 500/ hadoop)Access: 2014-02-20 18:00:06.586040913 -0800Modify: 2014-02-20 07:06:28.267889888 -0800Change: 2014-02-20 17:59:56.629052410 -0800
ns,NameNode fails to start due to 'fs.defaultFS' being null
ns,Skip Failing tests for now.
ns,Reconfiguring memory related properties of a service suffixes 'm' to memory related properties of other service.
ns,We originally wanted to couple decom/recom with putting the host component in / out of maintenance mode.After experimenting  we decided to undo that. This is the JIRA for the Ambari BE changes.
ns,1) has too much padding (See attached.)2) the sorting carets should have more padding-left so there is a bit more space between the column label3) the checkboxes should have more left padding. They are not balanced.4) The checkboxes and status icons are not vert centered with the hostname text.5) The input field for searching the hostname column should take up more horizontal space. with all that blank  makes it look like there is a missing column.
ns,Allow Falcon to be configured with keytab/security and custom params
ns,1) As admin  change a config but do not perform the restarts2) Create a test user (non-admin)3) login as test user4) in host details page  operation to restart is shown. Should not be shown
ns,STR: 1. Navigate to hosts page  wait for page loaded. 2. Click specific hosts link. 3. Wait for HostDetails page loaded. 4. Click Back. 5. Iterate. Actual result: sometimes the Hosts page hangs not resulting in presenting a HostDetails page. Speed of attached videos is 5 times faster than real.
ns,STR:1) Deploy cluster by default;2) Go to HA wizard3) Second wizard stepExpected result:1) In 'Additional NameNode' combobox should NOT be ability to choose host where NameNode component already installed.2) In 'JournalNode' comboboxes should NOT be ability to choose more than one JournalNode on one hostActual results:1) In 'Additional NameNode' combobox can be chosen host with already installed NameNode (see first screenshot)2) JournalNode components might be chosen to install for any host  including case when three JournalNode's might be installed on one host (see second screenshot).
ns,Following features must be implemented: /apps/falcon directory on hdfs and owned by falcon user make falcon able to be runned from custom user change alerts text
ns,Ambari-DDL-MySQL-CREATE.sql includes 'create ambarirca' database.CREATE DATABASE ambarirca;USE ambarirca;1) At minimum  this DDL should not be creating an ambarirca database and instead mix in the RCA tables with core Ambari tables (that's how Oracle DDL does it &#8211; I think). Not ideal but better than creating an ambarirca database in this script (a user would be surprised to see this).2) Alternatively  we need to split out RCA DDL from the core Ambari. We can doc that if you plan to use HDP 1.3.x stack that you need to to setup an RCA database (since RCA is only applicable to those with HDP 1.3.x stack). If we go this route  for consistency  we would need to do the same for the oracle DDL.Note: Right now  looks like the oracle DDL just puts the RCA tables in the same database as ambari core tables (basically #1 above).
ns,STR:1. Deploy cluster by default scenario w/o Storm and Falcon.2. Go to HDFS service page.3. Go to Nagios service page.4. Navigate to Nagios Web UI.5. Enter credentials nagiosadmin/passwordActual results: There are no alerts on service page. Web UI is unavailable due to ERROR 403.Screenshots attached.
ns,Log gets filled on install failure.13:28:08 856 WARN [qtp615964260-72] HeartBeatHandler:361 - State machine exceptionorg.apache.ambari.server.state.fsm.InvalidStateTransitionException: Invalid event: HOST_SVCCOMP_OP_SUCCEEDED at INSTALL_FAILED at org.apache.ambari.server.state.fsm.StateMachineFactory.doTransition(StateMachineFactory.java:297) at org.apache.ambari.server.state.fsm.StateMachineFactory.access$300(StateMachineFactory.java:39) at org.apache.ambari.server.state.fsm.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:440) at org.apache.ambari.server.state.svccomphost.ServiceComponentHostImpl.handleEvent(ServiceComponentHostImpl.java:730) at com.google.inject.persist.jpa.JpaLocalTxnInterceptor.invoke(JpaLocalTxnInterceptor.java:66)
ns,# Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License  Version 2.0 (the 'License'); you may not use this file except in compliance with the License. You may obtain a copy of the License at##http://www.apache.org/licenses/LICENSE-2.0# Unless required by applicable law or agreed to in writing  software distributed under the License is distributed on an 'AS IS' BASIS  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND  either express or implied. See the License for the specific language governing permissions and limitations under the License.# http://www.apache.org/licenses/LICENSE-2.0# Unless required by applicable law or agreed to in writing  software distributed under the License is distributed on an 'AS IS' BASIS  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND  either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.#
ns,Service Check does not work
ns,Slave components (such as DataNode  NodeManager  RegionServer  Supervisor  Ganglia Monitor) should include a 'Restart' command in their 'Actions' menu on the host details page.The Restart option should be shown and enabled when the component is started.
ns,STR: Deploy cluster with HDFS+ZK  Nagios  Ganglia. Add YARN+MR2  Tez services.Result: Alerts Ganglia Monitor process for HistoryServer and Ganglia Monitor process for ResourceManager don't dissappear after restarting all services
ns,1) remove 'Hive Metastore status' alert2) add 'Hive Metastore process' alert3) add 'HiveServer2 process' alert
ns,1) Configure NN HA2) stack 2.0.6 (but check stack 2.1 as well)3) Two alerts show' check_tcp: Port must be a positive integer'NameNode process on c6401.ambari.apache.orgNameNode process on c6402.ambari.apache.org4) Looked at /etc/nagios/objects/hadoop-services.cfg5) Saw:define service { host_name c6401.ambari.apache.org use hadoop-service service_description NAMENODE::NameNode process on c6401.ambari.apache.org servicegroups HDFS check_command check_tcp_wrapper!//test!-w 1 -c 1 normal_check_interval 0.5 retry_check_interval 0.25 max_check_attempts 3}define service { host_name c6402.ambari.apache.org use hadoop-service service_description NAMENODE::NameNode process on c6402.ambari.apache.org servicegroups HDFS check_command check_tcp_wrapper!//test!-w 1 -c 1 normal_check_interval 0.5 retry_check_interval 0.25 max_check_attempts 3}Notice in the above //test is the name of my nameservice.Attaching screen shot of my config. So looks like it's grabbing port from the wrong prop.
ns,1. Click on an app on ATS/jobs page which is running 2. It pops up a window with message 'Tez DAG has no ID associated with name hrt_qa_20140228161212_a5713292-8213-43a3-b61e-06685799a5b3'3. Press OK and the page refreshes and pops up the same window again.4. This goes on forever until the user closes the browser or enters a different URLInstead  the page should be redirected back to &lt;ambari server URL&gt;/#/main/jobs
ns,The details of how the notion of a decommissioned DN is stored has changed for 1.5.0. Add support to modify persisted data when Ambari is upgraded to 1.5.0.
ns,Wizard-&gt;Confirm Hosts step:1. Refactor parsing of json response with hosts warnings.2. Reduce latency on opening Host Warnings popup.
ns,3 hosts  third-host has only Ganglia monitor and Datanode.I kill the third machine agent. Hosts page correctly shows heartbeat lost.On the services page  ganglia shows 'yellow'  even though the ganglia server host is fine.Ganglia service should considered started if Ganglia Server is started.
ns,Hive Service Check Failed during Install Wizard
ns,Deployed 2-node cluster. Added 3rd node. Enabled security.After steps above on all 3 hosts tasks jammed and don't want to perform or fail for a very long time.VMs are alive  ambari-server and all ambari-agents are running.Finally got a reproduce using 2 commandscurl 'http://vm-0.vm:8080/api/v1/clusters/cc/services?params/run_smoke_test=false' -X PUT -H 'X-Requested-By: X-Requested-By' -u admin:admin --data '{'RequestInfo': {'context': 'Start All Services'}  'Body': {'ServiceInfo': {'state': 'STARTED'}}}' ; sleep 3; curl 'http://vm-0.vm:8080/api/v1/clusters/cc/hosts/vm-0.vm/host_components/APP_TIMELINE_SERVER' -X DELETE -H 'X-Requested-By: X-Requested-By' -u admin:adminThe way to reproduce is a bit different compared to an original description (I issue a DELETE request in 3 seconds after START_ALL_SERVICES request has been issued)  but the symptoms are the same: ServiceComponentHostNotFoundException exception is posted to log and operation is stuck on stage that contains 'App Timeline Server Start' command.
ns,Steps to reproduce:On the Ambari Server host  open /etc/ambari-server/conf/ambari.properties with a text editor.Add the following property:security.server.two_way_ssl = trueError messageINFO 2014-03-07 13:57:17 184 security.py:184 - Agent certificate not exists  sending sign requestINFO 2014-03-07 13:57:17 335 security.py:89 - SSL Connect being called.. connecting to the serverERROR 2014-03-07 13:57:17 414 security.py:76 - Two-way SSL authentication failed. Ensure that server and agent certificates were signed by the same CA and restart the agent. In order to receive a new agent certificate  remove existing certificate file from keys directory. As a workaround you can turn off two-way SSL authentication in server configuration(ambari.properties) Exiting..
ns,oozie-site.xml needs the following two services added to the list of services:org.apache.oozie.service.XLogStreamingService (needed to display logs)org.apache.oozie.service.JobsConcurrencyService (needed to run Recovery Service - for example  this would handle jobs that are dangling and stuck in RUNNING state)
ns,I clicked on Start All services button and nothing happened. Turns out that on the API call  the server throws a 500 exception that is silently lost. We should show in a dialog the error response from server. Similarly for Stop All action.PUT http://c6401:8080/api/v1/clusters/c1/services?params/run_smoke_test{'RequestInfo': {'context' :'_PARSE_.START.ALL_SERVICES'}  'Body': {'ServiceInfo': {'state': 'START{ 'status' : 500  'message' : 'org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: Invalid transition for servicecomponenthost  clusterName=c1  clusterId=3  serviceName=OOZIE  componentName=OOZIE_SERVER  hostname=c6402.ambari.apache.org  currentState=INSTALL_FAILED  newDesiredState=STARTED'}
ns,1) Following jaas.conf file needs to be on all storm component hosts:Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab='$keytab' storeKey=true useTicketCache=false serviceName='zookeeper' principal='$principal';};2) In YAML  following java configurations should have jaas.conf options:nimbus.childopts: '-Djava.security.auth.login.config=/path/to/jaas.conf'ui.childopts: '-Djava.security.auth.login.config=/path/to/jaas.conf'supervisor.childopts: '-Djava.security.auth.login.config=/path/to/jaas.conf'
ns,Steps to reproduce: Make Install all services request fail. Hit on Retry button. Make retry attempt to install all services fail.This will trigger start all services call and error pop-up will be displayed.Expected behavior: Start all services call should not be called.
ns,For host components whose service is in maintenance mode  the maintenance mode icon should be displayed to the right of the service name and we should display the host component health (green / red  etc) on the far left.When the host itself is in maintenance mode  we should not show any maintenance mode icon on the host component (unless the service is in maintenance mode). This allows the UI to distinguish which host components are in service-derived maintenance mode vs host-derived. Also  on this page  host-level operations apply to all but the host components in service-derived maintenance mode (regardless of the host maintenance mode)  so this display is more natural and easier to understand for the end user.
ns,Reproduced with such preconditions:On Customize Services page specify 'YARN Log Dir Prefix' to some custom dir. After deploying  run MapReduce2 Service check and check that:hadoop-mapreduce.jobsummary.log is empty  but /var/log/hadoop-yarn/yarn/hadoop-mapreduce.jobsummary.log contains jobs records.
ns,STR: Deploy cluster. Check that flag Do not show the Background Operations dialog when starting an operation is set to false. Click Restart DataNodes in Actions menu of HDFS. Click 'Trigger Restart' in appeared modal window.Result: Background Operations was not appeared.
ns,The fix for this issue would be to display that the slaves whose host is in 'host' maintenance mode will be skipped.For example  we have 3 hosts (host1  host2  and host3) with NodeManager installed on each. Say host3 is in maintenance mode.Rolling Restart dialog should say '1 NodeManager in maintenance mode will not be restarted'.
ns,After upgrading Ambari from 1.2.5 to 1.5.0 and Stack from 1.3.2 to 2.0.10When starting HDFS service  Datanode and SNameNode on the agent host failed to start due to JAVA_HOME=cbin/java is missing. In ambari.properties  java.home=/usr/jdk64/jdk1.6.0_3 after upgrade.
s,In a cluster node we had set the below in /etc/hive/conf/hive-site.xml&lt;property&gt; &lt;name&gt;hive.jar.directory&lt;/name&gt; &lt;value&gt;hdfs:///apps/hive/install&lt;/value&gt;&lt;/property&gt;The HDFS folder has the following contents# hadoop fs -ls -R /apps/hive/drwxr-xr-x - hive hdfs 0 2014-03-19 17:10 /apps/hive/install-rwxr-xr-x 3 hive hdfs 14719276 2014-03-19 17:10 /apps/hive/install/hive-exec.jardrwxrwxrwx - hive hdfs 0 2014-03-19 17:08 /apps/hive/warehouseAs user ambari-qa I run the hive command to hit this exception$ hiveLogging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.propertiesException in thread 'main' java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=ambari-qa  access=WRITE  inode='/apps/hive/install':hive:hdfs:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:265) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:251) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:232) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:176) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5481) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5463) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:5437) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2265) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2218) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2003) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1999) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1997) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:345) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:682) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:626) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212)Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=ambari-qa  access=WRITE  inode='/apps/hive/install':hive:hdfs:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:265) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:251) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:232) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:176) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5481) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5463) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:5437) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2265) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2218) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2003) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1999) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1997) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106) at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73) at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1602) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1461) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1386) at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394) at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390) at org.apache.had
ns,When stopping ATS  YARN service indicator blinks red giving the idea that YARN is going down and become solid red after that. YARN service should not be indicated as STOPPED if ATS is down.
ns,This line at the bottom of hive-env.sh disregards any user-provided values  masking them from the launch script.# Folder containing extra ibraries required for hive compilation/execution can be controlled by:export HIVE_AUX_JARS_PATH=/usr/lib/hcatalog/share/hcatalog/hcatalog-core.jarThis breaks this feature for users of both the environment export HIVE_AUX_JARS_PATH='my custom value' and the command line hive --auxpath 'my custom value' .
ns,The reason of bug is wrong query formation while triggering API to create clients on host.The data sent with the POST call to create client components on the host is:{'RequestInfo':{'context':'Install Clients'} 'Body':{'host_components':[{'HostRoles':{'component_name':'CLIENTS'}}]}}This is incorrect. There is no component with name 'CLIENTS'.
ns,Add Nagios  Ganglia via Add Service WizardProceed to step 'Customize Services'JS-error appears about null-object:/app/controllers/wizard/step7_controller.js : loadServiceTagsSuccess()if (serviceConfigsDef.sites.indexOf(site) &gt; -1)Also  HDFS by default is selected as active tab  but one of the 'new' services (Nagios for example) should be selected.
ns,We should show displayName  not componentName
ns,STR: during installer phase go to &lt;cluster&gt;:8080 /#/installer/step1 ; Change/delete any of BaseURL of repos ; Click 'Undo' buttonActual Results:Undo button does not work. Moreover  if we click Undo after any update of text in BaseURL  it leads to cleanup of it value atall.
ns,STR:1) Deploy cluster by default2) Go to Ganglia service page -&gt; Config tab 3) Change value 'Ganglia rrdcached base directory' (e.g. /var/lib/ganglia/rrd8)4) Restart Ganglia5) Change 'Ganglia rrdcached base directory' back to old value (by default - /var/lib/ganglia/rrds)6) Restart GangliaExpected result: Ganglia should be restarted successfullyCurrent result: Ganglia Server goes to 'installed' state and stuck thereError message from gmetad file:=============================Starting hdp-gmetad...=============================Base directory (-b) resolved via file system links!Please consult rrdcached '-b' documentation!Consider specifying the real directory (/var/lib/ganglia/rrd8)chgrp: cannot access '/var/run/ganglia/hdp/rrdcached.sock': No such file or directorychgrp: cannot access '/var/run/ganglia/hdp/rrdcached.limited.sock': No such file or directoryFailed to start /usr/bin/rrdcachedNot starting /usr/sbin/gmetad because starting /usr/bin/rrdcached failed.root 16990 0.0 0.0 108164 1560 ? S 03:19 0:00 /bin/bash --login -c service hdp-gmetad start &gt;&gt; /tmp/gmetad.log 2&gt;&amp;1 ; /bin/ps auwx | /bin/grep [g]metad &gt;&gt; /tmp/gmetad.log 2&gt;&amp;1
ns,JMXetricAgent instrumented JVM  see https://github.com/ganglia/jmxetricMar 28  2014 6:40:13 PM info.ganglia.jmxetric.JMXetricAgent premainSEVERE: Exception starting JMXetricAgentjava.net.UnknownHostException: {0}: Name or service not known at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method) at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901) at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1293) at java.net.InetAddress.getAllByName0(InetAddress.java:1246) at java.net.InetAddress.getAllByName(InetAddress.java:1162) at java.net.InetAddress.getAllByName(InetAddress.java:1098) at java.net.InetAddress.getByName(InetAddress.java:1048) at info.ganglia.gmetric4j.gmetric.AbstractProtocol.&lt;init&gt;(AbstractProtocol.java:29) at info.ganglia.gmetric4j.gmetric.Protocolv31x.&lt;init&gt;(Protocolv31x.java:34) at info.ganglia.gmetric4j.gmetric.GMetric.&lt;init&gt;(GMetric.java:108) at info.ganglia.jmxetric.XMLConfigurationService.configureGangliaFromXML(XMLConfigurationService.java:165) at info.ganglia.jmxetric.XMLConfigurationService.configure(XMLConfigurationService.java:67) at info.ganglia.jmxetric.JMXetricAgent.premain(JMXetricAgent.java:51) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at sun.instrument.InstrumentationImpl.loadClassAndStartAgent(InstrumentationImpl.java:382) at sun.instrument.InstrumentationImpl.loadClassAndCallPremain(InstrumentationImpl.java:397)The reason here is this config send from ui:...childopts: '-javaagent:/usr/lib/storm/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host={0} ...'it can't find hostname {0}  however this works fine:...childopts: '-javaagent:/usr/lib/storm/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host= port...'So let's leave it as empty if no ganglia server is present
ns,Ambari exposes the supervisor.enable property and in some early builds defaults it to true which causes the supervisor's to not launch workers assigned to them. The supervisor's will start up  the assignments are there  but the supervisor's just ignore them. We need to remove the supervisor.enable from Ambari as it seems like a very dangerous switch that doesn't have broad applicability.
ns,Steps to reproduce:1. Run hosts registration2. Switch to Registering category3. Wait till all hosts become registeredResult: Table is collapsed.JS error occured: Uncaught Error: assertion failed: calling set on destroyed object
ns,With the new server-client version match check introduced  it is a bit annoying when building ambari-web locally and testing on the server for e2e testing  as the server and client versions must match exactly or you cannot proceed. We'll disable the check if App.version == ''.
ns,Looks like Falcon service definition on the stack has changed by AMBARI-5278.Currently  any new definition of globals require duplicating them in the UI code but that was not done  so this is breaking Falcon installation.
ns,Icon 'Asterisk' on 'Assign Masters'/-Slaves steps does not display with 8-bit depth
ns,Print better logs for openssl issues on centos/rhel 6.5.
ns,When a Hive Tez DAG operator has a plan with very wide content  the text is not wrapped and hence bleeds out of the hover box. An additional problem is that this causes the hover to lose focus and hence close - thereby user cannot even see the hover (opens &amp; closes almost instantly).
ns,If you change RM port from 8088 to 50030 for migration of 1.x to 2.x   JVM metrics and few others are missing which results in yarn service summary page having multiple fields with n/a value.Step1: Change property below yarn.resourcemanager.webapp.address = localhost:50030Step 2: Start the service Click on Dashboard JMX is issue  Quick links is issue. RM gets started successfully.
ns,Steps: Install Ambari-1.3.2 with Oracle DB. Upgrade ambari to 1.5.0 Run 'ambari-server upgrade' command.Seems like check before execute doesn't work for Oracle. Since we ignore failures this is not block the upgrade.Exception:16:41:36 719 WARN [main] DBAccessorImpl:416 - Error executing query: ALTER TABLE clusterconfigmapping ADD CONSTRAINT FK_clustercfgmap_cluster_id FOREIGN KEY (cluster_id) REFERENCES clusters (cluster_id)java.sql.SQLSyntaxErrorException: ORA-02275: such a referential constraint already exists in the table at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:445) at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396) at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:879) at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:450) at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:192) at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:531) at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193) at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:1033) at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1329) at oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1909) at oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1871) at oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:318) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:413) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:399) at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:262) at org.apache.ambari.server.upgrade.UpgradeCatalog150.executeDDLUpdates(UpgradeCatalog150.java:375) at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:177) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:174) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:234)16:41:36 720 WARN [main] DBAccessorImpl:264 - Add FK constraint failed  constraintName = FK_clustercfgmap_cluster_id  tableName = clusterconfigmapping  errorCode = 2275  message = ORA-02275: such a referential constraint already exists in the table
ns,testNoNagiosServerCompoonent(org.apache.ambari.server.controller.nagios.NagiosPropertyProviderTest): Expected no alertstestNoNagiosService(org.apache.ambari.server.controller.nagios.NagiosPropertyProviderTest): Expected no alerts
ns,SSH bootstrap of agents failed on CentOS 6.5.==========================Copying OS type check script...==========================Could not create directory '/root/.ssh'.Warning: Permanently added 'c6502.ambari.apache.org 192.168.65.102' (RSA) to the list of known hosts.scp /usr/lib/python2.6/site-packages/ambari_server/os_check_type.pyhost=c6502.ambari.apache.org  exitcode=0==========================Running OS type check...==========================Traceback (most recent call last): File '/tmp/os_check_type1396641340.py'  line 22  in &lt;module&gt; from common_functions import OSCheckImportError: No module named common_functionsConnection to c6502.ambari.apache.org closed.SSH command execution finishedhost=c6502.ambari.apache.org  exitcode=1ERROR: Bootstrap of host c6502.ambari.apache.org fails because previous action finished with non-zero exit code (1)ERROR MESSAGE: tcgetattr: Invalid argumentConnection to c6502.ambari.apache.org closed.STDOUT: Traceback (most recent call last): File '/tmp/os_check_type1396641340.py'  line 22  in &lt;module&gt; from common_functions import OSCheckImportError: No module named common_functionsConnection to c6502.ambari.apache.org closed.
ns,For non-admin users action 'Switch to classic dashboard' returns JS error and 'Reset all widgets to default' action throws to Login page.
ns,STR:on Installer phase go to &lt;cluster&gt;:8080 /#/installer/step1Do not do any updates of 'Base Url' fieldcheck the 'Undo' button for 2.1 stack.Actual Result:Undo is present for all 3 Os reposExpected Result:Undo should be present only after any update of 'BaseURL' field----------Seems  functionally this does not affect us  but might be confusing for users.
ns,While upgrade  schema upgrade fails.Example error output (Oracle):rg.apache.ambari.server.AmbariException: Current database store version is not compatible with current server version  serverVersion=1.5.1.96  schemaVersion=1.4.1.25 at org.apache.ambari.server.controller.AmbariServer.checkDBVersion(AmbariServer.java:479) at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:149) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:528)and Postgres 11:39:27 364 ERROR [main] AmbariServer:531 - Failed to run the Ambari Serverorg.apache.ambari.server.AmbariException: Current database store version is not compatible with current server version  serverVersion=1.5.1.96  schemaVersion=1.5.0 at org.apache.ambari.server.controller.AmbariServer.checkDBVersion(AmbariServer.java:479) at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:149) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:528)
ns,After the user clicks 'Deploy' in the Installer Wizard  sometimes an error is shown showing 'java.lang.IllegalArgumentException: Could not access base url'. This happens while the wizard tries to save the user-specified repo base URLs.Installer should not be performing validation during deploy  because it had already been done during Select Stacks.
ns,remove classic dashboard from Ambari  as it's not used any more and doesn't support new services
ns,Typo in unit test. File said class was Hcat rather than Pig
ns,STR:1. Deploy Hadoop by default scenario with all services.2. Try to start Storm.Actual results: 'Start All Services' operation failed because of 'Supervisor start' failed. 'Start Storm' operation does not contain 'Supervisor start' popups  but all supervisors are STARTED after it have finished.
ns,I tried default centos 6.5 it works fine. But if I change /etc/issues and /etc/redhat-release to say:CentOS Linux release 6.5 (Final)then the registration fails with:INFO 2014-04-09 14:39:22 501 security.py:51 - SSL connection established. Two-way SSL authentication is turned off on the server.ERROR 2014-04-09 14:39:22 563 Controller.py:100 - Cannot register host with not supported os type  hostname=c6501.ambari.apache.org  serverOsType=redhat6  agentOstype=centos linux6In the agent logs.By default centos 6.5 has:CentOS release 6.5 (Final)but sometimes can have:CentOS Linux release 6.5 (Final)
ns,This is due to upgrade. We need to change it manually after upgrade://Now we have (redhat  suse  debian  other_detected_by_python)vi /etc/ambari-server/conf/ambari.properties(server.os_type=sles11) --&gt;( server.os_type=suse11)+restart the serverShould upgrade automatically deal with this?old code: os_info = platform.linux_distribution( None  None  None  ['SuSE'  'redhat' ]  0 ) os_name = os_info[0].lower() if os_name == 'suse': os_name = 'sles' os_version = os_info[1].split('.'  1)[0] master_os_type = os_name + os_version write_property(OS_TYPE_PROPERTY  master_os_type)
ns,Need to fix this on API side
ns,For hive:hive.auto.convert.join.noconditionaltask.size is set to 1000000000 This should be a fraction (1/3) of the container size.hive.tez.java.opts has '-Xmx1024m'This is different from both map and reduce sizes. Desired values are: map size if map size &gt; 2g else reduce sizemap size is set on the same cluster to ~500mbThe settings as the are will lead to many failed queries because the mapjoin conversion is to aggressive. If we don't change the container sizes based on cluster configs we will see wide spread problems with containers being killed or perf problems.
ns,Ambari Agent  as part of host checks  identifies conflicting 'alternatives' settings and reports back inside the 'last_agent_env' object.UI is not surfacing this in Host Checks popup.We should have a section called 'Alternatives Issues' and list out the alternatives names.For example: 'last_agent_env' : { 'stackFoldersAndFiles' : [ ... ]  'alternatives' : [ { 'name' : 'zookeeper-conf'  'target' : '/etc/zookeeper/conf.dist' }  { 'name' : 'hadoop-conf'  'target' : '/etc/hadoop/conf.dist' }  ]  'existingUsers' : [ .... ]  'existingRepos' : [ ... ]  ...In the above case  we want to highlight the fact that hadoop-conf and zookeeper-conf have conflicts.Alternatives Issues--------The following alternatives should be removedAlternativeshadoop-conf Exists on 3 hostszookeeper-conf Exists on 3 hosts
ns,1) Remove HDP-UTILS from Ambari .repo  and move into the HDP Stack Definition (Ambari does not depend on HDP-UTILS so this causes confusion  makes it harder to doc local repo setup  and is unclear on failures if not setup correctly).2) Requires support for multiple repositories in a Stack Definition  and ability to specify multiple repositories from UI.
ns,When Ambari create the metastore database in MySQL it uses auto create feature. This does not create the transaction tables  so any ACID operations (including streaming ingest) will not work.
ns,STR:1. Deploy cluster with multiplied NN directory(in our case /grid/0/hadoop/hdfs/namenode  /grid/1/hadoop/hdfs/namenode).2. Start NN moving.3. Reach step 'Manual commands'.Actual results:Paragraph 1 contains information about all dirs we should move.Paragraph 2 contains message 'Login to the target host XXX and change permissons for the NameNode dirs by running:' and only one command with hardcoded NN dir:chown -R hdfs:hadoop /hadoop/hdfs/namenode/Screenshot attached.When there are multiple directories specified  we need to show the actual path (but replace commas with a space).In case of '/grid/0/hadoop/hdfs/namenode /grid/1/hadoop/hdfs/namenode' as in the attached image  we should display:chown -R hdfs:hadoop /grid/0/hadoop/hdfs/namenode /grid/1/hadoop/hdfs/namenode
ns,Because of JS error (page refresh makes no effect).
ns,Create unit tests for following files:utils/misc.jsutils/number_utils.jsutils/string_utils.jsutils/validator.js
s,For Ambari 1.5.1 SQL standard authorization was on by default.Users with certification suites are running into problems related to this feature (which were not bugs in the auth systems  rather they were additional requirements to using it). This needs to be turned off by default. The feature is controlled by:hive.security.authorization.enabledWe want the default value to be set to false in Ambari 1.6.0.
ns,Unit tests for steps 6 (with small refactor)
ns,Problem:Call: /api/v1/clusters/cl1/requests?to=end&amp;page_size=20Actually result: return most recent 10 requestsExpected result: return most recent 20 requests.Call: /api/v1/clusters/cl1/requests?from=start&amp;page_size=3Actually result: return first 3 requests started from the first in most recent 10Expected result: return first 3 requests started from the very first('install services')In this case  we can never get history requests made before most recent 10.
ns,Checkbox 'client' without upper case letter
ns,NameNode HA wizard: Review page appears blank
ns,STR: Go to Dashboard page. Switch to 'Heatmaps' tab. Click on first host. On host page click 'Back'.Actual result: Appeared 'Cluster Status and Metrics' tab.Expected result: Should appear 'Heatmaps' tab.
ns,Fix UI Unit tests
ns,'Restart' option should not be enabled if a slave component is in 'decommissioned' state  but it is.STR:1) Go to 'Host details' page2) Make any slave component 'Decommissioned'Actual result: 'Restart' option is enabled (see screenshot)Expected result: 'Restart' option should be disabled.
ns,STR:Turn On Maintenance Mode for HDFSTurn Off Maintenance Mode for HDFSExpected result:Have not problems with ambari-agent.Actual result:Have problems with ambari-agent.
ns,During installer user does not see Ambari Version from UI (see screenshot)  but on monitoring phase it's available in the same Admin -&gt; About
ns,PROBLEM: Default action for dialog boxes in AmbariUSE CASE: Ambari UI doesn't allow you to press enter and trigger default actions. When a default action is high lighted 'green button' you should be able to hit enter and have the action be triggered. In this case add hit enter should make okay button trigger. Also  pressing escape should cancel the dialog box.
ns,Yarn Nodemanager Metrics take far too long between updates.To demonstrate:Run Terasort or anything that runs mapreduce:hdfs dfs -mkdir -p benchmarks/terasorthadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen -Dmapred.map.tasks=72 -Dmapred.reduce.tasks=36 1000000 benchmarks/terasort/inputhadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort -Dmapred.map.tasks=72 -Dmapred.reduce.tasks=36 benchmarks/terasort/input benchmarks/terasort/outputhdfs dfs -rm -R -skipTrash benchmarks/terasortThen repeatedly probe the API at:https://&lt;server&gt;:8081/api/v1/clusters/c1/services/YARN/components/NODEMANAGER?fields=host_components/metrics/yarnIt usually takes 2-3 minutes to see the metrics update  very repeatable.
ns,Create unit tests for following files: utils/object.js utils/date_utils.js utils/ui_effects_utils.js utils/updater.js
ns,Postgres issue:new restart_required relies on eclipselink default type converters  we avoided this in pastorg.postgresql.util.PSQLException: ERROR: column 'restart_required' is of type boolean but expression is of type integer Hint: You will need to rewrite or cast the expression. Position: 57 at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2161) at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1890) at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:403) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeUpdate(AbstractJdbc2Statement.java:331) at org.apache.ambari.server.orm.DBAccessorImpl.updateTable(DBAccessorImpl.java:447) at org.apache.ambari.server.orm.DBAccessorImpl.addColumn(DBAccessorImpl.java:371) at org.apache.ambari.server.upgrade.UpgradeCatalog160.executeDDLUpdates(UpgradeCatalog160.java:72) at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:177) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:176) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:225)MySQL issue:Inreresting MySQL feature  there should be no space between function name and parenthesis18:57:00 629 WARN [main] DBAccessorImpl:469 - Error executing query: insert into request(request_id  cluster_id  request_context  start_time  end_time  create_time) select distinct s.request_id  s.cluster_id  s.request_context  coalesce (cmd.start_time  -1)  coalesce (cmd.end_time  -1)  -1 from (select distinct request_id  cluster_id  request_context from stage ) s left join (select request_id  min(start_time) as start_time  max(end_time) as end_time from host_role_command group by request_id) cmd on s.request_id=cmd.request_idcom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: FUNCTION ambari.coalesce does not exist at sun.reflect.GeneratedConstructorAccessor14.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at com.mysql.jdbc.Util.handleNewInstance(Util.java:411) at com.mysql.jdbc.Util.getInstance(Util.java:386) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1054) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4237) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4169) at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2617) at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2778) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2828) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2777) at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:949) at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:795) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:466) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:452) at org.apache.ambari.server.upgrade.UpgradeCatalog150.executeDDLUpdates(UpgradeCatalog150.java:325) at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:177) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:176) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:225)
ns,Ambari installations of hive currently do not set any datanucleus related properties. There is such a thing as a datanucleus l2 cache  that is pretty bad for hive in a distributed environment if it is set. (If there is a lone embedded hive instance  with no other codepaths to the db  then it's fine  but that never happens in a distributed environment.)By default  if no setting is present  datanucleus defaults the l2 cache to being on  so hive ups the ante by defaulting to turning it off by default if no other setting is configured.Now  in a war of 'defaults'  the hive default should win  but this is an area where we have had recurring support issues from clients that turn it on expecting improved performance. Thus  I'd like ambari installed hive-site.xml to explicitly have this config parameter turned off  with a comment asking users to not switch it on as it impacts hive negatively.The parameter in question is 'datanucleus.cache.level2.type'   and it's value should be 'none'. (Note that I've seen some older configs that seem to do things like turning datanucleus.cache.level2 = false and stuff like that  that is bogus config and does nothing and should not be assumed to be a catch-all enabler.)As a comment  I'd like the following comment 'Disables datanucleus l2 cache. This must be set to 'none' for hive to work properly' or something to that effect.
ns,On Security wizard Start Services command ZOOKEEPER_SERVER Start task scheduled on ambari-server host remained in QUEUED status for about 30 mins and other non-completed commands were in PENDING status. For this time interval no command was in IN_PROGRESS status.Later executing stop all services command on the same cluster also made ZOOKEEPER_SERVER Stop task scheduled on ambari-server host to remain in QUEUED status for around 20 mins.
ns,Prior to upgrade  launch Add Services wizard using Ambari 1.5.0 (crucial step) Upgrade stack to a stack with new services and Ambari to 1.5.1 Add Services button is disabled  even though there are services that have not been added to the cluster
ns,I had a couple of finished jobs showing the jobs page. Then I clicked on one job and went back to the jobs page. For a moment both jobs were not shown as links - though they are clickable. This is basically an appearance issue where the job doesnt look like a link for a moment.
ns,When clicking on the Views icon in the top nav  the page content turns blank.We will show a view index page in a later release  but for now  let's just disable clicking on that icon.
ns,Enabled tez engine for hive;Executed select query;If while job is running I stop it by double pressign CTRL-C in hive shell then job will not have end time shown in jobs table and no failed icon (red X)  but in job details it will have status 'killed' and correct end time.If job is killed with yarn application -kill it will have correct end time and failed icon displayed in jobs table  but in job details it will not have neither status nor end time.
ns,Build 1.6.0-151Missing translation: installer.step3.hostWarningsPopup.alternatives.emptyinstaller.step3.hostWarningsPopup.alternatives.empty
ns,1. Stopped all host components on a host ( to be deleted) with ZookeeperServer. 2. Deleted a host from cluster. 3. Ran Zookeeper Service Check.Actual result: failed. Expected: ok.
ns,Update unit tests for utils/config.js.
ns,In build 1.6.0-159  the views call is being made but it's not listing the views in the dropdown.This was due to the API change and Ambari Web hasn't adjusted yet to this introduction of /versions/.
s,There is a possibility that 'Start all Services' command will fail on Disable Security wizard if App.Service DS model was not populated on the load of 'Disable security page' (timing issue). We need to make sure that the Service model has populated before the 'Disable Security page' is rendered.
ns,pig.properties should set pig.location.check.strict to false
ns,When deploying a brand new cluster  all services fail to install due to a parsing exception thrown from the Ambari Agents.File '/usr/lib/python2.6/site-packages/ambari_agent/CustomServiceOrchestrator.py'  line 113  in runCommandjson_path = self.dump_command_to_json(command)File '/usr/lib/python2.6/site-packages/ambari_agent/CustomServiceOrchestrator.py'  line 209  in dump_command_to_jsoncommand'clusterHostInfo' = manifestGenerator.decompressClusterHostInfo(command'clusterHostInfo')File '/usr/lib/python2.6/site-packages/ambari_agent/manifestGenerator.py'  line 116  in decompressClusterHostInfoindexes = convertRangeToList(v)File '/usr/lib/python2.6/site-packages/ambari_agent/manifestGenerator.py'  line 57  in convertRangeToListraise AgentException.AgentException('Broken data in given range  expected - ''m-n'' or ''m''  got : ' + str(r))AgentException: 'Broken data in given range  expected - m-n or m  got : -1he command being sent is{hs_host=[2]  namenode_host=[1]  snamenode_host=[2]  zookeeper_hosts=[0-2]  ganglia_server_host=[1]  nm_hosts=[0]  ganglia_monitor_hosts=[0-2]  all_hosts=[c6403.ambari.apache.org  c6401.ambari.apache.org  c6402.ambari.apache.org]  rm_host=[2]  app_timeline_server_hosts=[2]  slave_hosts=[0]  ambari_server_host=[-1]  nagios_server_host=[1]  all_ping_ports=[8670:0-2]}Notice the ambari-server-host which was added in that commit; it value is 1which would not parse correctly in manifestGenerator.pyI suspect Git e667dc7c9870864ff537374c819b7c1d1dd88e98 caused this problem.Steps to reproduce:1) Provision 3 c64 hosts2) Wipe your server database and re-create it with the embedded PSQL script3) Attempt to provision a cluster with various services.All services will fail to deploy b/c of the above exception. This was working without issues before the above suspect commit.
ns,This happens because HDFS and YARN/MapReduce requires to be restarted for Oozie smoke test to pass successfullyAs a fix to this issue: Don't run smoke test on 'Install  Start and Test' page of the add service wizard. Review page should ask user to restart all stale services.
ns,There is a space between 'banned.user' and '=' which make configuration here is ignored by container-executor  so some default banned users works to include hdfs. Space should be deleted between 'banned.user' and '='yarn.nodemanager.local-dirs=/grid/0/hadoop/yarn/local /grid/1/hadoop/yarn/localyarn.nodemanager.log-dirs=/grid/0/hadoop/yarn/log /grid/1/hadoop/yarn/logyarn.nodemanager.linux-container-executor.group=hadoopbanned.users = hdfs yarn mapred binmin.user.id=1000It should be banned.users=hdfs yarn mapred bin
ns,Fail: Execution of 'mkdir -p /tmp/HDP-artifacts/ ; curl --noproxy hadoop -kf --retry 10 http://hadoop:8080/resources//jdk-7u45-linux-x64.tar.gz -o /tmp/HDP-artifacts//jdk-7u45-linux-x64.tar.gz' returned 2. curl: option --noproxy: is unknownversion of curl that is available at Centos 5 and SLES 11 SP1 seems to have no support for '--noproxy' option.But such workaround works:no_proxy=i.ua curl http://www.i.ua -iI'm going to replace all '--noproxy' invocations with usage of $no_proxy env variable.
ns,A few items that need to be rectified for the File view submission:1. Modify code to abide by Ambari Coding Standards2. JavaDoc Interfaces and Methods.https://cwiki.apache.org/confluence/display/AMBARI/Coding+Guidelines+for+Ambari.
ns,Following upgrade documentation at http://docs.hortonworks.com/HDPDocuments/Ambari-1.5.1.0/bk_upgrading_Ambari/content/ambari-chap7_2x.html On executing ambari-server upgrade  PSQLException is logged inambari-server.log:21:36:20 498 INFO [main] SchemaUpgradeHelper:211 - Upgrading schema to target version = 1.6.021:36:20 528 INFO [main] SchemaUpgradeHelper:220 - Upgrading schema from source version = 1.5.1.11021:36:20 530 INFO [main] SchemaUpgradeHelper:141 - Upgrade path: [{ org.apache.ambari.server.upgrade.UpgradeCatalog160$$EnhancerByGuice$$ff8a4f66: sourceVersion = null  targetVersion = 1.6.0 }]21:36:20 530 INFO [main] SchemaUpgradeHelper:171 - Executing DDL upgrade...21:36:20 533 INFO [main] DBAccessorImpl:463 - Executing query: ALTER SCHEMA ambari OWNER TO 'ambari';21:36:20 534 INFO [main] DBAccessorImpl:463 - Executing query: ALTER ROLE 'ambari' SET search_path to 'ambari';21:36:20 598 INFO [main] DBAccessorImpl:463 - Executing query: CREATE TABLE hostgroup_configuration (blueprint_name VARCHAR(255) NOT NULL  hostgroup_name VARCHAR(255) NOT NULL  type_name VARCHAR(255) NOT NULL  config_data BYTEA NOT NULL  PRIMARY KEY (blueprint_name  hostgroup_name  type_name))21:36:20 962 INFO [main] DBAccessorImpl:463 - Executing query: CREATE TABLE viewentity (id BIGINT NOT NULL  view_name VARCHAR(255) NOT NULL  view_instance_name VARCHAR(255) NOT NULL  class_name VARCHAR(255) NOT NULL  id_property VARCHAR(255)  PRIMARY KEY (id))21:36:21 010 INFO [main] DBAccessorImpl:463 - Executing query: ALTER TABLE hostcomponentdesiredstate ADD restart_required BOOLEAN21:36:21 078 INFO [main] DBAccessorImpl:463 - Executing query: ALTER TABLE hostgroup_configuration ADD CONSTRAINT FK_hg_config_blueprint_name FOREIGN KEY (blueprint_name) REFERENCES hostgroup (blueprint_name)21:36:21 084 WARN [main] DBAccessorImpl:469 - Error executing query: ALTER TABLE hostgroup_configuration ADD CONSTRAINT FK_hg_config_blueprint_name FOREIGN KEY (blueprint_name) REFERENCES hostgroup (blueprint_name)org.postgresql.util.PSQLException: ERROR: there is no unique constraint matching given keys for referenced table 'hostgroup' at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2161) at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1890) at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:403) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:395) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:466) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:452) at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:337) at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:321) at org.apache.ambari.server.upgrade.UpgradeCatalog160.executeDDLUpdates(UpgradeCatalog160.java:85) at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:250) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:176) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:225)21:36:21 089 WARN [main] DBAccessorImpl:339 - Add FK constraint failed  constraintName = FK_hg_config_blueprint_name  tableName = hostgroup_configurationorg.postgresql.util.PSQLException: ERROR: there is no unique constraint matching given keys for referenced table 'hostgroup' at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2161) at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1890) at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:403) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:395) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:466) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:452) at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:337) at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:321) at org.apache.ambari.server.upgrade.UpgradeCatalog160.executeDDLUpdates(UpgradeCatalog160.java:85) at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:250) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:176) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:225)
ns,nimbus.childopts  ui.childopts and supervisor.childopts points to sasl configuration files after the security is disabled. web-ui should remove -Djava.security.auth.login.config parameter from these properties while disabling security.
ns,ActionScheduler is stucked when adding tasks to ActionQueue on large clusters (&gt;1000 nodes)
ns,During Ambari upgrade  automatically clear the persist state to prevent potential issues with Ambari Web not working properly (this was observed a number of times on upgraded clusters). Currently  we make the following call to get out of the inconsistent state so that Ambari Web works properly:curl -i -u admin:admin -H 'X-Requested-By: ambari' -X POST -d '{ 'CLUSTER_CURRENT_STATUS': '{/'clusterState/':/'CLUSTER_STARTED_5/'}' }' http://localhost:8080/api/v1/persistWe need to do something equivalent during upgrade.
s,After https is enable in Ambari server  Recommission a DN will fails with the following error message found in the Ambari-server log:WARN &#91;qtp1103265648-610&#93; nio:651 - javax.net.ssl.SSLException: Received fatal alert: certificate_unknown 00:32:05 458 INFO &#91;ExecutionScheduler_Worker-1&#93; JobRunShell:207 - Job LinearExecutionJobs.BatchRequestJob-2-1 threw a JobExecutionException: org.quartz.JobExecutionException: org.apache.ambari.server.AmbariException: Exception occurred while performing request &#91;See nested exception: org.apache.ambari.server.AmbariException: Exception occurred while performing request&#93; at org.apache.ambari.server.scheduler.AbstractLinearExecutionJob.execute(AbstractLinearExecutionJob.java:94) at org.quartz.core.JobRunShell.run(JobRunShell.java:202) at org.quartz.simpl.SimpleThreadPool$WorkerThread.run(SimpleThreadPool.java:573) Caused by: org.apache.ambari.server.AmbariException: Exception occurred while performing request at org.apache.ambari.server.scheduler.ExecutionScheduleManager.executeBatchRequest(ExecutionScheduleManager.java:479) at org.apache.ambari.server.state.scheduler.BatchRequestJob.doWork(BatchRequestJob.java:77) at org.apache.ambari.server.scheduler.AbstractLinearExecutionJob.execute(AbstractLinearExecutionJob.java:88) ... 2 more Caused by: com.sun.jersey.api.client.ClientHandlerException: javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No name matching localhost found at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:149) at com.sun.jersey.api.client.filter.CsrfProtectionFilter.handle(CsrfProtectionFilter.java:97) at org.apache.ambari.server.security.authorization.internal.InternalTokenClientFilter.handle(InternalTokenClientFilter.java:39) at com.sun.jersey.api.client.Client.handle(Client.java:648) at com.sun.jersey.api.client.WebResource.handle(WebResource.java:670) at com.sun.jersey.api.client.WebResource.method(WebResource.java:311) at org.apache.ambari.server.scheduler.ExecutionScheduleManager.performApiRequest(ExecutionScheduleManager.java:619) at org.apache.ambari.server.scheduler.ExecutionScheduleManager.executeBatchRequest(ExecutionScheduleManager.java:469) ... 4 more Caused by: javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No name matching localhost found at sun.security.ssl.Alerts.getSSLException(Alerts.java:192) at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1884) at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:276) at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:270) at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1341) at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:153) at sun.security.ssl.Handshaker.processLoop(Handshaker.java:868) at sun.security.ssl.Handshaker.process_record(Handshaker.java:804) at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1016) at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1312) at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1339) at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1323) at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:563) at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1091) at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250)
ns,View: Files UI clean-up and adjustments
ns,During installation through blueprint pig install fails with next exception:Traceback (most recent call last): File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/PIG/package/scripts/pig_client.py'  line 41  in &lt;module&gt; PigClient().execute() File '/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py'  line 112  in execute method(env) File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/PIG/package/scripts/pig_client.py'  line 30  in install self.configure(env) File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/PIG/package/scripts/pig_client.py'  line 35  in configure pig() File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/PIG/package/scripts/pig.py'  line 40  in pig properties=params.pig_properties) File '/usr/lib/python2.6/site-packages/resource_management/core/base.py'  line 148  in __init__ self.env.run() File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 149  in run self.run_action(resource  action) File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 115  in run_action provider_action() File '/usr/lib/python2.6/site-packages/resource_management/libraries/providers/properties_file.py'  line 48  in action_create mode = self.resource.mode File '/usr/lib/python2.6/site-packages/resource_management/core/base.py'  line 148  in __init__ self.env.run() File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 149  in run self.run_action(resource  action) File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 115  in run_action provider_action() File '/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py'  line 96  in action_create content = self._get_content() File '/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py'  line 136  in _get_content return content() File '/usr/lib/python2.6/site-packages/resource_management/core/source.py'  line 47  in __call__ return self.get_content() File '/usr/lib/python2.6/site-packages/resource_management/core/source.py'  line 126  in get_content rendered = self.template.render(self.context) File '/usr/lib/python2.6/site-packages/jinja2/environment.py'  line 891  in render return self.environment.handle_exception(exc_info  True) File '&lt;template&gt;'  line 2  in top-level template code File '/usr/lib/python2.6/site-packages/jinja2/filters.py'  line 176  in do_dictsort return sorted(value.items()  key=sort_func)AttributeError: 'unicode' object has no attribute 'items'
ns,Post install  Global properties are not being surfaced on service config page.
ns,Slider App details page should have an actions dropdown with the following actions Freeze: show when status == RUNNING Thaw: show when status == FROZEN Flex: show when status != FINISHED Destroy: show when status == FROZENExcept for Thaw  all actions will show a confirmation dialog. Hitting OK will make the call. Destroy should call DELETE on the app endpoint Freeze and Thaw should call PUT on app endpoint app state being set to FROZEN or RUNNING.
ns,Slider Apps View should provide /api/v1/app_types endpoint to provide definitions of various app_types supported by this Slider Apps View. Only apps in this type can be created through UI.
ns,Currently the new slider app wizard shows hardcoded app-types. We should instead show only those app-types which are returned by http://c6401:8080/api/v1/views/SLIDER/versions/1.0.0/instances/SLIDER_1/apptypes?fields=* endpoint.{ 'href' : 'http://c6401:8080/api/v1/views/SLIDER/versions/1.0.0/instances/SLIDER_1/apptypes?fields=*'  'items' : [ { 'href' : 'http://c6401:8080/api/v1/views/SLIDER/versions/1.0.0/instances/SLIDER_1/apptypes/ACCUMULO'  'id' : 'ACCUMULO'  'instance_name' : 'SLIDER_1'  'typeComponents' : [ { 'id' : 'ACCUMULO_MASTER'  'name' : 'ACCUMULO_MASTER'  'category' : 'MASTER'  'displayName' : 'ACCUMULO_MASTER'  'priority' : 1  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'ACCUMULO_MONITOR'  'name' : 'ACCUMULO_MONITOR'  'category' : 'MASTER'  'displayName' : 'ACCUMULO_MONITOR'  'priority' : 3  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'ACCUMULO_GC'  'name' : 'ACCUMULO_GC'  'category' : 'MASTER'  'displayName' : 'ACCUMULO_GC'  'priority' : 4  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'ACCUMULO_TRACER'  'name' : 'ACCUMULO_TRACER'  'category' : 'MASTER'  'displayName' : 'ACCUMULO_TRACER'  'priority' : 5  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'ACCUMULO_TSERVER'  'name' : 'ACCUMULO_TSERVER'  'category' : 'SLAVE'  'displayName' : 'ACCUMULO_TSERVER'  'priority' : 2  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'ACCUMULO_CLIENT'  'name' : 'ACCUMULO_CLIENT'  'category' : 'CLIENT'  'displayName' : 'ACCUMULO_CLIENT'  'priority' : 0  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 } ]  'typeDescription' : 'The Apache Accumulo sorted  distributed key/value store is a robust /n scalable  high performance data storage system that features cell-based/n access control and customizable server-side processing. It is based on/n Google's BigTable design and is built on top of Apache Hadoop /n Zookeeper  and Thrift./n Requirements:/n 1. Ensure parent dir for path (accumulo-site/instance.dfs.dir) is accessible to the App owner.'  'typeName' : 'ACCUMULO'  'typePackageFileName' : 'accumulo_v151.zip'  'typeVersion' : '1.5.1'  'version' : '1.0.0'  'view_name' : 'SLIDER'  'typeConfigs' : { 'agent.conf' : '/slider/agent/conf/agent.ini'  'application.def' : '/slider/accumulo_v151.zip'  'config_types' : 'accumulo-site'  'java_home' : '/usr/jdk64/jdk1.7.0_45'  'package_list' : 'files/accumulo-1.5.1-bin.tar.gz'  'site.accumulo-site.gc.port.client' : '0'  'site.accumulo-site.general.classpaths' : '$ACCUMULO_HOME/lib/accumulo-server.jar /n$ACCUMULO_HOME/lib/accumulo-core.jar /n$ACCUMULO_HOME/lib/accumulo-start.jar /n$ACCUMULO_HOME/lib/accumulo-fate.jar /n$ACCUMULO_HOME/lib/accumulo-proxy.jar /n$ACCUMULO_HOME/lib/[^.].*.jar /n$ZOOKEEPER_HOME/zookeeper[^.].*.jar /n$HADOOP_CONF_DIR /n$HADOOP_PREFIX/[^.].*.jar /n$HADOOP_PREFIX/lib/[^.].*.jar /n$HADOOP_PREFIX/share/hadoop/common/.*.jar /n$HADOOP_PREFIX/share/hadoop/common/lib/.*.jar /n$HADOOP_PREFIX/share/hadoop/hdfs/.*.jar /n$HADOOP_PREFIX/share/hadoop/mapreduce/.*.jar /n$HADOOP_PREFIX/share/hadoop/yarn/.*.jar /n/usr/lib/hadoop/.*.jar /n/usr/lib/hadoop/lib/.*.jar /n/usr/lib/hadoop-hdfs/.*.jar /n/usr/lib/hadoop-mapreduce/.*.jar /n/usr/lib/hadoop-yarn/.*.jar '  'site.accumulo-site.instance.dfs.dir' : '/apps/accumulo/data'  'site.accumulo-site.instance.secret' : 'DEFAULT'  'site.accumulo-site.instance.zookeeper.host' : '${ZK_HOST}'  'site.accumulo-site.master.port.client' : '0'  'site.accumulo-site.monitor.port.client' : '${ACCUMULO_MONITOR.ALLOCATED_PORT}'  'site.accumulo-site.monitor.port.log4j' : '0'  'site.accumulo-site.trace.port.client' : '0'  'site.accumulo-site.trace.token.property.password' : 'secret'  'site.accumulo-site.trace.user' : 'root'  'site.accumulo-site.tserver.cache.data.size' : '7M'  'site.accumulo-site.tserver.cache.index.size' : '20M'  'site.accumulo-site.tserver.memory.maps.max' : '80M'  'site.accumulo-site.tserver.port.client' : '0'  'site.accumulo-site.tserver.sort.buffer.size' : '50M'  'site.accumulo-site.tserver.walog.max.size' : '100M'  'site.global.accumulo_instance_name' : 'instancename'  'site.global.accumulo_root_password' : 'secret'  'site.global.app_install_dir' : '${AGENT_WORK_ROOT}/app/install'  'site.global.app_log_dir' : '${AGENT_LOG_ROOT}/app/log'  'site.global.app_pid_dir' : '${AGENT_WORK_ROOT}/app/run'  'site.global.app_root' : '${AGENT_WORK_ROOT}/app/install/accumulo-1.5.1'  'site.global.app_user' : 'yarn'  'site.global.gc_heapsize' : '64m'  'site.global.hadoop_conf_dir' : '/etc/hadoop/conf'  'site.global.hadoop_prefix' : '/usr/lib/hadoop'  'site.global.master_heapsize' : '128m'  'site.global.monitor_heapsize' : '64m'  'site.global.other_heapsize' : '128m'  'site.global.security_enabled' : 'false'  'site.global.tserver_heapsize' : '128m'  'site.global.user_group' : 'hadoop'  'site.global.zookeeper_home' : '/usr/lib/zookeeper' } }  { 'href' : 'http://c6401:8080/api/v1/views/SLIDER/versions/1.0.0/instances/SLIDER_1/apptypes/HBASE'  'id' : 'HBASE'  'instance_name' : 'SLIDER_1'  'typeComponents' : [ { 'id' : 'HBASE_MASTER'  'name' : 'HBASE_MASTER'  'category' : 'MASTER'  'displayName' : 'HBASE_MASTER'  'priority' : 1  'instanceCount' : 1  'maxInstanceCount' : 2  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'HBASE_REGIONSERVER'  'name' : 'HBASE_REGIONSERVER'  'category' : 'SLAVE'  'displayName' : 'HBASE_REGIONSERVER'  'priority' : 2  'instanceCount' : 1  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'HBASE_CLIENT'  'name' : 'HBASE_CLIENT'  'category' : 'CLIENT'  'displayName' : 'HBASE_CLIENT'  'priority' : 0  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 } ]  'typeDescription' : 'Apache HBase is the Hadoop database  a distributed  scalable  big data store./n Requirements:/n 1. Ensure parent dir for path (hbase-site/hbase.rootdir) is accessible to the App owner./n 2. Ensure ZK root (hbase-site/zookeeper.znode.parent) is unique for the App instance.'  'typeName' : 'HBASE'  'typePackageFileName' : 'hbase_v096 (1).zip'  'typeVersion' : '0.96.0.2.1.1'  'version' : '1.0.0'  'view_name' : 'SLIDER'  'typeConfigs' : { 'agent.conf' : '/slider/agent/conf/agent.ini'  'application.def' : '/slider/hbase_v096.zip'  'config_types' : 'core-site hdfs-site hbase-site'  'java_home' : '/usr/jdk64/jdk1.7.0_45'  'package_list' : 'files/hbase-0.96.1-hadoop2-bin.tar.gz'  'site.core-site.fs.defaultFS' : '${NN_URI}'  'site.global.app_install_dir' : '${AGENT_WORK_ROOT}/app/install'  'site.global.app_log_dir' : '${AGENT_LOG_ROOT}/app/log'  'site.global.app_pid_dir' : '${AGENT_WORK_ROOT}/app/run'  'site.global.app_root' : '${AGENT_WORK_ROOT}/app/install/hbase-0.96.1-hadoop2'  'site.global.app_user' : 'yarn'  'site.global.ganglia_server_host' : '${NN_HOST}'  'site.global.ganglia_server_id' : 'Application1'  'site.global.ganglia_server_port' : '8667'  'site.global.hbase_master_heapsize' : '1024m'  'site.global.hbase_regionserver_heapsize' : '1024m'  'site.global.security_enabled' : 'false'  'site.global.user_group' : 'hadoop'  'site.hbase-site.hbase.client.keyvalue.maxsize' : '10485760'  'site.hbase-site.hbase.client.scanner.caching' : '100'  'site.hbase-site.hbase.cluster.distributed' : 'true'  'site.hbase-site.hbase.defaults.for.version.skip' : 'true'  'site.hbase-site.hbase.hregion.majorcompaction' : '86400000'  'site.hbase-site.hbase.hregion.max.filesize' : '10737418240'  'site.hbase-site.hbase.hregion.memstore.block.multiplier' : '2'  'site.hbase-site.hbase.hregion.memstore.flush.size' : '134217728'  'site.hbase-site.hbase.hregion.memstore.mslab.enabled' : 'true'  'site.hbase-site.hbase.hstore.blockingStoreFiles' : '10'  'site.hbase-site.hbase.hstore.compactionThreshold' : '3'  'site.hbase-site.hbase.hstore.flush.retries.number' : '120'  'site.hbase-site.hbase.local.dir' : '${hbase.tmp.dir}/local'  'site.hbase-site.hbase.master.info.port' : '${HBASE_MASTER.ALLOCATED_PORT}'  'site.hbase-site.hbase.regionserver.global.memstore.lowerLimit' : '0.38'  'site.hbase-site.hbase.regionserver.global.memstore.upperLimit' : '0.4'  'site.hbase-site.hbase.regionserver.handler.count' : '60'  'site.hbase-site.hbase.regionserver.info.port' : '0'  'site.hbase-site.hbase.regionserver.port' : '0'  'site.hbase-site.hbase.rootdir' : '${NN_URI}/apps/hbase/data'  'site.hbase-site.hbase.security.authentication' : 'simple'  'site.hbase-site.hbase.security.authorization' : 'false'  'site.hbase-site.hbase.stagingdir' : '${NN_URI}/apps/hbase/staging'  'site.hbase-site.hbase.superuser' : 'yarn'  'site.hbase-site.hbase.tmp.dir' : '${AGENT_WORK_ROOT}/work/app/tmp'  'site.hbase-site.hbase.zookeeper.property.clientPort' : '2181'  'site.hbase-site.hbase.zookeeper.quorum' : '${ZK_HOST}'  'site.hbase-site.hbase.zookeeper.useMulti' : 'true'  'site.hbase-site.hfile.block.cache.size' : '0.40'  'site.hbase-site.zookeeper.session.timeout' : '30000'  'site.hbase-site.zookeeper.znode.parent' : '/hbase-unsecure'  'site.hdfs-site.dfs.namenode.http-address' : '${NN_HOST}:50070'  'site.hdfs-site.dfs.namenode.https-address' : '${NN_HOST}:50470' } }  { 'href' : 'http://c6401:8080/api/v1/views/SLIDER/versions/1.0.0/instances/SLIDER_1/apptypes/STORM'  'id' : 'STORM'  'instance_name' : 'SLIDER_1'  'typeComponents' : [ { 'id' : 'NIMBUS'  'name' : 'NIMBUS'  'category' : 'MASTER'  'displayName' : 'NIMBUS'  'priority' : 1  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'STORM_REST_API'  'name' : 'STORM_REST_API'  'category' : 'MASTER'  'displayName' : 'STORM_REST_API'  'priority' : 2  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'SUPERVISOR'  'name' : 'SUPERVISOR'  'category' : 'SLAVE'  'displayName' : 'SUPERVISOR'  'priority' : 5  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'STORM_UI_SERVER'  'name' : 'STORM_UI_SERVER'  'category' : 'MASTER'  'displayName' : 'STORM_UI_SERVER'  'priority' : 3  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'DRPC_SERVER'  'name' : 'DRPC_SERVER'  'category' : 'MASTER'  'displayName' : 'DRPC_SERVER'  'priority' : 4  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 } ]  'typeDescription' : 'Apache Hadoop Stream processing framework'  'typeName' : 'STORM'  'typePackageFileName' : 'storm_v091.zip'  'typeVersion' : '0.9.1.2.1'  'version' : '1.0.0'  'view_name' : 'SLIDER'  'typeConfigs' : { 'agent.conf' : '/slider/agent/conf/agent.ini'  'application.def' : '/slider/storm_v091.zip'  'config_types' : 'storm-site'  'java_home' : '/usr/jdk64/jdk1.7.0_45'  'package_list' : 'files/apache-storm-0.9.1.2.1.1.0-237.tar.gz'  'site.global.app_root' : '${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237'  'site.global.app_user' : 'yarn'  'site.global.ganglia_server_host' : '${NN_HOST}'  'site.global.ganglia_server_id' : 'Application2'  'site.global.rest_api_admin_port' : '${STORM_REST_API.ALLOCATED_PORT}'  'site.global.rest_api_port' : '${STORM_REST_API.ALLOCATED_PORT}'  'site.global.security_enabled' : 'false'  'site.global.user_group' : 'hadoop'  'site.storm-site.dev.zookeeper.path' : '${AGENT_WORK_ROOT}/app/tmp/dev-storm-zookeeper'  'site.storm-site.drpc.childopts' : '-Xmx768m'  'site.storm-site.drpc.invocations.port' : '${DRPC_SERVER.ALLOCATED_PORT}'  'site.storm-site.drpc.port' : '${DRPC_SERVER.ALLOCATED_PORT}'  'site.storm-site.drpc.queue.size' : '128'  'site.storm-site.drpc.request.timeout.secs' : '600'  'site.storm-site.drpc.worker.threads' : '64'  'site.storm-site.java.library.path' : '/usr/local/lib:/opt/local/lib:/usr/lib'  'site.storm-site.logviewer.appender.name' : 'A1'  'site.storm-site.logviewer.childopts' : '-Xmx128m'  'site.storm-site.logviewer.port' : '${SUPERVISOR.ALLOCATED_PORT}'  'site.storm-site.nimbus.childopts' : '-Xmx1024m -Djava.security.auth.login.config=/etc/storm/storm_jaas.conf -javaagent:${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host={0} port=8669 wireformat31x=true mode=multicast config=${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Nimbus_JVM'  'site.storm-site.nimbus.cleanup.inbox.freq.secs' : '600'  'site.storm-site.nimbus.file.copy.expiration.secs' : '600'  'site.storm-site.nimbus.host' : '${NIMBUS_HOST}'  'site.storm-site.nimbus.inbox.jar.expiration.secs' : '3600'  'site.storm-site.nimbus.monitor.freq.secs' : '10'  'site.storm-site.nimbus.reassign' : 'true'  'site.storm-site.nimbus.supervisor.timeout.secs' : '60'  'site.storm-site.nimbus.task.launch.secs' : '120'  'site.storm-site.nimbus.task.timeout.secs' : '30'  'site.storm-site.nimbus.thrift.max_buffer_size' : '1048576'  'site.storm-site.nimbus.thrift.port' : '${NIMBUS.ALLOCATED_PORT}'  'site.storm-site.nimbus.topology.validator' : 'backtype.storm.nimbus.DefaultTopologyValidator'  'site.storm-site.storm.cluster.mode' : 'distributed'  'site.storm-site.storm.local.dir' : '${AGENT_WORK_ROOT}/app/tmp/storm'  'site.storm-site.storm.local.mode.zmq' : 'false'  'site.storm-site.storm.messaging.netty.buffer_size' : '5242880'  'site.storm-site.storm.messaging.netty.client_worker_threads' : '1'  'site.storm-site.storm.messaging.netty.max_retries' : '30'  'site.storm-site.storm.messaging.netty.max_wait_ms' : '1000'  'site.storm-site.storm.messaging.netty.min_wait_ms' : '100'  'site.storm-site.storm.messaging.netty.server_worker_threads' : '1'  'site.storm-site.storm.messaging.transport' : 'backtype.storm.messaging.netty.Context'  'site.storm-site.storm.thrift.transport' : 'backtype.storm.security.auth.SimpleTransportPlugin'  'site.storm-site.storm.zookeeper.connection.timeout' : '15000'  'site.storm-site.storm.zookeeper.port' : '2181'  'site.storm-site.storm.zookeeper.retry.interval' : '1000'  'site.storm-site.storm.zookeeper.retry.intervalceiling.millis' : '30000'  'site.storm-site.storm.zookeeper.retry.times' : '5'  'site.storm-site.storm.zookeeper.root' : '/storm'  'site.storm-site.storm.zookeeper.servers' : '['${ZK_HOST}']'  'site.storm-site.storm.zookeeper.session.timeout' : '20000'  'site.storm-site.supervisor.childopts' : '-Xmx256m -Djava.security.auth.login.config=/etc/storm/storm_jaas.conf -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=${SUPERVISOR.ALLOCATED_PORT} -javaagent:${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host={0} port=8669 wireformat31x=true mode=multicast config=${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Supervisor_JVM'  'site.storm-site.supervisor.enable' : 'true'  'site.storm-site.supervisor.heartbeat.frequency.secs' : '5'  'site.storm-site.supervisor.monitor.frequency.secs' : '3'  'site.storm-site.supervisor.slots.ports' : '[${SUPERVISOR.ALLOCATED_PORT}  ${SUPERVISOR.ALLOCATED_PORT}]'  'site.storm-site.supervisor.worker.start.timeout.secs' : '120'  'site.storm-site.supervisor.worker.timeout.secs' : '30'  'site.storm-site.task.heartbeat.frequency.secs' : '3'  'site.storm-site.task.refresh.poll.secs' : '10'  'site.storm-site.topology.acker.executors' : 'null'  'site.storm-site.topology.builtin.metrics.bucket.size.secs' : '60'  'site.storm-site.topology.debug' : 'false'  'site.storm-site.topology.disruptor.wait.strategy' : 'com.lmax.disruptor.BlockingWaitStrategy'  'site.storm-site.topology.enable.message.timeouts' : 'true'  'site.storm-site.topology.error.throttle.interval.secs' : '10'  'site.storm-site.topology.executor.receive.buffer.size' : '1024'  'site.storm-site.topology.executor.send.buffer.size' : '1024'  'site.storm-site.topology.fall.back.on.java.serialization' : 'true'  'site.storm-site.topology.kryo.factory' : 'backtype.storm.serialization.DefaultKryoFactory'  'site.storm-site.topology.max.error.report.per.interval' : '5'  'site.storm-site.topology.max.spout.pending' : 'null'  'site.storm-site.topology.max.task.parallelism' : 'null'  'site.storm-site.topology.message.timeout.secs' : '30'  'site.storm-site.topology.optimize' : 'true'  'site.storm-site.topology.receiver.buffer.size' : '8'  'site.storm-site.topology.skip.missing.kryo.registrations' : 'false'  'site.storm-site.topology.sleep.spout.wait.strategy.time.ms' : '1'  'site.storm-site.topology.spout.wait.strategy' : 'backtype.storm.spout.SleepSpoutWaitStrategy'  'site.storm-site.topology.state.synchronization.timeout.secs' : '60'  'site.storm-site.topology.stats.sample.rate' : '0.05'  'site.storm-site.topology.tick.tuple.freq.secs' : 'null'  'site.storm-site.topology.transfer.buffer.size' : '1024'  'site.storm-site.topology.trident.batch.emit.interval.millis' : '500'  'site.storm-site.topology.tuple.serializer' : 'backtype.storm.serialization.types.ListDelegateSerializer'  'site.storm-site.topology.worker.childopts' : 'null'  'site.storm-site.topology.worker.shared.thread.pool.size' : '4'  'site.storm-site.topology.workers' : '1'  'site.storm-site.transactional.zookeeper.port' : 'null'  'site.storm-site.transactional.zookeeper.root' : '/transactional'  'site.storm-site.transactional.zookeeper.servers' : 'null'  'site.storm-site.ui.port' : '${STORM_UI_SERVER.ALLOCATED_PORT}'  'site.storm-site.worker.childopts' : '-Xmx768m -javaagent:${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host={0} port=8669 wireformat31x=true mode=multicast config=${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Worker_%ID%_JVM'  'site.storm-site.worker.heartbeat.frequency.secs' : '1'  'site.storm-site.zmq.hwm' : '0'  'site.storm-site.zmq.linger.millis' : '5000'  'site.storm-site.zmq.threads' : '1' } } ]}
ns,During bootstrapping of agents  installation retrieve versions of available packages from system package manager. After some test was founded that in different situations package managers can produce specific info/debug output which can be reason of wrong parsing of output.Example:Zypper command:se2mon1400652583-9:/etc/zypp/repos.d # zypper search -s --match-exact ambari-agent Building repository 'Hortonworks Data Platform Utils Version - HDP-UTILS-1.1.0.16' cache &#91;done&#93;Building repository 'Hortonworks Data Platform Utils Version - HDP-UTILS-1.1.0.17' cache &#91;done&#93;Building repository 'Hosted (SLES_11)' cache &#91;done&#93;Building repository 'ambari-1.6.0 - Updates' cache &#91;done&#93;Building repository 'Ambari 1.x' cache &#91;done&#93;Building repository 'PostgreSQL and related packages (SLE_11_SP3)' cache &#91;done&#93;Loading repository data...Reading installed packages...S | Name | Type | Version | Arch | Repository -----------------------------------+---------------------- ambari-agent  package  1.6.0-46  x86_64  ambari-1.6.0 - Updates ambari-agent  package  1.2.0.1-1  x86_64  Ambari 1.xParsed as: Building repository 'ambari-1.6.0 We should suppress any debug/info output in setupAgent.py to avoid any unexpected situation.
ns,As a part of this ticket 1) Remove rename icon from infront of the file and make it a different column.2) reorder the icon bar in the sequence Download  Move  Rename and Delete
ns,API should be able to process filter with predicates(&lt; &gt; =) for fields with float values; For example field: metrics/load/load_one.Currently the Greater than predicate will fail for values in between 0.0 and 1.0
s,This HBase properties are empty after install via blueprint: hbase.coprocessor.region.classes hbase.coprocessor.master.classesBut they required to be filled.After enabling security they became filled.
ns,Lets say I have a running app. I freeze it and then destroy it. The app goes away from the /apps response. However the UI still continues to show it. I think the mapper is not removing deleted entries from the model.Mapper should remove entries not being sent by /apps.
ns,getEffectiveState should not fetch map of all hosts every time Status commands should not be sent until component is installed
ns,Ambari should generate config files sorted by key. It would make doing compares between files much easier.
ns,We can get such situation when user using custom java.
ns,UI keeps showing that the agents are being installed  because the bootstrap API GET call keeps returning that hostsStatus/status is RUNNINGI suspected time.sleep(1) instruction. If restart takes too long time  situation  when ambari-agent.log has not been created yet is possible. So  tail command returned not 0 retcode and exit from procedure. But in logs I cant see any non-zero retcodes. Thus  too short logs could be explaned by this issue.
ns,When using a proxy for internet access  add support to set properties for proxy user and password.Dhttp.proxyUser=someUserName -Dhttp.proxyPassword=somePassword
ns,quick link for Storm UI was linked with nimbus server. it should use Storm UI server host.
ns,Some of the storm configs have extra 'localhost' appended to the host name.'nimbus.childopts' : '-Xmx1024m -Djava.security.auth.login.config=/etc/storm/conf/storm_jaas.conf -javaagent:/usr/lib/storm/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host=c6401.ambari.apache.orglocalhost port=8649 wireformat31x=true mode=multicast config=/usr/lib/storm/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Nimbus_JVM''supervisor.childopts' : '-Xmx256m -Djava.security.auth.login.config=/etc/storm/conf/storm_jaas.conf -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=56431 -javaagent:/usr/lib/storm/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host=c6401.ambari.apache.orglocalhost port=8650 wireformat31x=true mode=multicast config=/usr/lib/storm/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Supervisor_JVM''worker.childopts' : '-Xmx768m -javaagent:/usr/lib/storm/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host=c6401.ambari.apache.orglocalhost port=8650 wireformat31x=true mode=multicast config=/usr/lib/storm/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Worker_%ID%_JVM'
ns,STR: Stop HDFS with started Nagios service. Turn on Maintenance mode for HDFS.Result: All alerts dissapear  except 'HDFS capacity utilization' - it hangs for a long time (in my case it was smthng about 10-11 minutes)
ns,Config page for services that use App.YARNDefaultsProvider is not displayed.Following JS error thrown:Uncaught TypeError: Cannot call method 'forEach' of null yarn_defaults_provider.js:291
ns,remove dots in --help params description and in warningscopy fails if jdbc selected by --jdbc-driver is already in resourceshide other points of ambari-server setup if jdbc options are passed and server is running
ns,Master components info is missing in service summary.
ns,PROBLEM: If the Ambari Agent installation fails for whatever reason then a process of ambari-agent is left running. This results in the ambari-agent status to show as it not running. If you then start another ambari-agent it dies because the port is already in use.If the script could check the PID file and check for a running process then it would resolve this issue.BUSINESS IMPACT: Not a huge business impact as the workaround is to kill the running ambari-agent processWorkaround: Kill running ambari-agent process before startingANALYSIS: I cannot reproduce this issue in house and the SE who raised it can not reproduce on demand.
ns,Add Host wizard get stuck on Confirm Hosts step
ns,When posting a command such as{ 'RequestInfo': { 'action': 'check_host'  'context': 'Check host'  'parameters': { 'check_execute_list': 'host_resolution_check'  'hosts': 'c6401.ambari.apache.org  c6402.ambari.apache.org  c6403.ambari.apache.org  foobar'  'threshold': '20' } }  'Requests/resource_filters': [ { 'hosts': 'c6401.ambari.apache.org c6402.ambari.apache.org' } ]The returned result from the custom action has some integer values coerced into floats: 'structured_out' : { 'host_resolution_check' : { 'exit_code' : '0'  'failed_count' : 0.0  'failures' : [ ]  'message' : 'All hosts resolved to an IP address.'  'success_count' : 4.0 }The structured_output written out to disk does NOT have the float values:{'host_resolution_check': {'failures': []  'message': 'All hosts resolved to an IP address.'  'failed_count': 0  'success_count': 4  'exit_code': '0'}} Therefore this is a problem with the framework and not the command.
ns,Call refreshqueues from the REST API (replace resource.manager.host and YourClusterName). And be sure to include header 'X-Requested-By' : 'ambari' and set authentication.POST/api/v1/clusters/YourClusterName/requests/{ 'RequestInfo' : { 'command' : 'REFRESHQUEUES'  'context' : 'Refresh YARN Capacity Scheduler' }  'Requests/resource_filters': [{ 'service_name' : 'YARN'  'component_name' : 'RESOURCEMANAGER'  'hosts' : 'resource.manager.host' }]}
ns,Problem:Ambari Server command 'ssh root@vmhost ambari-server start' hangs on message 'Ambari Server 'start' completed successfully.'. Same command executed successfully on the local console  as a result this behavior can be reproduced only via remote execution of commands.How to reproduce: Deploy server Stop server locally Start server from another host using ssh command
ns,When user clicks on some component filter on Service summary page  it opens hosts page  but filter is not applied.
ns,STR1. Go to add Service Wizard.2. Select some new services and proceed to deploy.3. Close Wizard (Esc-button).4. Wait a little bit (maybe page-refresh needed).5. Go to YARN summary.6. New 'none' components will appear periodically.See screenshot.
ns,The SqlServerSink should support pushing metrics to MySQL store.This Jira addresses changes needed to support sink to a MySQL store.
ns,Steps to reproduce:1. Go to Hosts page2. Choose filter AlertsResult:Hosts are not filtered by alerts.The request with filter by alerts has incorrect url data.
ns,Using SLES 11 SP3 quick-start image on EC2. Doesn't look like php5-json is available  but php53-json is available.WORKAROUND:I modified NAGIOS/metainfo.xml and this worked. &lt;package&gt; &lt;name&gt;php5*-json&lt;/name&gt; &lt;/package&gt;
ns,1) on 'manage config groups' pressing return does something even though return is not valid (i.e. can't save)2) Once you press return in #1  then you have to press esc twice to close the dialog3) once you open the nested dialog (to add a group)  esc closes the parent  then esc again  closes the nested dialog4) once you open the nested dialog  also notice it doesn't start focus on the name field  you have to click to get that focus.
ns,Ambari REST client (Groovy) enhancements -refactor - simplified unit tests taking leverage groovy's metaclass capabilities-more REST calls implemented-api compatibility with version 1.6.0
ns,Hosts page  'Actions' menu (dropdown)Each action shows confirmation popup with list of affected hosts.For big cluster this may be 2000+ hosts.If there are more than 3 hosts  then show 'host1  host2  host3  and X more hosts show all'
ns,Proceed to step3  wait while registration is complete.Host checks requests are set every second.UI should set next request only when previous is completed.
ns,It's not possible to input 'Enter' on 'Target hosts' textarea.
ns,STR:1. go to dashboard page2. click NameNode link inside HDFS Links widget.instead of go to NameNode host detail page  it returns an empty page
ns,Go to installer step7Click override for some propertySelect 'New Config Group'Click 'OK'JS-error appears - 404 error. Missing clusterName in request URL.
ns,If I have a host that has IP 10.0.2.15  in Ambari 1.6.0 if I start to filter by IP (by typing 10.0....)  the hosts that match 'startsWith' stay displayed.In Ambari 1.6.1  now it only does exact match  so once I start typing  all hosts disappear until I finally type the whole thing in for exact match.Hosts/ip.matches(10)
ns,Steps To Reproduce Select 2.X stack and go ahead to 'Select Services' page Navigate back to 'Select Stack' page and select 1.x stack Go ahead to step-8 'Review' page. On clicking next  API call to create components for HDFS service fails with UI displaying an error message.Invalid Request: Unsupported or invalid component in stack  clusterName=cc  serviceName=HDFS  componentName=JOURNALNODE  stackInfo=HDP-1.3
s,STR:  Installed a 3-node cluster for HDP 1.3 stack HDFS+MapReduce+Nagios+Ganglia+zooKeeper installed with slave components installed on all 3 hosts. Enable security with no kerberos setup On expected failure of security wizard  Disable security. After successfully disabling security  Following API returns incorrect number for started_count of Datanode. It says 0 but Datanode is actually running on all hostshttp://server:8080/api/v1/clusters/c1/components/?ServiceComponentInfo/category.in(SLAVE CLIENT)&amp;fields=ServiceComponentInfo/service_name ServiceComponentInfo/installed_count ServiceComponentInfo/started_count ServiceComponentInfo/total_count&amp;minimal_response=trueReason:During wrong kerberos setup DN processes fail to start  but leave stale pid file owned by root. Next one DN start command starts DN process  but can not override pid file. So the server considers DN as stopped. If we start DN once more  commands fail soon after start (due to lock file at data dir owned by already running DN). Agent reports to server that DN is not running  so server displays a correct information from his point of view.
ns,STR:Delete all widgets from dashboard.Go to Metrics-&gt;Add menu.Result: Appeared inappropriate dropdown: bad_metrics.png
ns,Delete operation is not allowed for a hostComponent if it is in STARTED state.Currently delete menu item is shown when a hostComponent is flagged decommissioned and in STARTED state. Performing delete operation in this condition returns API 500 server error. If the hostComponent is brought in INSTALLED state with the decommissioned flag and then delete operation is performed then it happens as expectedDelete menu item should be shown When hostComponent is in INSTALLED state and should be grayed when it is in STARTED state. ambari-web client should not consider decommission status of a hostComponent while validating the required condition to enable/disable 'delete' menu item.
ns,On the second step of HA 'Enable NameNode HA Wizard' do not work listboxes with hostnames. Real additional components position does not depend from values in listboxes (see screenshots).It is possible to choose any host in any listbox  which is incorrect.
ns,The /stacks api uses sub-resource names such as stackServices and serviceComponents instead of services and components which are the names of the resources specified in the URL. These incorrect resource names would need to be used in any queries for stack resources.For example:To get stack service named HDFS the URL would be:api/v1/stacks/HDP/versions/2.1/services/HDFSBut  if we wanted to do a query of for HDFS services across all versions:api/v1/stacks/HDP/versions?stackServices/StackServices/service_name=HDFSInstead this should be:api/v1/stacks/HDP/versions?services/StackServices/service_name=HDFSFix all sub-resource names that are returned and fix sub-resource names used in queries and partial response.
ns,STR:1. Open HA wizard2. Proceed to Select Host step3. Try to select host for any master componentResult:nothing changing  js error emerge.
ns,Tasks with 'aborted' status are not included in any 'Aborted' category.
ns,On the Step 7 namenode_heapsize value is empty and on deploy even if its value filled it saved as 'm' which cause error on NameNode startup.Part of object passed for configuration saving:namenode_heapsize: 'm'namenode_opt_maxnewsize: '200m'namenode_opt_newsize: '200m'nodemanager_heapsize: '1024'
ns,On service page for Nagios  under 'Service actions' menu  'Restart all' operation does not work correctly. There is dialog window after pressing  but after confirmation there is not any activity. In API there is not any new request.Note: described situation relates only for Nagios. Other services make restart correctly (including generating new requests in API).
ns,I've added 1 host through the add host wizardon deploy step all hosts were present. and install operations were performed on all of them.
ns,When performing ambari-server reset with external DB  ambari-server does nothing but outputs commands for resetting it manually.But:1. Commands are wrong  at least on Suse:su -postgres --command=psql -f /var/lib/ambari-server/resources/Ambari-DDL-Postgres-DROP.sql -v username=''ambari'' -v password=''bigdata''su: invalid option -- 'o'Try 'su --help' for more information.2. This commands should take in account that external DB can be located on the another host.3. Maybe the best option would be to give user ability to reset automatically  for example via command line switch like ambari-server reset -a
ns,On 'Confirm Hosts' step &gt; Hosts Check popup window  the 'Show Reports' link is missing even if the host check warnings existed.Reason:The newly added hosts checks (jdk  disk  repo and hostNameResolution) dont trigger the ''show reports' link to show up.
s,During generating private key and certificates using openssl password of key shown in logs:11:21:30 735 INFO [main] ShellCommandUtil:44 - Command openssl genrsa -des3 -passout pass:**** -out /var/lib/ambari-server/keys/ca.key 4096 was finished with exit code: 0 - the operation was completely successfully.11:21:30 750 INFO [main] ShellCommandUtil:44 - Command openssl req -passin pass:**** -new -key /var/lib/ambari-server/keys/ca.key -out /var/lib/ambari-server/keys/ca.csr -batch was finished with exit code: 0 - the operation was completely successfully.11:21:30 766 INFO [main] ShellCommandUtil:44 - Command open**** ca -create_serial -out /var/lib/ambari-server/keys/ca.crt -days 365 -keyfile /var/lib/ambari-server/keys/ca.key -key vgGAzzSaCPkI3F7UU7qZZY6CahDUTSnY7B9a8TH0YiGDB10LdJ -selfsign -extensions jdk7_ca -config /var/lib/ambari-server/keys/ca.config -batch -infiles /var/lib/ambari-server/keys/ca.csr was finished with exit code: 0 - the operation was completely successfully.11:21:30 773 INFO [main] ShellCommandUtil:44 - Command openssl pkcs12 -export -in /var/lib/ambari-server/keys/ca.crt -inkey /var/lib/ambari-server/keys/ca.key -certfile /var/lib/ambari-server/keys/ca.crt -out /var/lib/ambari-server/keys/keystore.p12 -password pass:**** -passin pass:**** see '-key vgGAzzSaCPkI3F7UU7qZZY6CahDUTSnY7B9a8TH0YiGDB10LdJ'
ns,In 1.6.1  filtering/sorting/paging on the Hosts page has been converted from client-side to server-side in order to address scalability issues.As a result of that  the responsiveness of UI is dependent upon how quickly the server can respond to filtering/sorting/paging calls (and also depends on the network). While UI is waiting for the new table content to come from the server  there should be some indication that work is in progress. For example  we can put an overlay on the table with a spinner (gray out the table with a spinner on top - kind of like when switching filters on JIRA's Agile Board).
ns,STR:Change property for some service (Hive as example)Save changesClick Service Actions-&gt;Restart all buttonActual result:Service Actions-&gt;Restart all button does not work (do nothing). Restart passed  but restart icon still present.Expected result:Service Actions-&gt;Restart all button works. Restart passed  restart icon is not present after action.
ns,STR Install cluster Add hostActual resultAfter adding host all components on all hosts are stoppedExpected resultAll components on all hosts are started
ns,STR:1) Deploy cluster with defualt settings.2) Navigate on Dashboard pageExpected result:All charts are present.Actual result:Have spinners instead some charts.
ns,Get sql metrics logic should be reviewed
ns,Upon clicking on 'Deploy' from Review page in Add Services Wizard  sometimes the UI shows a server error saying 'Resource Already Exists'.It looks like the UI is trying to add client components on hosts that already have them.I've seen it for HDFS_CLIENT  MAPREDUCE2_CLIENT  etc.  when trying to add Oozie (for sure)  Storm (I think)  and possibly others.
ns,step 6 pagination is slow
ns,That is a bit confusing we should change that  since we changed the way itworks &lt;os type='redhat6'&gt;should be &lt;os family='redhat6'&gt;
ns,We take some data from Ganglia metrics for hosts and if Ganglia is not installed browser throw js error which blocks popup initializing.
ns,When initiating Rolling Restart / Service Stop  they would like an option (via a checkbox  for example) to put the service in maintenance mode (if it is not already in MM) to avoid getting a lot of alerts.
ns,Currently if you use the python client  if you issue a DELETE request followed by a GET request  that GET request becomes a DELETE request. You can imagine why that's undesirable.The problem is that we set the CUSTOMREQUEST field on the DELETE  but we don't unset it on the next GET. pycurl sees it still set and assumes we're still doing a DELETE.
ns,It seems that we call App.componentConfigMapper every 15 seconds.This mapper takes more than 2 seconds to run. While the mapper is running  the entire UI is frozen.
ns,It takes significant amount of time to populate clusterHostInfo for every Execution Cmd on a 2000 node cluster.org.apache.ambari.server.utils.StageUtils.getClusterHostInfo(Map  Cluster)
ns,Causes CPU usage go upto 1500 % and UI becomes unusable.1000's of Exception:00:01:50 666 ERROR [pool-1-thread-17] JMXPropertyProvider:539 - Caught exception getting JMX metrics : Connection refusedAll JMX endpoints called for any JMX metric.
ns,It seems that the max number of target hosts for a bulk operation is capped by the current 'page size' on the Hosts page.For example  on the 2K cluster with 2K DataNodes: Go to Hosts page Click on Actions. Either going to Filtered Hosts (2005) or All Hosts (2005)  select 'DataNodes &gt; Stop' A confirmation popup tells me that 50 DataNodes are about to be stopped. This should have been ~ 2000 DataNodes instead. 50 is the current page size on the Hosts page that I set.Another scenario: Set page size to 50. Select 100 hosts using 'check all'  'next page'  then 'check all' Go to 'Actions &gt; Selected Hosts &gt; DataNodes &gt; Stop'. Again  it tries to perform actions on only 50 DataNodes.
ns,Steps to reproduce1. Filter by DataNodes2. Increase the page size to 503. Select all 50 and decommision datanodes.This ends up with a spinner .This might be a blocker. Attaching snapshot.
ns,Rolling restart page  there is a ' Only restart NodeManagers with stale configs' checkbox  nothing happened no matter if that one was checked or not. 1. On service(HDFS or YARN) actions &gt; restart all DN (NM). All DN(NM) will be restart no matter if the ' Only restart NodeManagers with stale configs' option checked.
ns,header in task details popup looks broken when we run decommission for many Datanodes.
ns,Add service wizard removes any new property added to core-site and global after cluster installation
ns,STR1. Go to hosts page2. Filter by Components. Check DATANODE3. At Bottom right corner I see Show 10 | 1-10 of 10
ns,1. page size 102. select all 10 hosts on first page3. go to page 2  select 2 hosts (total 12 selection)4. click on '12 host selected' and it goes to page 1  1-10 of 12 hosts (correct)5. click to go to page 2 (to see the 2 hosts selected) but it resets to page 1  1-10 of 103
ns,STR1. Go to MapReduce Config Page2. Change JobTracker new generation size with a new value3. Clike Save buttonResult no confirmation popup  and save button turns grey  when switching pages the save button in the warning popup is not working neither.
ns,On a 110 node cluster I went to the hosts page and went into All Hosts &gt; Hosts &gt; Restart All Components. Following that I got an error dialog (image attached) and the below exceptions in the log21:03:20 017 ERROR [qtp1391464722-1188] AmbariJpaLocalTxnInterceptor:114 - [DETAILED ERROR] Rollback reason:Local Exception Stack:Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.4.0.v20120608-r11652): org.eclipse.persistence.exceptions.DatabaseExceptionInternal Exception: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO requestoperationlevel (operation_level_id  cluster_name  host_component_name  host_name  level_name  request_id  service_name) VALUES (2  'Horton'  NULL  'horton-1.c.pramod-thangali.internal horton-10.c.pramod-thangali.internal horton-100.c.pramod-thangali.internal horton-101.c.pramod-thangali.internal horton-103.c.pramod-thangali.internal horton-104.c.pramod-thangali.internal horton-105.c.pramod-thangali.internal horton-106.c.pramod-thangali.internal horton-107.c.pramod-thangali.internal horton-108.c.pramod-thangali.internal horton-109.c.pramod-thangali.internal horton-11.c.pramod-thangali.internal horton-12.c.pramod-thangali.internal horton-13.c.pramod-thangali.internal horton-14.c.pramod-thangali.internal horton-15.c.pramod-thangali.internal horton-16.c.pramod-thangali.internal horton-17.c.pramod-thangali.internal horton-18.c.pramod-thangali.internal horton-19.c.pramod-thangali.internal horton-2.c.pramod-thangali.internal horton-20.c.pramod-thangali.internal horton-21.c.pramod-thangali.internal horton-22.c.pramod-thangali.internal horton-23.c.pramod-thangali.internal horton-24.c.pramod-thangali.internal horton-25.c.pramod-thangali.internal horton-26.c.pramod-thangali.internal horton-27.c.pramod-thangali.internal horton-28.c.pramod-thangali.internal horton-29.c.pramod-thangali.internal horton-3.c.pramod-thangali.internal horton-30.c.pramod-thangali.internal horton-31.c.pramod-thangali.internal horton-32.c.pramod-thangali.internal horton-33.c.pramod-thangali.internal horton-34.c.pramod-thangali.internal horton-35.c.pramod-thangali.internal horton-36.c.pramod-thangali.internal horton-37.c.pramod-thangali.internal horton-38.c.pramod-thangali.internal horton-39.c.pramod-thangali.internal horton-4.c.pramod-thangali.internal horton-40.c.pramod-thangali.internal horton-41.c.pramod-thangali.internal horton-42.c.pramod-thangali.internal horton-43.c.pramod-thangali.internal horton-44.c.pramod-thangali.internal horton-45.c.pramod-thangali.internal horton-46.c.pramod-thangali.internal horton-47.c.pramod-thangali.internal horton-48.c.pramod-thangali.internal horton-49.c.pramod-thangali.internal horton-5.c.pramod-thangali.internal horton-50.c.pramod-thangali.internal horton-51.c.pramod-thangali.internal horton-52.c.pramod-thangali.internal horton-53.c.pramod-thangali.internal horton-54.c.pramod-thangali.internal horton-55.c.pramod-thangali.internal horton-56.c.pramod-thangali.internal horton-57.c.pramod-thangali.internal horton-58.c.pramod-thangali.internal horton-59.c.pramod-thangali.internal horton-6.c.pramod-thangali.internal horton-60.c.pramod-thangali.internal horton-61.c.pramod-thangali.internal horton-62.c.pramod-thangali.internal horton-63.c.pramod-thangali.internal horton-64.c.pramod-thangali.internal horton-65.c.pramod-thangali.internal horton-66.c.pramod-thangali.internal horton-67.c.pramod-thangali.internal horton-68.c.pramod-thangali.internal horton-69.c.pramod-thangali.internal horton-7.c.pramod-thangali.internal horton-70.c.pramod-thangali.internal horton-71.c.pramod-thangali.internal horton-72.c.pramod-thangali.internal horton-73.c.pramod-thangali.internal horton-74.c.pramod-thangali.internal horton-75.c.pramod-thangali.internal horton-76.c.pramod-thangali.internal horton-77.c.pramod-thangali.internal horton-78.c.pramod-thangali.internal horton-79.c.pramod-thangali.internal horton-8.c.pramod-thangali.internal horton-80.c.pramod-thangali.internal horton-81.c.pramod-thangali.internal horton-82.c.pramod-thangali.internal horton-83.c.pramod-thangali.internal horton-84.c.pramod-thangali.internal horton-85.c.pramod-thangali.internal horton-86.c.pramod-thangali.internal horton-87.c.pramod-thangali.internal horton-88.c.pramod-thangali.internal horton-9.c.pramod-thangali.internal horton-90.c.pramod-thangali.internal horton-91.c.pramod-thangali.internal horton-92.c.pramod-thangali.internal horton-93.c.pramod-thangali.internal horton-94.c.pramod-thangali.internal horton-95.c.pramod-thangali.internal horton-96.c.pramod-thangali.internal horton-97.c.pramod-thangali.internal horton-98.c.pramod-thangali.internal horton-99.c.pramod-thangali.internal horton-master-1.c.pramod-thangali.internal horton-master-2.c.pramod-thangali.internal horton-master-3.c.pramod-thangali.internal'  'Host'  25  NULL) was aborted. Call getNextException to see the cause.Error Code: 0Call: INSERT INTO requestoperationlevel (operation_level_id  cluster_name  host_component_name  host_name  level_name  request_id  service_name) VALUES (?  ?  ?  ?  ?  ?  ?) bind =&gt; [7 parameters bound]Query: InsertObjectQuery(org.apache.ambari.server.orm.entities.RequestResourceFilterEntity@2c6ae575) at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:333) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.processExceptionForCommError(DatabaseAccessor.java:1501) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeJDK12BatchStatement(DatabaseAccessor.java:875) at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:145) at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.appendCall(ParameterizedSQLBatchWritingMechanism.java:88) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:571) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeCall(DatabaseAccessor.java:537) at org.eclipse.persistence.internal.sessions.AbstractSession.basicExecuteCall(AbstractSession.java:1800) at org.eclipse.persistence.sessions.server.ClientSession.executeCall(ClientSession.java:286) at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.executeCall(DatasourceCallQueryMechanism.java:207) at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.executeCall(DatasourceCallQueryMechanism.java:193) at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.insertObject(DatasourceCallQueryMechanism.java:342) at org.eclipse.persistence.internal.queries.StatementQueryMechanism.insertObject(StatementQueryMechanism.java:162) at org.eclipse.persistence.internal.queries.StatementQueryMechanism.insertObject(StatementQueryMechanism.java:177) at org.eclipse.persistence.internal.queries.DatabaseQueryMechanism.insertObjectForWrite(DatabaseQueryMechanism.java:471) at org.eclipse.persistence.queries.InsertObjectQuery.executeCommit(InsertObjectQuery.java:80) at org.eclipse.persistence.queries.InsertObjectQuery.executeCommitWithChangeSet(InsertObjectQuery.java:90) at org.eclipse.persistence.internal.queries.DatabaseQueryMechanism.executeWriteWithChangeSet(DatabaseQueryMechanism.java:286) at org.eclipse.persistence.queries.WriteObjectQuery.executeDatabaseQuery(WriteObjectQuery.java:58) at org.eclipse.persistence.queries.DatabaseQuery.execute(DatabaseQuery.java:852) at org.eclipse.persistence.queries.DatabaseQuery.executeInUnitOfWork(DatabaseQuery.java:751) at org.eclipse.persistence.queries.ObjectLevelModifyQuery.executeInUnitOfWorkObjectLevelModifyQuery(ObjectLevelModifyQuery.java:108) at org.eclipse.persistence.queries.ObjectLevelModifyQuery.executeInUnitOfWork(ObjectLevelModifyQuery.java:85) at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.internalExecuteQuery(UnitOfWorkImpl.java:2875) at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1602) at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1584) at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1535) at org.eclipse.persistence.internal.sessions.CommitManager.commitNewObjectsForClassWithChangeSet(CommitManager.java:224) at org.eclipse.persistence.internal.sessions.CommitManager.commitAllObjectsForClassWithChangeSet(CommitManager.java:191) at org.eclipse.persistence.internal.sessions.CommitManager.commitAllObjectsWithChangeSet(CommitManager.java:136) at org.eclipse.persistence.internal.sessions.AbstractSession.writeAllObjectsWithChangeSet(AbstractSession.java:3914) at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitToDatabase(UnitOfWorkImpl.java:1419) at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitToDatabase(RepeatableWriteUnitOfWork.java:634) at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitToDatabaseWithChangeSet(UnitOfWorkImpl.java:1509) at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitRootUnitOfWork(RepeatableWriteUnitOfWork.java:266) at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitAndResume(UnitOfWorkImpl.java:1147) at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commitInternal(EntityTransactionImpl.java:84) at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:63) at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:91) at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:72) at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:52) at org.apache.ambari.server.actionmanager.ActionDBAccessorImpl$$EnhancerByGuice$$ddd0d231.persistActions(&lt;generated&gt;) at org.apache.ambari.server.actionmanager.ActionManager.sendActions(ActionManager.java:95) at org.apache.ambari.server.actionmanager.ActionManager.sendActions(ActionManager.java:84) at org.apache.ambari.server.controller.AmbariManagementControllerImpl.createAction(AmbariManagementControllerImpl.java:2583) at org.apache.ambari.server.controller.internal.RequestResourceProvider$1.invoke(RequestResourceProvider.java:124) at org.apache.ambari.server.controller.internal.RequestResourceProvider$1.invoke(RequestResourceProvider.java:121) at org.apache.ambari.server.controller.internal.AbstractResourceProvider.createResources(AbstractResourceProvider.java:237) at org.apache.ambari.server.controller.internal.RequestResourceProvider.createResources(RequestResourceProvider.java:121) at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:274) at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:75) at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:36) at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:72) at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:135) at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:103) at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:72) at org.apache.ambari.server.api.services.RequestService.createRequests(RequestService.java:119) at sun.reflect.GeneratedMethodAccessor113.invoke(Unknown Source)
ns,See attached video.Somehow  there are always 10 hosts selected. After explicitly hitting 'clear selection'  page refresh somehow reverts back to 10 hosts being selected.
ns,see attached.
ns,When we open hosts page  we can see  that we have excessive requests as spinner appears two times one after another.
ns,In Yarn Service page  NodeManagers Status: 3 active / 0 lost / 0 unhealthy / 0 rebooted / 1 decommissionedIn Host Detailed Page of that decommissioned NM  the state of the NM is labeled as 'stopped'  which is not consistent comparing to Service page.API call:'href' : 'http://172.18.145.115:8080/api/v1/clusters/cl1/hosts/us3mon1404188570-3.cs1cloud.internal/host_components/NODEMANAGER'  'HostRoles' : { 'cluster_name' : 'cl1'  'component_name' : 'NODEMANAGER'  'desired_admin_state' : 'DECOMMISSIONED'  'desired_stack_id' : 'HDP-2.1'  'desired_state' : 'STARTED'  'host_name' : 'us3mon1404188570-3.cs1cloud.internal'  'maintenance_state' : 'OFF'  'service_name' : 'YARN'  'stack_id' : 'HDP-2.1'  'stale_configs' : false  'state' : 'INSTALLED' As a result  user cannot start the NodeManger as it is considered to be decommissioned by ResourceManger.
ns,On Customize services page set next values for users and groups:Name: ValueProxy group for Hive WebHCat Oozie and Falcon: custom-usersgrHDFS User: custom-hdfsMapReduce User: custom-mapredYARN User: custom-yarnHBase User: custom-hbaseHive User: custom-hiveHCat User: custom-hcatWebHCat User: custom-hcatOozie User: custom-oozieFalcon User: custom-falconStorm User: custom-stormZooKeeper User: custom-zookeeperGanglia User: custom-nobodyNagios User: custom-nagiosNagios Group: custom-nagiosgrSmoke Test User: ambari-qaTez User: custom-tezHadoop Group: custom-hadoopgrSkip group modifications during install: falseAfter deploy many services fail to start:MapReduce  Hbase  Hive  WebHcat  Falcon  OozieSame error for all:Call From ins1404365245-9.cs1cloud.internal/172.18.145.154 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused
ns,STR:1. Run Move Wizard for some master component.2. Do not click Complete button after all operation will be finished. Close wizard (confirm closing).3. Without page refresh open wizard for the same master component and try it to reassign to another host (ex. back to the previous host).Wizard will get stuck  as source host for master component will be calculated incorrectly.This issue is related to host components mapper. After first running of move wizard in the model there were 2 masters one with host before moving and the new one. Old host component was deleted on server but UI model still contains it. And it brakes algorithm of calculating source host on assign master step.
ns,When Move Wizard is running on cluster with a large number of hosts  assign master step has input to enter hostname instead of select dropdown. If this dropdown is empty  it shows an error. But if in the same time target host was selected correctly the next button is enabled.
ns,Manually running the postgres create sql script encountered errors.Errors: psql:Ambari-DDL-Postgres-CREATE.sql:22: ERROR: column 'config_attributes' specified more than once psql:Ambari-DDL-Postgres-CREATE.sql:103: ERROR: relation 'clusterconfig' does not exist psql:Ambari-DDL-Postgres-CREATE.sql:126: ERROR: relation 'clusterconfig' does not existRepro Steps:1. Spin up a CentOS 6.4 VM vagrant up c6401 vagrant ssh c6401 sudo su - wget http://public-repo-1.hortonworks.com/ambari/centos6/1.x/updates/1.6.0/ambari.repo cp ambari.repo /etc/yum.repos.d yum install ambari-server -y2. Setup a postgres database vi /etc/yum.repos.d/CentOS-Base.repo edit the &#91;base&#93; and &#91;updates&#93; sections by adding the following line without quotes  'exclude=postgresql*' yum localinstall http://yum.postgresql.org/9.3/redhat/rhel-6-x86_64/pgdg-centos93-9.3-1.noarch.rpm yum install postgresql-server cd /etc/yum.repos.d/ service postgresql initdb chkconfig postgresql on service postgresql start3. Run the script cp /vagrant/Ambari-DDL-Postgres-CREATE.sql /var/lib/ambari-server/resources/ (assuming you have the latest file from trunk) cp /var/lib/ambari-server/resources/Ambari-DDL-Postgres-CREATE.sql /var/lib/pgsql/ su - postgres psql Follow step #1 at http://docs.hortonworks.com/HDPDocuments/Ambari-1.6.0.0/bk_ambari_reference/content/nndb-using-ambari-postgresql.html where $AMBARIDATABASE is ambari  $AMBARIUSER is postgres  and $AMBARISCHEMA is ambari. /i Ambari-DDL-Postgres-CREATE.sql;Then verify the errors. When done  issue /q on the psql command line to exit.
ns,On the 2k-node cluster  service config pages load slowly (though the load time has improved significantly from before).The load time depends on the service.Here are some sample load times:HDFS: 5sYARN: 17sMR2: 16sTEZ: 10sHBASE: 3sHIVE: 13sWEBHCAT: 3sFALCON: 3sSTORM: 17sOOZIE: 11sGANGLIA: 1sNAGIOS: 1sZOOKEEPER: 2sPIG: 2sSQOOP: n/a (no config page)
ns,Step 4 of the Ambari installer has a broken link.When 'Choosing Services'  the link for the 'ExtJS' library license under the 'Oozie' service has moved from http://www.sencha.com/products/extjs/license/It should be updated to the URL below since it covers that Ext JS is available under GPL v3 license:http://www.sencha.com/legal/open-source-faq/
ns,In the Background Operations popup  the filter state persists at the Host and Task levels  even after closing the popup (the filter should be cleared once the user goes back up one level or closes the popup).This causes a lot of confusion because the user will expect these filters to be cleared but instead cannot see the hosts and tasks that they are expecting.
ns,STR: Enabled NNHA.Active NameNode is on suse1101 hostStandby NameNode is on suse1102 host Stopped SNN. Opened move NN wizardResult: Next button is active when I choose existing topology (suse1101 and suse1102)Next button is inactive when I choose custom topology (suse1103 and suse1101)UPD: Reproduced also on non-HA cluster.
ns,'Uncaught TypeError: Cannot read property 'properties' of undefined'at /ambari-web/app/views/common/quick_view_link_view.js:246
ns,Testing using the 1.6.1 RC bits (branch-1.6.1  hash=ffb702b252a8b979529864ec5579465a122060b5) revealed that Add Services can corrupt config files in an unpredictable manner.We have seen: adding Pig resulted in core-site dropping all existing properties except for 'hadoop.proxyuser.falcon.hosts and hadoop.proxyuser.falcon.groups' add Storm resulted in core-site retaining all existing properties but dropping hadoop.proxyuser.* settings for all services except Falcon adding Storm dropped a number of existing properties in yarn-site (and NodeManagers can no longer restart after that)The bottomline here is that behavior seems pretty random (adding the same service on different clusters results in different behavior).Also  when adding a service  certain configs (like hdfs-site) were not cloned while most other configs (like yarn-site  mapred-site) get cloned even if no property changes are made regardless of which service is added.Update: changes to hdfs-site during Add Services Wizard never persists.
ns,STR: Click AddHost Type existing host Click 'Register and Confirm' buttonActual result: There present only message about 'SSH Private Key is required'Excpected Result: SHould be also present message like 'These hosts are already exists'.---------------------------------Note: the excpected message only present if we type/select ss key and will click 'Register and Confirm'
ns,If ATS stopped  Jobs page continuously produces following js errors:NetworkError: 400 Bad Request - http://172.18.145.185:8080/proxy?url=http://us1mon1402550931-re-5.cs1cloud.internal:8188/ws/v1/timeline/HIVE_QUERY_ID?fields=events primaryfilters otherinfo&amp;secondaryFilter=tez:true&amp;limit=11&amp;_=1402578530622'proxy?...8530622'NetworkError: 400 Bad Request - http://172.18.145.185:8080/proxy?url=http://us1mon1402550931-re-5.cs1cloud.internal:8188/ws/v1/timeline/HIVE_QUERY_ID?limit=1&amp;secondaryFilter=tez:true&amp;_=1402578531674'proxy?...8531674'NetworkError: 400 Bad Request - http://172.18.145.185:8080/proxy?url=http://us1mon1402550931-re-5.cs1cloud.internal:8188/ws/v1/timeline/HIVE_QUERY_ID?limit=1&amp;secondaryFilter=tez:true&amp;_=1402578537654'
ns,Default config group name contains 'undefined' instead of service name in Manage Config Groups popup
ns,STROn step 4 of Install Wizard  select any of the services dependent on YARN (Piq  Oozie  Hive) but don't select YARN itself. Press 'Next' button.Expected resultPopup with the info about YARN dependence appears.Actual resultNothing happens. JS error occurs:Uncaught TypeError: Cannot read property 'get' of undefinedatapp/controllers/wizard/step4_controller.js:180
ns,'Customize Services' step indicates that 8 YARN properties need revision but no one is highlighted as incorrect.
ns,Hostname CaSe SeNsiTiVity causes hostnames don't match error.'in the install wizard  the hostnames MUST be lower case in the user input  otherwise the 'hostnames don't match' during the install and it fails. Ambari should tolower() both hosts before comparing.'Effected Install/Add Host Wizard.
ns,STRProceed to step 7 of Install Wizard. Return to step 1 and choose a different stack version. Go forward.ResultUnable to proceed from step 3. JS error occurs:Uncaught Error: &lt;DS.StateManager:ember26192&gt; could not respond to event didChangeData in state rootState.loaded.updated.uncommitted.WorkaroundTo log out and log in again before perforfing installation with different stack version.
ns,Background : Centos7 is very different from centos6.5 and currently Ambari &amp; HDP are not supported on Centos7Supporting Ambari on CentOS7/RHEL7 has multiple elements :1)mvn build of Ambari should be successful. document required steps in wiki.2)Ambari server &amp; agent should be modified to support centos7/redhat7. following changes are necessary :2.1)/usr/lib/python2.6 symbolic link .2.2)code changes to add redhat7 as a new OS family.3)HDP centos7 rpms + new repo to be made available to deploy different services on centos7/redhat7 .Currently centos7 is not supported by the Ambari/HDP build team.This JIRA captures 2.2)
ns,Create main page with table of jobs
ns,Add ZKFC component on summary page. Each Zkfc should under related Namenode  with a smaller size.
ns,added a host to a cluster (manually installed and started an agent)http://c6403.ambari.apache.org:8080/api/v1/hosts/c6401.ambari.apache.org{ 'href' : 'http://c6403.ambari.apache.org:8080/api/v1/hosts/c6401.ambari.apache.org'  'Hosts' : { 'host_health_report' : ''  'host_name' : 'c6401.ambari.apache.org'  'host_state' : 'HEALTHY'  'host_status' : 'HEALTHY' }}After the agent is stopped for the host{ 'href' : 'http://c6403.ambari.apache.org:8080/api/v1/hosts/c6401.ambari.apache.org'  'Hosts' : { 'host_health_report' : ''  'host_name' : 'c6401.ambari.apache.org'  'host_state' : 'HEARTBEAT_LOST'  'host_status' : 'UNKNOWN' }}At this point I can perform AddHost and start the wizard. But AddHost will eventually fail - although now I see it just waiting.We need some form of error indication when host is not lost.
ns,Service pluggability: New added services in the stack has Add Property link in log-4j and env categories
ns,STR Install cluster Go to Add Service Wizard Proceed to step 'Customize services' While configs are loading click 'Next'.If user tried to install services with configs that should be provided manually using UI (like Nagios)  he will get JS-error on next step.
ns,'all/none' links affect already installed services on 'Select Services' step
ns,The UI links to Ganglia Web to display the graphs for a specific host as http://&lt;ganglia-host&gt;/ganglia/mobile_helper.php?show_host_metrics=1&amp;h=&lt;target-fqdn&gt;&amp;c=HDPNameNode&amp;r=hour&amp;cs=&amp;ce=Note that c=HDPNameNode is static.I believe this worked before as the NameNode gmond was repeated on all the hosts (we should not have and this has been fixed but now it's causing this issue).For this ticket  let's change the call the UI makes from 'HDPNameNode' to 'HDPSlaves' and verify that host-level Ganglia links work for all hosts on a multi-node cluster (including hosts with clients only).
ns,When attempting to delete a host through the UI  and then re-add it  the re-add operation fails because a record already exists in the clusterhostmapping table.This can be reproduced as follows (host names will change of course) 1. Create a cluster and add a host so that it is populated in the clusterhostmapping table.2. Make sure the agent is running.3. On the server  run ambari-server restart  and immediately run the following repeatedly in another terminal window before the restart finishes  curl --write-out %{http_code} --show-error -u admin:admin -H 'X-Requested-By:1' -i -X DELETE http://c6404.ambari.apache.org:8080/api/v1/clusters/dev/hosts/c6407.ambari.apache.orgHTTP/1.1 200 OKSet-Cookie: AMBARISESSIONID=z91px2l41uc6dwjv52zl2mcu;Path=/Expires: Thu  01 Jan 1970 00:00:00 GMTContent-Type: text/plainContent-Length: 0Server: Jetty(7.6.7.v20120910)4. Quickly verify that the host name is removed from the clusterhostmapping table.5. On the agent  run ambari-agent restart  and repeatedly requery the clusterhostmapping table  until the record is reinserted (should take no more than 30 seconds to appear).6. Run the curl command to attempt to re-add the host  and receive the error message curl --write-out %{http_code} --show-error -u admin:admin -H 'X-Requested-By:1' -i POST http://c6404.ambari.apache.org:8080/api/v1/clusters/dev/hosts/c6407.ambari.apache.orgHTTP/1.1 500 Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.4.0.v20120608-r11652): org.eclipse.persistence.exceptions.DatabaseException Internal Exception: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO ClusterHostMapping (cluster_id  host_name) VALUES (2  'c6407.ambari.apache.org') was aborted. Call getNextException to see the cause. Error Code: 0 Call: INSERT INTO ClusterHostMapping (cluster_id  host_name) VALUES (?  ?) bind =&gt; [2 parameters bound]Set-Cookie: AMBARISESSIONID=1je1wahcml82f11gjrserxgdyl;Path=/Content-Type: text/plain;charset=ISO-8859-1Content-Length: 530Server: Jetty(7.6.7.v20120910){ 'status': 500  'message': 'Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.4.0.v20120608-r11652): org.eclipse.persistence.exceptions.DatabaseException/nInternal Exception: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO ClusterHostMapping (cluster_id  host_name) VALUES (2  /u0027c6407.ambari.apache.org/u0027) was aborted. Call getNextException to see the cause./nError Code: 0/nCall: INSERT INTO ClusterHostMapping (cluster_id  host_name) VALUES (?  ?)/n/tbind /u003d/u003e [2 parameters bound]'At this point  here is the state of the tables.select * from clusterhostmapping where host_name = 'c6407.ambari.apache.org'; cluster_id | host_name------------+------------------------- 2 | c6407.ambari.apache.orgselect * from hoststate where host_name = 'c6407.ambari.apache.org'; agent_version | available_mem | current_state | health_status | host_name | time_in_state | maintenance_state---------------------+---------------+---------------+----------------------------------------------+-------------------------+---------------+------------------- {'version':'1.6.0'} | 250232 | INIT | {'healthStatus':'HEALTHY' 'healthReport':''} | c6407.ambari.apache.org | 1405718796141 | {'2':'ON'}I then deleted both records  restarted the server  and was then able to add the host successfully.This is a bug in the persistence layer.
ns,1) installed the files view (built from source)2) I created an instance of the viewPOSThttp://c6401.ambari.apache.org:8080/api/v1/views/FILES/versions/0.1.0/instances/MyFiles[ {'ViewInstanceInfo' : { 'properties' : { 'dataworker.defaultFs' : 'webhdfs://c6401.ambari.apache.org:50070'  'dataworker.username' : 'ambari-qa' } }} ]3) I restart ambari-server and get this exception  so ambari-server can't start up. If I delete the view jar and restart  then I can get ambari-server to start.00:41:59 433 INFO [main] Server:266 - jetty-7.6.7.v2012091000:41:59 914 WARN [main] WebAppContext:489 - Failed startup of context o.e.j.w.WebAppContext{/views/FILES/0.1.0/MyFiles file:/var/lib/ambari-server/resources/views/work/FILES%7B0.1.0%7D/} /var/lib/ambari-server/resources/views/work/FILES{0.1.0}java.util.zip.ZipException: invalid entry size (expected 12027 but got 11985 bytes) at java.util.zip.ZipInputStream.readEnd(ZipInputStream.java:403) at java.util.zip.ZipInputStream.read(ZipInputStream.java:195)
ns,Management Console: UI Layout  Basic Routing and Create User Management Page (with mock data)
ns,In Ambari if a stack service has configs with final=true  then during install wizard these configs should have the final-checkbox checked. Also  if the user changes these checkboxes  the values should be stored in the properties_attributes of configs when Deploy is hit.
ns,Ambari-DDL-Postgres-CREATE.sql fix for CLUSTER.OPERATE
ns,testDecidePopulationStrategy_unsupportedSchema(org.apache.ambari.server.controller.internal.BlueprintResourceProviderTest) Time elapsed: 0.02 sec &lt;&lt;&lt; FAILURE!java.lang.AssertionError:Expected: (exception with message a string containing 'Configuration definition schema is not supported' and an instance of java.lang.IllegalArgumentException) got: &lt;java.lang.IllegalArgumentException: 'Configuration format provided in Blueprint is not supported'&gt;
ns,Unit tests are broken after commit f3aab68ec417d8e2c39c216962e1e1d47d56d401The root cause is the properties in InMemoryDefaultTestModule.java.
ns,HUE is defined in HDP-1.3.2 stack with configuration sites but there is no agent scripts defining configure/stop/start/status functions for the service.HUE is not supported to work with Ambari. In that case we should not expose HUE via stack definition API as supported service of HDP-1.x stack.
ns,Trying to deploy latest trunk build ambari-server-1.7.0-70 and hit the following when running ambari-server startambari-server.log01:30:53 074 INFO [main] AmbariServer:157 - ********* Meta Info initialized **********01:30:53 086 INFO [main] ClustersImpl:103 - Initializing the ClustersImpl01:30:53 679 INFO [main] AmbariManagementControllerImpl:230 - Initializing the AmbariManagementControllerImpl01:30:53 894 INFO [main] AmbariServer:487 - Checking DB store version01:30:53 894 WARN [main] AmbariServer:502 - Current database store version is not compatible with current server version  serverVersion=null  schemaVersion=null01:30:53 895 ERROR [main] AmbariServer:592 - Failed to run the Ambari Serverorg.apache.ambari.server.AmbariException: Current database store version is not compatible with current server version  serverVersion=null  schemaVersion=null at org.apache.ambari.server.controller.AmbariServer.checkDBVersion(AmbariServer.java:503) at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:164) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:589)The metainfo table is empty with no values indicating version of Ambari installed.
ns,API triggered on clicking Retry button results in failure with 400 status code
ns,See the screenshot attached.JS error Uncaught TypeError: Cannot read property 'filterProperty' of undefined at ambari-web/app/views/common/configs/services_config.js:338
s,Steps:Go to 'Customize services page'.Switch to other tab many a time (50 switches = about 100MB).Result: firefox browser gets 1.2 GB in memory.
ns,STR: Install single node cluster with Postgres 9 server as Ambari and Oozie DB (Stack - 2.0). Postgres server should be on dedicated host; Start all services.Actual result: Oozie server will not start.Error message for Oozie:2014-07-23 08:42:57 947 - Error while executing command 'start':Traceback (most recent call last): File '/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py'  line 111  in execute method(env) File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/OOZIE/package/scripts/oozie_server.py'  line 43  in start oozie_service(action='start') File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/OOZIE/package/scripts/oozie_service.py'  line 43  in oozie_service Execute( db_connection_check_command  tries=5  try_sleep=10) File '/usr/lib/python2.6/site-packages/resource_management/core/base.py'  line 148  in __init__ self.env.run() File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 149  in run self.run_action(resource  action) File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 115  in run_action provider_action() File '/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py'  line 239  in action_run raise exFail: Execution of '/usr/jdk64/jdk1.7.0_45/bin/java -cp /usr/lib/ambari-agent/DBConnectionVerification.jar:/usr/lib/oozie/libserver/postgresql-9.0-801.jdbc4.jar org.apache.ambari.server.DBConnectionVerification jdbc:postgresql://&lt;host&gt;:5432/ooziedb oozieuser [PROTECTED] org.postgresql.Driver' returned 1. ERROR: Unable to connect to the DB. Please check DB connection properties.java.lang.ClassNotFoundException: org.postgresql.Driver
ns,Tests run locally and pass 100% consistently.These test results are fishy; they randomly fail on different OS deployments. Even the simple logic ones fail. There are only 5 alert targets ever created  yet there are 9 returned sometimes. I'm wondering if this is because we tried to load some data in the @BeforeClass instead of @Before - maybe there's a weird Guice/JUnit race condition going on.I can't say I can fix this with confidence since I can't reproduce it. I'm going to move some things to @Before and hope it helps.java.lang.AssertionError: expected:&lt;5&gt; but was:&lt;9&gt; at org.junit.Assert.fail(Assert.java:93) at org.junit.Assert.failNotEquals(Assert.java:647) at org.junit.Assert.assertEquals(Assert.java:128) at org.junit.Assert.assertEquals(Assert.java:472) at org.junit.Assert.assertEquals(Assert.java:456) at org.apache.ambari.server.orm.dao.AlertDispatchDAOTest.testFindAllTargets(AlertDispatchDAOTest.java:117)
ns,Oozie fails for stack 2.0 and 2.1
ns,STR: Deployed cluster with Flume  without Flume Agents. Added Flume agent on each host. Configured 7 agents  assigned to different hosts. Changed config of one Flume agent. Clicked 'Refresh configs' action.Result: Nothing happened.
ns,STR: Go to YARN &gt; Config Under 'Scheduler' section  modify 'Capacity Scheduler' configs. Hit Save. The page reloads. The modifications are reverted back.
ns,Installed 1.7.0 build and RM HA is not available unless I go enable #experimental.Should not be experimental. RM HA is available with HDP 2.1+ Stack (but not with HDP 2.0.* stack).
ns,Configure RM HA. None of the summary info shows in Services &gt; YARN &gt; Summary All of the RM Dashboard widgets show N/A
ns,Go to HDFS config page.Change Name Node java heap size.Save configs.'Successful' message appears  but on config page edit boxes have wrong values.After clicking 'OK' fields get right value again.Same issue reproduced on Nano but it looks like there it fails to save configs.
ns,STR:1) Deploy cluster2) Go to Enable Resourse Manager Enable HA wizard3) Go to 'Select Host' page4) Select some different from default value in combobox 'Additional Resourse manager'5) Click 'Next' and go to 'Review' page6) Click 'Back' and again go to 'Select Host' page.Actual result: value in 'Additional Resourse Manager' is default againExpected result: value in 'Additional Resourse Manager' is the same with choosen previously.
ns,When installing Flume service by default if we dont configure any agents  we end up with Flume being the only service shown in a red STOPPED state. For the case where there are no agents  this should be set to STARTED.
ns,On Assign Slaves step  elements in the table got broken in two lines.see attached.
ns,STR:1) Deploy 3-node cluster2) Enable RM HA3) Go to YARN Service page  Summary tabResult: There are not Resource Managers on Summary tab4) Select config tab and after that return back to Summary tabResult: Summary tab is as expected
ns,add service wizard doesn't add secure cinfigs
ns,STR: On 3-node cluster  Go to Assign Masters page. Add another HBase Master. (Note: HBase Master and ZK server are only addable components.) Go forward to the review page. Review page will show correct information that 2 HBase master are selected for installation Eventually on deploying the cluster installation  HBase Master is only created and installed on the host that was a default selection on Assign Master page
ns,1. Remove 'Users' and 'Access' category from 'Admin' tab on the main menu.2. Rename 'Clusters' --&gt; 'Repositories'3. Rename 'misc' --&gt; 'Service Accounts'
ns,PROBLEM:default installation of Ambari sets webhcat-site.xmltempleton.hive.properties=hive.metastore.local=false  hive.metastore.uris=thrift://localhost:9933  hive.metastore.sasl.enabled=falsethe proper value should be the FQDN of the thrift host name  plus hive.metastore.execute.setugi=truefor example:hive.metastore.local=false  hive.metastore.uris=thrift://this.fqdn.com:9933  hive.metastore.sasl.enabled=false hive.metastore.execute.setugi=true
ns,Similar to NN HA.
